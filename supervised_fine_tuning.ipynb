{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Supervised Fine-Tuning\n",
                "\n",
                "This notebook demonstrates how to fine-tune a language model. We'll use parameter-efficient techniques and memory optimization strategies(PEFT)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Install Required Dependencies\n",
                "\n",
                "We install the necessary packages for model training, including Hugging Face transformers, PEFT for parameter-efficient fine-tuning, and bitsandbytes for quantization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#%pip install transformers datasets peft bitsandbytes accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Import Libraries\n",
                "\n",
                "Import all required libraries including transformers for model loading, datasets for data handling, and PEFT for efficient fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import torch\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "from datasets import Dataset, load_dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer, \n",
                "    AutoModelForCausalLM, \n",
                "    TrainingArguments, \n",
                "    Trainer,\n",
                "    DataCollatorForLanguageModeling\n",
                ")\n",
                "\n",
                "from peft import get_peft_model, LoraConfig, TaskType\n",
                "import bitsandbytes as bnb\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2b: Hardware Detection & Configuration\n",
                "\n",
                "Automatically detect hardware capabilities and configure training settings appropriately:\n",
                "- **Windows/NVIDIA (4GB VRAM)**: Use 4-bit quantization, small batches, gradient checkpointing\n",
                "- **Mac M3 Ultra (512GB unified)**: No quantization, larger batches, full precision available\n",
                "- **CPU fallback**: Supported but slow\n",
                "\n",
                "The notebook auto-detects available hardware and adjusts settings. You can override with environment variables in `.env`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import platform\n",
                "import psutil\n",
                "\n",
                "def detect_hardware():\n",
                "    \"\"\"Detect available hardware and return configuration dict.\"\"\"\n",
                "    config = {\n",
                "        \"device_type\": \"cpu\",\n",
                "        \"device_name\": \"CPU\",\n",
                "        \"total_memory_gb\": psutil.virtual_memory().total / (1024**3),\n",
                "        \"use_quantization\": True,\n",
                "        \"use_gradient_checkpointing\": True,\n",
                "        \"recommended_batch_size\": 1,\n",
                "        \"recommended_optimizer\": \"paged_adamw_8bit\",\n",
                "        \"use_fp16\": True,\n",
                "        \"use_bf16\": False\n",
                "    }\n",
                "    \n",
                "    # Check for CUDA (NVIDIA GPU)\n",
                "    if torch.cuda.is_available():\n",
                "        config[\"device_type\"] = \"cuda\"\n",
                "        config[\"device_name\"] = torch.cuda.get_device_name(0)\n",
                "        vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
                "        config[\"gpu_memory_gb\"] = vram_gb\n",
                "        \n",
                "        # 4GB VRAM or less: aggressive optimization\n",
                "        if vram_gb <= 6:\n",
                "            config[\"use_quantization\"] = True\n",
                "            config[\"recommended_batch_size\"] = 1\n",
                "            config[\"recommended_optimizer\"] = \"paged_adamw_8bit\"\n",
                "        # 6-16GB VRAM: moderate optimization\n",
                "        elif vram_gb <= 16:\n",
                "            config[\"use_quantization\"] = True\n",
                "            config[\"recommended_batch_size\"] = 2\n",
                "            config[\"recommended_optimizer\"] = \"paged_adamw_32bit\"\n",
                "        # >16GB VRAM: minimal optimization\n",
                "        else:\n",
                "            config[\"use_quantization\"] = False\n",
                "            config[\"recommended_batch_size\"] = 4\n",
                "            config[\"recommended_optimizer\"] = \"adamw_torch\"\n",
                "            config[\"use_gradient_checkpointing\"] = False\n",
                "    \n",
                "    # Check for Apple Silicon (MPS)\n",
                "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
                "        config[\"device_type\"] = \"mps\"\n",
                "        config[\"device_name\"] = f\"Apple Silicon ({platform.processor()})\"\n",
                "        \n",
                "        # M3 Ultra with 512GB unified memory - use full power\n",
                "        if config[\"total_memory_gb\"] > 256:\n",
                "            config[\"use_quantization\"] = False\n",
                "            config[\"use_gradient_checkpointing\"] = False\n",
                "            config[\"recommended_batch_size\"] = 8\n",
                "            config[\"recommended_optimizer\"] = \"adamw_torch\"\n",
                "            config[\"use_fp16\"] = False\n",
                "            config[\"use_bf16\"] = True  # Better for Apple Silicon\n",
                "        # Other M-series chips\n",
                "        else:\n",
                "            config[\"use_quantization\"] = True\n",
                "            config[\"recommended_batch_size\"] = 2\n",
                "            config[\"recommended_optimizer\"] = \"adamw_torch\"\n",
                "    \n",
                "    # CPU fallback\n",
                "    else:\n",
                "        config[\"device_type\"] = \"cpu\"\n",
                "        config[\"use_quantization\"] = False\n",
                "        config[\"recommended_batch_size\"] = 1\n",
                "        config[\"recommended_optimizer\"] = \"adamw_torch\"\n",
                "        config[\"use_fp16\"] = False\n",
                "    \n",
                "    return config\n",
                "\n",
                "# Detect hardware and apply overrides from .env\n",
                "hw_config = detect_hardware()\n",
                "\n",
                "# Allow manual overrides from environment\n",
                "auto_detect = os.getenv(\"AUTO_DETECT_HARDWARE\", \"true\").lower() in (\"true\", \"1\", \"yes\")\n",
                "if not auto_detect:\n",
                "    hw_config[\"device_type\"] = os.getenv(\"DEVICE_TYPE\", hw_config[\"device_type\"])\n",
                "    force_quant = os.getenv(\"FORCE_QUANTIZATION\", \"\").lower() in (\"true\", \"1\", \"yes\")\n",
                "    if force_quant:\n",
                "        hw_config[\"use_quantization\"] = True\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"HARDWARE CONFIGURATION\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Device Type: {hw_config['device_type'].upper()}\")\n",
                "print(f\"Device Name: {hw_config['device_name']}\")\n",
                "print(f\"Total System Memory: {hw_config['total_memory_gb']:.1f} GB\")\n",
                "if \"gpu_memory_gb\" in hw_config:\n",
                "    print(f\"GPU Memory: {hw_config['gpu_memory_gb']:.1f} GB\")\n",
                "print(f\"Use Quantization: {hw_config['use_quantization']}\")\n",
                "print(f\"Gradient Checkpointing: {hw_config['use_gradient_checkpointing']}\")\n",
                "print(f\"Recommended Batch Size: {hw_config['recommended_batch_size']}\")\n",
                "print(f\"Recommended Optimizer: {hw_config['recommended_optimizer']}\")\n",
                "print(f\"FP16: {hw_config['use_fp16']} | BF16: {hw_config['use_bf16']}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Load and Prepare Dataset\n",
                "\n",
                "Load JSONL files from a directory or a single file and convert into a Hugging Face Dataset. The function automatically detects whether `DATASET_PATH` points to a directory (loads all `.jsonl` files) or a single file.\n",
                "\n",
                "**Behavior:**\n",
                "- If `DATASET_PATH` is a directory: loads all `.jsonl` and `.train.jsonl` files recursively\n",
                "- If `DATASET_PATH` is a file: loads that single JSONL file\n",
                "- Combines all data into a single unified dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "def load_jsonl_dataset(path):\n",
                "    \"\"\"Load JSONL file(s) and convert to Dataset format.\n",
                "    \n",
                "    Args:\n",
                "        path: Can be a file path (loads single file) or directory path (loads all .jsonl files)\n",
                "    \n",
                "    Returns:\n",
                "        Dataset: Combined dataset from all JSONL files found\n",
                "    \"\"\"\n",
                "    data = []\n",
                "    path = Path(path)\n",
                "    \n",
                "    if path.is_file():\n",
                "        # Single file\n",
                "        print(f\"Loading single file: {path}\")\n",
                "        with open(path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                line = line.strip()\n",
                "                if line:\n",
                "                    data.append(json.loads(line))\n",
                "    elif path.is_dir():\n",
                "        # Directory - load all .jsonl files\n",
                "        jsonl_files = sorted(path.rglob('*.jsonl'))\n",
                "        if not jsonl_files:\n",
                "            raise ValueError(f\"No .jsonl files found in directory: {path}\")\n",
                "        \n",
                "        print(f\"Found {len(jsonl_files)} JSONL files in {path}\")\n",
                "        for jsonl_file in jsonl_files:\n",
                "            print(f\"  Loading: {jsonl_file.name}\")\n",
                "            with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
                "                for line in f:\n",
                "                    line = line.strip()\n",
                "                    if line:\n",
                "                        data.append(json.loads(line))\n",
                "    else:\n",
                "        raise ValueError(f\"Path does not exist: {path}\")\n",
                "    \n",
                "    print(f\"Total examples loaded: {len(data)}\")\n",
                "    return Dataset.from_list(data)\n",
                "\n",
                "# Load from directory or single file\n",
                "dataset_path = os.getenv(\"DATASET_PATH\")\n",
                "if not dataset_path:\n",
                "    raise ValueError(\"DATASET_PATH not set in .env file\")\n",
                "\n",
                "dataset = load_jsonl_dataset(dataset_path)\n",
                "print(f\"\\nDataset size: {len(dataset)}\")\n",
                "print(f\"Sample example:\\n{dataset[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Initialize Model and Tokenizer\n",
                "\n",
                "Load model with hardware-appropriate configuration:\n",
                "- **Low memory (4GB)**: 4-bit quantization, aggressive optimization\n",
                "- **High memory (M3 Ultra)**: Full precision, no quantization, maximum performance\n",
                "\n",
                "The quantization and device mapping are automatically configured based on detected hardware."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import BitsAndBytesConfig\n",
                "\n",
                "# Load model name and sanitize\n",
                "model_name = os.getenv(\"MODEL_NAME\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
                "if isinstance(model_name, str) and model_name.strip().startswith(\"MODEL_NAME=\"):\n",
                "    model_name = model_name.split(\"=\", 1)[1]\n",
                "model_name = model_name.strip().strip('\"').strip(\"'\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"Loading Model: {model_name}\")\n",
                "print(f\"{'='*60}\")\n",
                "\n",
                "# Prepare model loading arguments based on hardware\n",
                "model_kwargs = {\n",
                "    \"trust_remote_code\": False\n",
                "}\n",
                "\n",
                "# Configure quantization (only for low-memory systems)\n",
                "if hw_config[\"use_quantization\"]:\n",
                "    print(\"✓ Using 4-bit quantization (low-memory mode)\")\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_use_double_quant=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.float16\n",
                "    )\n",
                "    model_kwargs[\"quantization_config\"] = bnb_config\n",
                "    model_kwargs[\"device_map\"] = \"auto\"\n",
                "else:\n",
                "    print(\"✓ Loading full precision model (high-memory mode)\")\n",
                "    # For MPS (Apple Silicon), let PyTorch handle device placement\n",
                "    if hw_config[\"device_type\"] == \"mps\":\n",
                "        model_kwargs[\"torch_dtype\"] = torch.float16 if hw_config[\"use_fp16\"] else torch.bfloat16\n",
                "    else:\n",
                "        model_kwargs[\"device_map\"] = \"auto\"\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# Load model\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
                "\n",
                "# Apply memory optimizations based on hardware\n",
                "if hw_config[\"use_gradient_checkpointing\"]:\n",
                "    print(\"✓ Gradient checkpointing enabled\")\n",
                "    model.gradient_checkpointing_enable()\n",
                "    model.config.use_cache = False\n",
                "else:\n",
                "    print(\"✓ Gradient checkpointing disabled (high-memory mode)\")\n",
                "\n",
                "# Move to MPS if available and not using device_map\n",
                "if hw_config[\"device_type\"] == \"mps\" and \"device_map\" not in model_kwargs:\n",
                "    model = model.to(\"mps\")\n",
                "    print(\"✓ Model moved to Apple Silicon (MPS)\")\n",
                "\n",
                "print(f\"{'='*60}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: Configure Parameter-Efficient Fine-Tuning (LoRA)\n",
                "\n",
                "Set up LoRA configuration to drastically reduce trainable parameters while maintaining model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LoRA configuration for parameter-efficient training\n",
                "peft_config = LoraConfig(\n",
                "    task_type=TaskType.CAUSAL_LM,\n",
                "    inference_mode=False,\n",
                "    r=32,  # Low rank\n",
                "    lora_alpha=64,\n",
                "    lora_dropout=0.1,\n",
                "    target_modules=[\"q_proj\", \"v_proj\"]  # Apply to attention layers\n",
                ")\n",
                "\n",
                "# Wrap model with LoRA adapters\n",
                "model = get_peft_model(model, peft_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: Preprocess Dataset\n",
                "\n",
                "Tokenize the dataset and format it for causal language modeling. We concatenate prompt and completion for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_function(examples):\n",
                "    \"\"\"Tokenize text and prepare for causal language modeling\"\"\"\n",
                "    PROMPT_COLUMN = \"instruction\"\n",
                "    COMPLETION_COLUMN = \"output\"\n",
                "    texts = [\n",
                "        f\"{prompt} {completion}{tokenizer.eos_token}\"\n",
                "        for prompt, completion in zip(examples[PROMPT_COLUMN], examples[COMPLETION_COLUMN])\n",
                "    ]\n",
                "    \n",
                "    # FIX: Change padding=False to padding=\"max_length\"\n",
                "    # This ensures every single example is exactly 512 tokens\n",
                "    model_inputs = tokenizer(\n",
                "        texts,\n",
                "        max_length=512,\n",
                "        truncation=True,\n",
                "        padding=\"max_length\" \n",
                "    )\n",
                "    \n",
                "    # IMPORTANT: Manually set labels. \n",
                "    # This prevents the \"labels have excessive nesting\" error found in your log.\n",
                "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
                "    \n",
                "    return model_inputs\n",
                "\n",
                "# Apply preprocessing\n",
                "tokenized_dataset = dataset.map(\n",
                "    preprocess_function,\n",
                "    batched=True,\n",
                "    remove_columns=dataset.column_names\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7: Configure Training Arguments\n",
                "\n",
                "Set up training parameters optimized for limited memory (small batch sizes, gradient accumulation, and memory-saving options). These values are read from environment variables (see `.env` or `.env.example`) and fall back to safe defaults if not set.\n",
                "\n",
                "Important behavior:\n",
                "- The notebook reads training settings from `.env` (for example: `OUTPUT_DIR`, `MODEL_NAME`, `PER_DEVICE_TRAIN_BATCH_SIZE`, `GRADIENT_ACCUMULATION_STEPS`, `NUM_TRAIN_EPOCHS`, `LEARNING_RATE`, `FP16`, `LOGGING_STEPS`, `SAVE_STEPS`, `SAVE_TOTAL_LIMIT`, `OPTIMIZER`).\n",
                "- Before training, the notebook automatically creates a **model-specific output directory** at `OUTPUT_DIR/<sanitized MODEL_NAME>` (slashes and colons in `MODEL_NAME` are replaced with `_`), and `TrainingArguments.output_dir` is set to that path.\n",
                "\n",
                "Verification & workflow:\n",
                "- After changing `.env`, **restart the kernel** and re-run Cell 2 (which calls `load_dotenv()`), then re-run this cell and the diagnostic cell immediately after it to confirm the resolved settings and final output directory.\n",
                "\n",
                "If you prefer timestamped run directories (e.g., `OUTPUT_DIR/<model>/2026-01-09_14-20-00`) or automatic retention of older runs, tell me and I will add that behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Memory-efficient training arguments (load from env with safe defaults)\n",
                "# Create a model-specific output directory under OUTPUT_DIR (e.g. ./results/TinyLlama_TinyLlama-1.1B-Chat-v1.0)\n",
                "base_output = os.getenv(\"OUTPUT_DIR\", \"./results\")\n",
                "model_name_env = os.getenv(\"MODEL_NAME\", \"model\")\n",
                "# Sanitize model name to safe directory name\n",
                "sanitized_model_name = model_name_env.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
                "model_output_dir = os.path.join(base_output, sanitized_model_name)\n",
                "\n",
                "# Ensure directory exists\n",
                "os.makedirs(model_output_dir, exist_ok=True)\n",
                "\n",
                "# Apply hardware-aware defaults with .env overrides\n",
                "# If user sets value in .env, use that; otherwise use hardware recommendation\n",
                "def get_env_or_default(key, hw_default):\n",
                "    \"\"\"Get env var or use hardware-recommended default.\"\"\"\n",
                "    env_val = os.getenv(key)\n",
                "    if env_val is None:\n",
                "        return hw_default\n",
                "    # Parse based on type\n",
                "    if isinstance(hw_default, bool):\n",
                "        return env_val.lower() in (\"1\", \"true\", \"yes\")\n",
                "    elif isinstance(hw_default, int):\n",
                "        return int(env_val)\n",
                "    elif isinstance(hw_default, float):\n",
                "        return float(env_val)\n",
                "    return env_val\n",
                "\n",
                "# Determine precision settings\n",
                "use_fp16 = get_env_or_default(\"FP16\", hw_config[\"use_fp16\"])\n",
                "use_bf16 = get_env_or_default(\"BF16\", hw_config[\"use_bf16\"])\n",
                "\n",
                "# MPS doesn't support fp16 in training args - use bf16 instead\n",
                "if hw_config[\"device_type\"] == \"mps\" and use_fp16:\n",
                "    print(\"⚠ MPS detected: switching from FP16 to BF16 for training\")\n",
                "    use_fp16 = False\n",
                "    use_bf16 = True\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=model_output_dir,\n",
                "    per_device_train_batch_size=get_env_or_default(\"PER_DEVICE_TRAIN_BATCH_SIZE\", hw_config[\"recommended_batch_size\"]),\n",
                "    gradient_accumulation_steps=get_env_or_default(\"GRADIENT_ACCUMULATION_STEPS\", 8),\n",
                "    num_train_epochs=get_env_or_default(\"NUM_TRAIN_EPOCHS\", 3),\n",
                "    learning_rate=get_env_or_default(\"LEARNING_RATE\", 2e-4),\n",
                "    fp16=use_fp16,\n",
                "    bf16=use_bf16,\n",
                "    logging_steps=get_env_or_default(\"LOGGING_STEPS\", 10),\n",
                "    save_steps=get_env_or_default(\"SAVE_STEPS\", 100),\n",
                "    save_total_limit=get_env_or_default(\"SAVE_TOTAL_LIMIT\", 2),\n",
                "    report_to=os.getenv(\"REPORT_TO\", \"none\"),\n",
                "    dataloader_pin_memory=get_env_or_default(\"DATALOADER_PIN_MEMORY\", False),\n",
                "    remove_unused_columns=get_env_or_default(\"REMOVE_UNUSED_COLUMNS\", False),\n",
                "    optim=get_env_or_default(\"OPTIMIZER\", hw_config[\"recommended_optimizer\"])\n",
                ")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"TRAINING CONFIGURATION\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Output Directory: {model_output_dir}\")\n",
                "print(f\"Batch Size: {training_args.per_device_train_batch_size}\")\n",
                "print(f\"Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
                "print(f\"Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
                "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
                "print(f\"Learning Rate: {training_args.learning_rate}\")\n",
                "print(f\"Optimizer: {training_args.optim}\")\n",
                "print(f\"FP16: {training_args.fp16} | BF16: {training_args.bf16}\")\n",
                "print(f\"{'='*60}\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick check: resolved training args (read from environment) and model output directory\n",
                "resolved_training_args = {\n",
                "    \"output_dir\": model_output_dir,\n",
                "    \"per_device_train_batch_size\": int(os.getenv(\"PER_DEVICE_TRAIN_BATCH_SIZE\", \"1\")),\n",
                "    \"gradient_accumulation_steps\": int(os.getenv(\"GRADIENT_ACCUMULATION_STEPS\", \"8\")),\n",
                "    \"num_train_epochs\": int(os.getenv(\"NUM_TRAIN_EPOCHS\", \"3\")),\n",
                "    \"learning_rate\": float(os.getenv(\"LEARNING_RATE\", \"2e-4\")),\n",
                "    \"fp16\": os.getenv(\"FP16\", \"True\").lower() in (\"1\", \"true\", \"yes\"),\n",
                "    \"logging_steps\": int(os.getenv(\"LOGGING_STEPS\", \"10\")),\n",
                "    \"save_steps\": int(os.getenv(\"SAVE_STEPS\", \"100\")),\n",
                "    \"save_total_limit\": int(os.getenv(\"SAVE_TOTAL_LIMIT\", \"2\")),\n",
                "    \"report_to\": os.getenv(\"REPORT_TO\", \"none\"),\n",
                "    \"dataloader_pin_memory\": os.getenv(\"DATALOADER_PIN_MEMORY\", \"False\").lower() in (\"1\", \"true\", \"yes\"),\n",
                "    \"remove_unused_columns\": os.getenv(\"REMOVE_UNUSED_COLUMNS\", \"False\").lower() in (\"1\", \"true\", \"yes\"),\n",
                "    \"optim\": os.getenv(\"OPTIMIZER\", \"paged_adamw_8bit\")\n",
                "}\n",
                "\n",
                "print(\"Model name:\", model_name_env)\n",
                "print(\"Sanitized model name:\", sanitized_model_name)\n",
                "print(\"Model output directory:\", model_output_dir)\n",
                "print(\"Resolved training args:\")\n",
                "print(json.dumps(resolved_training_args, indent=2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Output directories & run organization\n",
                "\n",
                "- **Base output directory** is configured via `.env` using **OUTPUT_DIR** (default `./results`).\n",
                "- The notebook creates a **model-specific** subdirectory automatically: **OUTPUT_DIR/<sanitized MODEL_NAME>**.\n",
                "  - The notebook sanitizes `MODEL_NAME` by replacing `/` and `:` with `_` (e.g. `TinyLlama/TinyLlama-1.1B-Chat-v1.0` → `TinyLlama_TinyLlama-1.1B-Chat-v1.0`).\n",
                "- This subdirectory is created before training starts so all Trainer checkpoints and logs go into that folder.\n",
                "\n",
                "Recommended model candidates (start with one at a time):\n",
                "- **Llama 2 (3B)** — `meta-llama/Llama-2-3b-chat-hf`\n",
                "  - Good quality for instruction-following; may require HF access tokens. Best starting point for 3B family.\n",
                "- **RedPajama / Together (3B)** — `togethercomputer/RedPajama-INCITE-3B-v1`\n",
                "  - Open alternative with similar scale and good community support.\n",
                "- **Mistral (3B family)** — use a Mistral 3B variant from Hugging Face (e.g., `mistralai/*`).\n",
                "  - Competitive performance; pick an official 3B HF repo if available.\n",
                "\n",
                "Tips when trying a larger model:\n",
                "- Use 4-bit quantization (`BitsAndBytesConfig`) and set `bnb_4bit_compute_dtype=torch.float16`.\n",
                "- Load with `device_map='auto'` and allow CPU offload so the bigger model can use remaining host RAM.\n",
                "- Enable `gradient_checkpointing` and set `model.config.use_cache = False` to reduce peak memory during training.\n",
                "\n",
                "How to test a candidate:\n",
                "1. Update `MODEL_NAME` in `.env` to the chosen model (e.g., `MODEL_NAME=meta-llama/Llama-2-3b-chat-hf`).\n",
                "2. Restart the kernel, re-run Cell 2 (which loads `.env`), then run the *load-test* cell (I can add this cell for you to automatically test loads) or re-run Cell 7 and the diagnostic cell to see if the model output directory and settings resolve correctly.\n",
                "3. If the model fails to load on your laptop, revert to a smaller candidate or consider using a remote GPU (recommended for >3B models).\n",
                "\n",
                "If you'd like, I can add a small **load-test** cell that attempts to load each candidate and reports success/failure and approximate VRAM usage. Would you like me to add that now?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 8: Initialize Trainer and Start Training\n",
                "\n",
                "Create the trainer with our model, dataset, and training configuration, then start the fine-tuning process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data collator for language modeling\n",
                "data_collator = DataCollatorForLanguageModeling(\n",
                "    tokenizer=tokenizer,\n",
                "    mlm=False  # Causal language modeling\n",
                ")\n",
                "\n",
                "# Initialize trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    data_collator=data_collator\n",
                ")\n",
                " \n",
                "# Start training\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 9: Save the Fine-Tuned Model\n",
                "\n",
                "Save the trained adapters and tokenizer for later use. The base model is not saved to save space."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save only the LoRA adapters (not the full model)\n",
                "# Create model-specific subdirectory under fine-tuned-model\n",
                "fine_tuned_base = \"./fine-tuned-model\"\n",
                "fine_tuned_model_dir = os.path.join(fine_tuned_base, sanitized_model_name)\n",
                "os.makedirs(fine_tuned_model_dir, exist_ok=True)\n",
                "\n",
                "model.save_pretrained(fine_tuned_model_dir)\n",
                "tokenizer.save_pretrained(fine_tuned_model_dir)\n",
                "print(f\"Model saved successfully to: {fine_tuned_model_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 10: Test the Fine-Tuned Model\n",
                "\n",
                "Test the model with a sample prompt to verify the fine-tuning results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the model\n",
                "model.eval()\n",
                "prompt = os.getenv(\"TEST_PROMPT\", \"Tell me a joke about cats.\")\n",
                "print(f\"Using TEST_PROMPT: {prompt!r}\")\n",
                "\n",
                "# Handle device placement based on hardware\n",
                "if hw_config[\"device_type\"] == \"cuda\":\n",
                "    device = \"cuda\"\n",
                "elif hw_config[\"device_type\"] == \"mps\":\n",
                "    device = \"mps\"\n",
                "else:\n",
                "    device = \"cpu\"\n",
                "\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=100,\n",
                "        temperature=0.7,\n",
                "        do_sample=True\n",
                "    )\n",
                "    \n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
