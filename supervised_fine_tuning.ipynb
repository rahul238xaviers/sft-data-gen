{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Supervised Fine-Tuning on Limited GPU Memory\n",
                "\n",
                "This notebook demonstrates how to fine-tune a language model. We'll use parameter-efficient techniques and memory optimization strategies(PEFT)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Install Required Dependencies\n",
                "\n",
                "We install the necessary packages for model training, including Hugging Face transformers, PEFT for parameter-efficient fine-tuning, and bitsandbytes for quantization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "#%pip install transformers datasets peft bitsandbytes accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Import Libraries\n",
                "\n",
                "Import all required libraries including transformers for model loading, datasets for data handling, and PEFT for efficient fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import torch\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "from datasets import Dataset, load_dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer, \n",
                "    AutoModelForCausalLM, \n",
                "    TrainingArguments, \n",
                "    Trainer,\n",
                "    DataCollatorForLanguageModeling\n",
                ")\n",
                "\n",
                "from peft import get_peft_model, LoraConfig, TaskTypeimport bitsandbytes as bnb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Load and Prepare Dataset\n",
                "\n",
                "Load the JSONL file and convert it into a Hugging Face Dataset. We assume each line contains 'prompt' and 'completion' fields."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset size: 112\n",
                        "{'instruction': 'According to the Life Insurance Code of Practice, what is the effective date of the document?', 'output': 'March 2025'}\n"
                    ]
                }
            ],
            "source": [
                "def load_jsonl_dataset(file_path):\n",
                "    \"\"\"Load JSONL file and convert to Dataset format\"\"\"\n",
                "    data = []\n",
                "    with open(file_path, 'r') as f:\n",
                "        for line in f:\n",
                "            data.append(json.loads(line))\n",
                "    return Dataset.from_list(data)\n",
                "\n",
                "# Replace with your actual file path\n",
                "dataset_path = os.getenv(\"DATASET_PATH\")\n",
                "dataset = load_jsonl_dataset(dataset_path)\n",
                "print(f\"Dataset size: {len(dataset)}\")\n",
                "print(dataset[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Initialize Model and Tokenizer\n",
                "\n",
                "Load a lightweight model suitable for 4GB memory. We use a quantized version of a small model and apply 4-bit quantization to reduce memory usage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import BitsAndBytesConfig\n",
                "\n",
                "# Configuration for 4-bit quantization\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "# Load model and tokenizer (small model for limited memory)\n",
                "model_name =  os.getenv(\"MODEL_NAME\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map={\"\": 0}  # Load entirely on GPU 0\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: Configure Parameter-Efficient Fine-Tuning (LoRA)\n",
                "\n",
                "Set up LoRA configuration to drastically reduce trainable parameters while maintaining model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
                    ]
                }
            ],
            "source": [
                "# LoRA configuration for parameter-efficient training\n",
                "peft_config = LoraConfig(\n",
                "    task_type=TaskType.CAUSAL_LM,\n",
                "    inference_mode=False,\n",
                "    r=8,  # Low rank\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0.1,\n",
                "    target_modules=[\"q_proj\", \"v_proj\"]  # Apply to attention layers\n",
                ")\n",
                "\n",
                "# Wrap model with LoRA adapters\n",
                "model = get_peft_model(model, peft_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: Preprocess Dataset\n",
                "\n",
                "Tokenize the dataset and format it for causal language modeling. We concatenate prompt and completion for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map: 100%|██████████| 112/112 [00:00<00:00, 1274.94 examples/s]\n"
                    ]
                }
            ],
            "source": [
                "def preprocess_function(examples):\n",
                "    \"\"\"Tokenize text and prepare for causal language modeling\"\"\"\n",
                "    # Combine prompt and completion\n",
                "    PROMPT_COLUMN = \"instruction\"\n",
                "    COMPLETION_COLUMN = \"output\"\n",
                "    texts = [\n",
                "        f\"{prompt} {completion}{tokenizer.eos_token}\"\n",
                "        for prompt, completion in zip(examples[PROMPT_COLUMN], examples[COMPLETION_COLUMN])\n",
                "    ]\n",
                "    \n",
                "    # Tokenize with truncation\n",
                "    model_inputs = tokenizer(\n",
                "        texts,\n",
                "        max_length=512,\n",
                "        truncation=True,\n",
                "        padding=False\n",
                "    )\n",
                "    \n",
                "    # Create labels for causal LM (shifted by 1)\n",
                "    model_inputs[\"labels\"] = [\n",
                "        [-100] * (len(token_ids) - 1) + [token_ids[-1]]\n",
                "        for token_ids in model_inputs[\"input_ids\"]\n",
                "    ]\n",
                "    \n",
                "    return model_inputs\n",
                "\n",
                "# Apply preprocessing to dataset\n",
                "tokenized_dataset = dataset.map(\n",
                "    preprocess_function,\n",
                "    batched=True,\n",
                "    remove_columns=dataset.column_names\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7: Configure Training Arguments\n",
                "\n",
                "Set up training parameters optimized for limited memory, including gradient accumulation, small batch sizes, and memory-saving options."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Memory-efficient training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    per_device_train_batch_size=1,        # Minimum batch size\n",
                "    gradient_accumulation_steps=8,        # Effective batch size of 8\n",
                "    num_train_epochs=3,\n",
                "    learning_rate=2e-4,\n",
                "    fp16=True,                            # Mixed precision training\n",
                "    logging_steps=10,\n",
                "    save_steps=100,\n",
                "    save_total_limit=2,\n",
                "    report_to=\"none\",                     # Disable external tracking\n",
                "    dataloader_pin_memory=False,          # Reduce memory pressure\n",
                "    remove_unused_columns=False,          # Keep all columns\n",
                "    optim=\"paged_adamw_8bit\"              # Memory-efficient optimizer\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 8: Initialize Trainer and Start Training\n",
                "\n",
                "Create the trainer with our model, dataset, and training configuration, then start the fine-tuning process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [42/42 10:54, Epoch 3/3]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>10</td>\n",
                            "      <td>2.282400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>20</td>\n",
                            "      <td>2.078900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>30</td>\n",
                            "      <td>2.029400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>40</td>\n",
                            "      <td>1.978100</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "TrainOutput(global_step=42, training_loss=2.0830026808239164, metrics={'train_runtime': 674.8586, 'train_samples_per_second': 0.498, 'train_steps_per_second': 0.062, 'total_flos': 291142917513216.0, 'train_loss': 2.0830026808239164, 'epoch': 3.0})"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Data collator for language modeling\n",
                "data_collator = DataCollatorForLanguageModeling(\n",
                "    tokenizer=tokenizer,\n",
                "    mlm=False  # Causal language modeling\n",
                ")\n",
                "\n",
                "# Initialize trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    data_collator=data_collator\n",
                ")\n",
                "\n",
                "# Start training\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 9: Save the Fine-Tuned Model\n",
                "\n",
                "Save the trained adapters and tokenizer for later use. The base model is not saved to save space."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved successfully!\n"
                    ]
                }
            ],
            "source": [
                "# Save only the LoRA adapters (not the full model)\n",
                "model.save_pretrained(\"./fine-tuned-model\")\n",
                "tokenizer.save_pretrained(\"./fine-tuned-model\")\n",
                "print(\"Model saved successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 10: Test the Fine-Tuned Model\n",
                "\n",
                "Test the model with a sample prompt to verify the fine-tuning results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "What is life insurance? How does it work, and what types of policies are available? Life insurance is designed to provide financial support to beneficiaries in case of the policyholder's death. Policies have life, term, and whole life insurance options, which can be tailored to fit specific needs and situations. Life insurance is designed to give peace of mind to beneficiaries and reduce the financial burden on loved ones. Term Life Insurance: This type of policy is designed to\n"
                    ]
                }
            ],
            "source": [
                "# Test the model\n",
                "model.eval()\n",
                "prompt = os.getenv(\"TEST_PROMPT\", \"Tell me a joke about cats.\")\n",
                "print(f\"Using TEST_PROMPT: {prompt!r}\")\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=100,\n",
                "        temperature=0.7,\n",
                "        do_sample=True\n",
                "    )\n",
                "    \n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".sftEnv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
