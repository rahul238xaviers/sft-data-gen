{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Supervised Fine-Tuning\n",
                "\n",
                "This notebook demonstrates how to fine-tune a language model. We'll use parameter-efficient techniques and memory optimization strategies(PEFT)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Install Required Dependencies\n",
                "\n",
                "We install the necessary packages for model training, including Hugging Face transformers, PEFT for parameter-efficient fine-tuning, and bitsandbytes for quantization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#%pip install transformers datasets peft bitsandbytes accelerate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Import Libraries\n",
                "\n",
                "Import all required libraries including transformers for model loading, datasets for data handling, and PEFT for efficient fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import torch\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "from datasets import Dataset, load_dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer, \n",
                "    AutoModelForCausalLM, \n",
                "    TrainingArguments, \n",
                "    Trainer,\n",
                "    DataCollatorForLanguageModeling\n",
                ")\n",
                "\n",
                "from peft import get_peft_model, LoraConfig, TaskType\n",
                "import bitsandbytes as bnb\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Load and Prepare Dataset\n",
                "\n",
                "Load the JSONL file and convert it into a Hugging Face Dataset. We assume each line contains 'prompt' and 'completion' fields."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_jsonl_dataset(file_path):\n",
                "    \"\"\"Load JSONL file and convert to Dataset format\"\"\"\n",
                "    data = []\n",
                "    with open(file_path, 'r') as f:\n",
                "        for line in f:\n",
                "            data.append(json.loads(line))\n",
                "    return Dataset.from_list(data)\n",
                "\n",
                "# Replace with your actual file path\n",
                "dataset_path = os.getenv(\"DATASET_PATH\")\n",
                "dataset = load_jsonl_dataset(dataset_path)\n",
                "print(f\"Dataset size: {len(dataset)}\")\n",
                "print(dataset[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Initialize Model and Tokenizer\n",
                "\n",
                "Load a lightweight model suitable for 4GB memory. We use a quantized version of a small model and apply 4-bit quantization to reduce memory usage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import BitsAndBytesConfig\n",
                "\n",
                "# Configuration for 4-bit quantization (use float16 for consumer GPUs)\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16\n",
                ")\n",
                "\n",
                "# Load model and tokenizer (small model for limited memory)\n",
                "model_name = os.getenv(\"MODEL_NAME\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
                "# Sanitize accidental values and whitespace (handles cases like 'MODEL_NAME=...')\n",
                "if isinstance(model_name, str) and model_name.strip().startswith(\"MODEL_NAME=\"):\n",
                "    model_name = model_name.split(\"=\", 1)[1]\n",
                "model_name = model_name.strip().strip('\"').strip(\"'\")\n",
                "\n",
                "print(\"Using MODEL_NAME:\", repr(model_name))\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",  # allow offload if needed\n",
                "    trust_remote_code=False\n",
                ")\n",
                "\n",
                "# Memory-savers\n",
                "model.gradient_checkpointing_enable()\n",
                "model.config.use_cache = False\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: Configure Parameter-Efficient Fine-Tuning (LoRA)\n",
                "\n",
                "Set up LoRA configuration to drastically reduce trainable parameters while maintaining model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LoRA configuration for parameter-efficient training\n",
                "peft_config = LoraConfig(\n",
                "    task_type=TaskType.CAUSAL_LM,\n",
                "    inference_mode=False,\n",
                "    r=8,  # Low rank\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0.1,\n",
                "    target_modules=[\"q_proj\", \"v_proj\"]  # Apply to attention layers\n",
                ")\n",
                "\n",
                "# Wrap model with LoRA adapters\n",
                "model = get_peft_model(model, peft_config)\n",
                "model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: Preprocess Dataset\n",
                "\n",
                "Tokenize the dataset and format it for causal language modeling. We concatenate prompt and completion for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_function(examples):\n",
                "    \"\"\"Tokenize text and prepare for causal language modeling\"\"\"\n",
                "    # Combine prompt and completion\n",
                "    PROMPT_COLUMN = \"instruction\"\n",
                "    COMPLETION_COLUMN = \"output\"\n",
                "    texts = [\n",
                "        f\"{prompt} {completion}{tokenizer.eos_token}\"\n",
                "        for prompt, completion in zip(examples[PROMPT_COLUMN], examples[COMPLETION_COLUMN])\n",
                "    ]\n",
                "    \n",
                "    # Tokenize with truncation\n",
                "    model_inputs = tokenizer(\n",
                "        texts,\n",
                "        max_length=512,\n",
                "        truncation=True,\n",
                "        padding=False\n",
                "    )\n",
                "    \n",
                "    # Create labels for causal LM (shifted by 1)\n",
                "    model_inputs[\"labels\"] = [\n",
                "        [-100] * (len(token_ids) - 1) + [token_ids[-1]]\n",
                "        for token_ids in model_inputs[\"input_ids\"]\n",
                "    ]\n",
                "    \n",
                "    return model_inputs\n",
                "\n",
                "# Apply preprocessing to dataset\n",
                "tokenized_dataset = dataset.map(\n",
                "    preprocess_function,\n",
                "    batched=True,\n",
                "    remove_columns=dataset.column_names\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7: Configure Training Arguments\n",
                "\n",
                "Set up training parameters optimized for limited memory (small batch sizes, gradient accumulation, and memory-saving options). These values are read from environment variables (see `.env` or `.env.example`) and fall back to safe defaults if not set.\n",
                "\n",
                "Important behavior:\n",
                "- The notebook reads training settings from `.env` (for example: `OUTPUT_DIR`, `MODEL_NAME`, `PER_DEVICE_TRAIN_BATCH_SIZE`, `GRADIENT_ACCUMULATION_STEPS`, `NUM_TRAIN_EPOCHS`, `LEARNING_RATE`, `FP16`, `LOGGING_STEPS`, `SAVE_STEPS`, `SAVE_TOTAL_LIMIT`, `OPTIMIZER`).\n",
                "- Before training, the notebook automatically creates a **model-specific output directory** at `OUTPUT_DIR/<sanitized MODEL_NAME>` (slashes and colons in `MODEL_NAME` are replaced with `_`), and `TrainingArguments.output_dir` is set to that path.\n",
                "\n",
                "Verification & workflow:\n",
                "- After changing `.env`, **restart the kernel** and re-run Cell 2 (which calls `load_dotenv()`), then re-run this cell and the diagnostic cell immediately after it to confirm the resolved settings and final output directory.\n",
                "\n",
                "If you prefer timestamped run directories (e.g., `OUTPUT_DIR/<model>/2026-01-09_14-20-00`) or automatic retention of older runs, tell me and I will add that behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Memory-efficient training arguments (load from env with safe defaults)\n",
                "# Create a model-specific output directory under OUTPUT_DIR (e.g. ./results/TinyLlama_TinyLlama-1.1B-Chat-v1.0)\n",
                "base_output = os.getenv(\"OUTPUT_DIR\", \"./results\")\n",
                "model_name_env = os.getenv(\"MODEL_NAME\", \"model\")\n",
                "# Sanitize model name to safe directory name\n",
                "sanitized_model_name = model_name_env.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
                "model_output_dir = os.path.join(base_output, sanitized_model_name)\n",
                "\n",
                "# Ensure directory exists\n",
                "os.makedirs(model_output_dir, exist_ok=True)\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=model_output_dir,\n",
                "    per_device_train_batch_size=int(os.getenv(\"PER_DEVICE_TRAIN_BATCH_SIZE\", \"1\")),\n",
                "    gradient_accumulation_steps=int(os.getenv(\"GRADIENT_ACCUMULATION_STEPS\", \"8\")),\n",
                "    num_train_epochs=int(os.getenv(\"NUM_TRAIN_EPOCHS\", \"3\")),\n",
                "    learning_rate=float(os.getenv(\"LEARNING_RATE\", \"2e-4\")),\n",
                "    fp16=(os.getenv(\"FP16\", \"True\").lower() in (\"1\", \"true\", \"yes\")),\n",
                "    logging_steps=int(os.getenv(\"LOGGING_STEPS\", \"10\")),\n",
                "    save_steps=int(os.getenv(\"SAVE_STEPS\", \"100\")),\n",
                "    save_total_limit=int(os.getenv(\"SAVE_TOTAL_LIMIT\", \"2\")),\n",
                "    report_to=os.getenv(\"REPORT_TO\", \"none\"),\n",
                "    dataloader_pin_memory=(os.getenv(\"DATALOADER_PIN_MEMORY\", \"False\").lower() in (\"1\", \"true\", \"yes\")),\n",
                "    remove_unused_columns=(os.getenv(\"REMOVE_UNUSED_COLUMNS\", \"False\").lower() in (\"1\", \"true\", \"yes\")),\n",
                "    optim=os.getenv(\"OPTIMIZER\", \"paged_adamw_8bit\")\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick check: resolved training args (read from environment) and model output directory\n",
                "resolved_training_args = {\n",
                "    \"output_dir\": model_output_dir,\n",
                "    \"per_device_train_batch_size\": int(os.getenv(\"PER_DEVICE_TRAIN_BATCH_SIZE\", \"1\")),\n",
                "    \"gradient_accumulation_steps\": int(os.getenv(\"GRADIENT_ACCUMULATION_STEPS\", \"8\")),\n",
                "    \"num_train_epochs\": int(os.getenv(\"NUM_TRAIN_EPOCHS\", \"3\")),\n",
                "    \"learning_rate\": float(os.getenv(\"LEARNING_RATE\", \"2e-4\")),\n",
                "    \"fp16\": os.getenv(\"FP16\", \"True\").lower() in (\"1\", \"true\", \"yes\"),\n",
                "    \"logging_steps\": int(os.getenv(\"LOGGING_STEPS\", \"10\")),\n",
                "    \"save_steps\": int(os.getenv(\"SAVE_STEPS\", \"100\")),\n",
                "    \"save_total_limit\": int(os.getenv(\"SAVE_TOTAL_LIMIT\", \"2\")),\n",
                "    \"report_to\": os.getenv(\"REPORT_TO\", \"none\"),\n",
                "    \"dataloader_pin_memory\": os.getenv(\"DATALOADER_PIN_MEMORY\", \"False\").lower() in (\"1\", \"true\", \"yes\"),\n",
                "    \"remove_unused_columns\": os.getenv(\"REMOVE_UNUSED_COLUMNS\", \"False\").lower() in (\"1\", \"true\", \"yes\"),\n",
                "    \"optim\": os.getenv(\"OPTIMIZER\", \"paged_adamw_8bit\")\n",
                "}\n",
                "\n",
                "print(\"Model name:\", model_name_env)\n",
                "print(\"Sanitized model name:\", sanitized_model_name)\n",
                "print(\"Model output directory:\", model_output_dir)\n",
                "print(\"Resolved training args:\")\n",
                "print(json.dumps(resolved_training_args, indent=2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Output directories & run organization\n",
                "\n",
                "- **Base output directory** is configured via `.env` using **OUTPUT_DIR** (default `./results`).\n",
                "- The notebook creates a **model-specific** subdirectory automatically: **OUTPUT_DIR/<sanitized MODEL_NAME>**.\n",
                "  - The notebook sanitizes `MODEL_NAME` by replacing `/` and `:` with `_` (e.g. `TinyLlama/TinyLlama-1.1B-Chat-v1.0` → `TinyLlama_TinyLlama-1.1B-Chat-v1.0`).\n",
                "- This subdirectory is created before training starts so all Trainer checkpoints and logs go into that folder.\n",
                "\n",
                "Recommended model candidates (start with one at a time):\n",
                "- **Llama 2 (3B)** — `meta-llama/Llama-2-3b-chat-hf`\n",
                "  - Good quality for instruction-following; may require HF access tokens. Best starting point for 3B family.\n",
                "- **RedPajama / Together (3B)** — `togethercomputer/RedPajama-INCITE-3B-v1`\n",
                "  - Open alternative with similar scale and good community support.\n",
                "- **Mistral (3B family)** — use a Mistral 3B variant from Hugging Face (e.g., `mistralai/*`).\n",
                "  - Competitive performance; pick an official 3B HF repo if available.\n",
                "\n",
                "Tips when trying a larger model:\n",
                "- Use 4-bit quantization (`BitsAndBytesConfig`) and set `bnb_4bit_compute_dtype=torch.float16`.\n",
                "- Load with `device_map='auto'` and allow CPU offload so the bigger model can use remaining host RAM.\n",
                "- Enable `gradient_checkpointing` and set `model.config.use_cache = False` to reduce peak memory during training.\n",
                "\n",
                "How to test a candidate:\n",
                "1. Update `MODEL_NAME` in `.env` to the chosen model (e.g., `MODEL_NAME=meta-llama/Llama-2-3b-chat-hf`).\n",
                "2. Restart the kernel, re-run Cell 2 (which loads `.env`), then run the *load-test* cell (I can add this cell for you to automatically test loads) or re-run Cell 7 and the diagnostic cell to see if the model output directory and settings resolve correctly.\n",
                "3. If the model fails to load on your laptop, revert to a smaller candidate or consider using a remote GPU (recommended for >3B models).\n",
                "\n",
                "If you'd like, I can add a small **load-test** cell that attempts to load each candidate and reports success/failure and approximate VRAM usage. Would you like me to add that now?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 8: Initialize Trainer and Start Training\n",
                "\n",
                "Create the trainer with our model, dataset, and training configuration, then start the fine-tuning process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data collator for language modeling\n",
                "data_collator = DataCollatorForLanguageModeling(\n",
                "    tokenizer=tokenizer,\n",
                "    mlm=False  # Causal language modeling\n",
                ")\n",
                "\n",
                "# Initialize trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    data_collator=data_collator\n",
                ")\n",
                "\n",
                "# Start training\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 9: Save the Fine-Tuned Model\n",
                "\n",
                "Save the trained adapters and tokenizer for later use. The base model is not saved to save space."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save only the LoRA adapters (not the full model)\n",
                "model.save_pretrained(\"./fine-tuned-model\")\n",
                "tokenizer.save_pretrained(\"./fine-tuned-model\")\n",
                "print(\"Model saved successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 10: Test the Fine-Tuned Model\n",
                "\n",
                "Test the model with a sample prompt to verify the fine-tuning results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the model\n",
                "model.eval()\n",
                "prompt = os.getenv(\"TEST_PROMPT\", \"Tell me a joke about cats.\")\n",
                "print(f\"Using TEST_PROMPT: {prompt!r}\")\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=100,\n",
                "        temperature=0.7,\n",
                "        do_sample=True\n",
                "    )\n",
                "    \n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".sftEnv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
