{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6ddf9d",
   "metadata": {},
   "source": [
    "# Production Model Deployment Notebook\n",
    "\n",
    "**Purpose:** Deploy fine-tuned model to Ollama for LIMA integration\n",
    "\n",
    "**Process:** Merge LoRA adapters → Convert to GGUF → Import to Ollama\n",
    "\n",
    "**Reference:** [LIMA_INTEGRATION.private.md](LIMA_INTEGRATION.private.md) | [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup: Import Dependencies & Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7988a04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulkumar/dev/.trainingEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "# Core ML libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging for production visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('deployment.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6d7a8",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration & Validate Environment\n",
    "\n",
    "Load environment variables and validate all required configurations are present before proceeding with deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fa84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 20:52:38,354 - INFO - Loading environment configuration...\n",
      "2026-01-12 20:52:38,359 - INFO - Configuration loaded:\n",
      "2026-01-12 20:52:38,359 - INFO -   Base Model: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "2026-01-12 20:52:38,360 - INFO -   Fine-tuned Model Path: fine-tuned-model/Qwen_Qwen2.5-Coder-32B-Instruct\n",
      "2026-01-12 20:52:38,360 - INFO -   Output Model Name: lima-finetuned-model\n",
      "2026-01-12 20:52:38,361 - INFO -   Quantization Level: Q4_K_M\n",
      "2026-01-12 20:52:38,361 - INFO - ✓ Environment validation complete\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading environment configuration...\")\n",
    "load_dotenv()\n",
    "\n",
    "# Extract configuration from environment\n",
    "BASE_MODEL = os.getenv(\"BASE_MODEL\", \"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", BASE_MODEL)\n",
    "OUTPUT_MODEL_NAME = os.getenv(\"OUTPUT_MODEL_NAME\")\n",
    "QUANTIZATION = os.getenv(\"QUANTIZATION\", \"Q4_K_M\")  # Options: Q4_K_M, Q5_K_M, Q8_0\n",
    "\n",
    "# Validate required configuration\n",
    "if not OUTPUT_MODEL_NAME:\n",
    "    raise ValueError(\n",
    "        \"OUTPUT_MODEL_NAME must be set in .env file.\\n\"\n",
    "        \"Example: OUTPUT_MODEL_NAME=my-custom-model\"\n",
    "    )\n",
    "\n",
    "# Define paths\n",
    "FINE_TUNED_PATH = Path(\"./fine-tuned-model\") / MODEL_NAME.replace(\"/\", \"_\")\n",
    "MERGED_MODEL_PATH = Path(\"./merged-model\")\n",
    "OLLAMA_MODEL_PATH = Path(f\"./{OUTPUT_MODEL_NAME}.gguf\")\n",
    "\n",
    "logger.info(f\"Configuration loaded:\")\n",
    "logger.info(f\"  Base Model: {BASE_MODEL}\")\n",
    "logger.info(f\"  Fine-tuned Model Path: {FINE_TUNED_PATH}\")\n",
    "logger.info(f\"  Output Model Name: {OUTPUT_MODEL_NAME}\")\n",
    "logger.info(f\"  Quantization Level: {QUANTIZATION}\")\n",
    "\n",
    "# Validate fine-tuned model exists\n",
    "if not FINE_TUNED_PATH.exists():\n",
    "    logger.error(f\"Fine-tuned model not found at: {FINE_TUNED_PATH}\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"Fine-tuned model directory does not exist: {FINE_TUNED_PATH}\\n\"\n",
    "        \"Please run supervised_fine_tuning.ipynb first.\"\n",
    "    )\n",
    "\n",
    "logger.info(\"✓ Environment validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6612f2",
   "metadata": {},
   "source": [
    "## Step 2: Load Base Model & LoRA Adapters\n",
    "\n",
    "Load the pre-trained base model and apply the fine-tuned LoRA adapters.\n",
    "\n",
    "⚠️ **Note:** This step requires significant memory depending on model size. Consider GPU availability for large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80edc6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 20:52:50,286 - INFO - Loading base model: Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "2026-01-12 20:52:50,287 - INFO -   (This may take several minutes depending on model size...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:29<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 20:53:21,200 - INFO - ✓ Base model loaded successfully\n",
      "2026-01-12 20:53:21,201 - INFO - Loading LoRA adapters from: fine-tuned-model/Qwen_Qwen2.5-Coder-32B-Instruct\n",
      "2026-01-12 20:53:22,332 - INFO - ✓ LoRA adapters loaded successfully\n",
      "2026-01-12 20:53:22,334 - INFO - Model loaded with 32,797,430,784 parameters\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(f\"Loading base model: {BASE_MODEL}\")\n",
    "    logger.info(\"  (This may take several minutes depending on model size...)\")\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        device_map=\"auto\",  # Automatically handle device placement\n",
    "        trust_remote_code=True,  # Required for some models\n",
    "        torch_dtype=\"auto\"  # Use model's native precision\n",
    "    )\n",
    "    logger.info(\"✓ Base model loaded successfully\")\n",
    "    \n",
    "    logger.info(f\"Loading LoRA adapters from: {FINE_TUNED_PATH}\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        str(FINE_TUNED_PATH),\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    logger.info(\"✓ LoRA adapters loaded successfully\")\n",
    "    logger.info(f\"Model loaded with {model.num_parameters():,} parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load model: {str(e)}\")\n",
    "    raise RuntimeError(f\"Model loading failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81a39d",
   "metadata": {},
   "source": [
    "## Step 3: Merge LoRA Adapters into Base Model\n",
    "\n",
    "Merge the LoRA adapter weights into the base model to create a standalone model. This is required for GGUF conversion.\n",
    "\n",
    "⚠️ **Warning:** This operation requires memory equal to the size of the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b71bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 20:53:28,170 - INFO - Merging LoRA adapters into base model...\n",
      "2026-01-12 20:53:28,171 - INFO -   (This creates a standalone model without adapter overhead)\n",
      "2026-01-12 20:53:29,166 - INFO - ✓ Model merge completed successfully\n",
      "2026-01-12 20:53:29,266 - INFO -   Memory cleanup performed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(\"Merging LoRA adapters into base model...\")\n",
    "    logger.info(\"  (This creates a standalone model without adapter overhead)\")\n",
    "    \n",
    "    merged_model = model.merge_and_unload()\n",
    "    logger.info(\"✓ Model merge completed successfully\")\n",
    "    \n",
    "    # Clear memory of original model if needed\n",
    "    del model\n",
    "    del base_model\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    logger.info(\"  Memory cleanup performed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Model merge failed: {str(e)}\")\n",
    "    raise RuntimeError(f\"Failed to merge model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0381e",
   "metadata": {},
   "source": [
    "## Step 4: Save Merged Model to Disk\n",
    "\n",
    "Save the merged model and tokenizer in HuggingFace format. This creates a complete, standalone model that can be converted to GGUF format or shared/deployed independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48906f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 20:53:45,653 - INFO - Saving merged model to: merged-model\n",
      "2026-01-12 20:53:45,655 - INFO -   Saving model weights...\n",
      "2026-01-12 20:53:58,216 - INFO -   Saving tokenizer...\n",
      "2026-01-12 20:53:59,450 - INFO - ✓ Model saved successfully to: merged-model\n",
      "2026-01-12 20:53:59,452 - INFO -   Model size: 61.04 GB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(f\"Saving merged model to: {MERGED_MODEL_PATH}\")\n",
    "    MERGED_MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    logger.info(\"  Saving model weights...\")\n",
    "    merged_model.save_pretrained(\n",
    "        MERGED_MODEL_PATH,\n",
    "        safe_serialization=True,  # Use safetensors format (recommended)\n",
    "        max_shard_size=\"5GB\"  # Shard large models for easier handling\n",
    "    )\n",
    "    \n",
    "    # Save tokenizer\n",
    "    logger.info(\"  Saving tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "    tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "    \n",
    "    # Save model card with metadata\n",
    "    model_card = f\"\"\"---\n",
    "base_model: {BASE_MODEL}\n",
    "fine_tuned_from: {FINE_TUNED_PATH}\n",
    "created: {os.popen('date').read().strip()}\n",
    "quantization: {QUANTIZATION}\n",
    "purpose: LIMA integration\n",
    "---\n",
    "\n",
    "# {OUTPUT_MODEL_NAME}\n",
    "\n",
    "This is a fine-tuned version of {BASE_MODEL} optimized for LIMA application.\n",
    "\"\"\"\n",
    "    (MERGED_MODEL_PATH / \"README.md\").write_text(model_card)\n",
    "    \n",
    "    logger.info(f\"✓ Model saved successfully to: {MERGED_MODEL_PATH}\")\n",
    "    logger.info(f\"  Model size: {sum(f.stat().st_size for f in MERGED_MODEL_PATH.rglob('*') if f.is_file()) / (1024**3):.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save model: {str(e)}\")\n",
    "    raise RuntimeError(f\"Model save operation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0faaa25",
   "metadata": {},
   "source": [
    "## Step 5: Convert to GGUF Format\n",
    "\n",
    "Convert the HuggingFace model to GGUF format for Ollama. This requires llama.cpp tooling.\n",
    "\n",
    "**Prerequisites:**\n",
    "- llama.cpp will be automatically cloned and built\n",
    "- Python requirements from llama.cpp will be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d534c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:01:52,082 - INFO - llama.cpp already exists, pulling latest changes...\n",
      "2026-01-12 21:01:53,921 - INFO - Installing essential Python packages for conversion...\n",
      "2026-01-12 21:01:53,922 - INFO -   Installing numpy...\n",
      "2026-01-12 21:01:53,942 - WARNING -     Failed to install numpy: /Users/rahulkumar/dev/.trainingEnv/bin/python: No module named pip\n",
      "\n",
      "2026-01-12 21:01:53,942 - INFO -   Installing sentencepiece...\n",
      "2026-01-12 21:01:53,961 - WARNING -     Failed to install sentencepiece: /Users/rahulkumar/dev/.trainingEnv/bin/python: No module named pip\n",
      "\n",
      "2026-01-12 21:01:53,961 - INFO -   Installing gguf...\n",
      "2026-01-12 21:01:53,980 - WARNING -     Failed to install gguf: /Users/rahulkumar/dev/.trainingEnv/bin/python: No module named pip\n",
      "\n",
      "2026-01-12 21:01:53,980 - INFO -   Installing protobuf...\n",
      "2026-01-12 21:01:53,998 - WARNING -     Failed to install protobuf: /Users/rahulkumar/dev/.trainingEnv/bin/python: No module named pip\n",
      "\n",
      "2026-01-12 21:01:53,998 - INFO - Installing additional llama.cpp requirements...\n",
      "2026-01-12 21:01:54,017 - WARNING - Some additional requirements failed, but continuing...\n",
      "2026-01-12 21:01:54,017 - INFO - ✓ sentencepiece verified: version 0.2.1\n",
      "2026-01-12 21:01:54,018 - INFO - Building llama.cpp tools (this may take a few minutes)...\n",
      "2026-01-12 21:01:54,030 - WARNING - Build had issues but continuing: Makefile:6: *** Build system changed:\n",
      " The Makefile build has been replaced by CMake.\n",
      "\n",
      " For build instructions see:\n",
      " https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n",
      "\n",
      ".  Stop.\n",
      "\n",
      "2026-01-12 21:01:54,031 - INFO - Converting to GGUF format: lima-finetuned-model.gguf\n",
      "2026-01-12 21:01:54,031 - INFO -   Running: /Users/rahulkumar/dev/.trainingEnv/bin/python llama.cpp/convert_hf_to_gguf.py merged-model --outfile lima-finetuned-model.gguf --outtype f16\n",
      "2026-01-12 21:03:21,774 - INFO - ✓ GGUF conversion completed\n",
      "2026-01-12 21:03:21,775 - INFO - ✓ GGUF model created: lima-finetuned-model.gguf\n",
      "2026-01-12 21:03:21,776 - INFO -   File size: 61.04 GB\n"
     ]
    }
   ],
   "source": [
    "def ensure_llama_cpp():\n",
    "    \"\"\"Ensure llama.cpp is available and up-to-date\"\"\"\n",
    "    llama_cpp_path = Path(\"./llama.cpp\")\n",
    "    \n",
    "    if not llama_cpp_path.exists():\n",
    "        logger.info(\"Cloning llama.cpp repository...\")\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(f\"Failed to clone llama.cpp: {result.stderr}\")\n",
    "        logger.info(\"✓ llama.cpp cloned successfully\")\n",
    "    else:\n",
    "        logger.info(\"llama.cpp already exists, pulling latest changes...\")\n",
    "        subprocess.run([\"git\", \"-C\", str(llama_cpp_path), \"pull\"], capture_output=True)\n",
    "    \n",
    "    # Install essential packages first\n",
    "    logger.info(\"Installing essential Python packages for conversion...\")\n",
    "    essential_packages = [\n",
    "        \"numpy\",\n",
    "        \"sentencepiece\", \n",
    "        \"gguf\",\n",
    "        \"protobuf\"\n",
    "    ]\n",
    "    \n",
    "    for package in essential_packages:\n",
    "        logger.info(f\"  Installing {package}...\")\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", package],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            logger.warning(f\"    Failed to install {package}: {result.stderr}\")\n",
    "        else:\n",
    "            logger.info(f\"    ✓ {package} installed\")\n",
    "    \n",
    "    # Now try to install remaining requirements from llama.cpp\n",
    "    requirements_file = llama_cpp_path / \"requirements.txt\"\n",
    "    if requirements_file.exists():\n",
    "        logger.info(\"Installing additional llama.cpp requirements...\")\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_file)],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            logger.warning(\"Some additional requirements failed, but continuing...\")\n",
    "        else:\n",
    "            logger.info(\"✓ Additional requirements installed\")\n",
    "    \n",
    "    # Verify sentencepiece is available\n",
    "    try:\n",
    "        import sentencepiece\n",
    "        logger.info(f\"✓ sentencepiece verified: version {sentencepiece.__version__}\")\n",
    "    except ImportError:\n",
    "        raise RuntimeError(\n",
    "            \"sentencepiece installation failed. Please install manually:\\n\"\n",
    "            \"  pip install sentencepiece protobuf\"\n",
    "        )\n",
    "    \n",
    "    # Build quantization tools\n",
    "    logger.info(\"Building llama.cpp tools (this may take a few minutes)...\")\n",
    "    result = subprocess.run(\n",
    "        [\"make\", \"-C\", str(llama_cpp_path)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if result.returncode != 0:\n",
    "        logger.warning(f\"Build had issues but continuing: {result.stderr}\")\n",
    "    else:\n",
    "        logger.info(\"✓ llama.cpp tools built successfully\")\n",
    "    \n",
    "    return llama_cpp_path\n",
    "\n",
    "def convert_to_gguf(merged_path: Path, output_path: Path, llama_cpp_path: Path):\n",
    "    \"\"\"Convert HuggingFace model to GGUF format\"\"\"\n",
    "    logger.info(f\"Converting to GGUF format: {output_path}\")\n",
    "    \n",
    "    convert_script = llama_cpp_path / \"convert_hf_to_gguf.py\"\n",
    "    if not convert_script.exists():\n",
    "        # Try alternative script name\n",
    "        convert_script = llama_cpp_path / \"convert.py\"\n",
    "        if not convert_script.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Conversion script not found in {llama_cpp_path}.\\n\"\n",
    "                \"Expected: convert_hf_to_gguf.py or convert.py\"\n",
    "            )\n",
    "    \n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(convert_script),\n",
    "        str(merged_path),\n",
    "        \"--outfile\", str(output_path),\n",
    "        \"--outtype\", \"f16\"  # Use f16 precision for unquantized version\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"  Running: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        logger.error(f\"Conversion stdout: {result.stdout}\")\n",
    "        logger.error(f\"Conversion stderr: {result.stderr}\")\n",
    "        raise RuntimeError(\n",
    "            f\"GGUF conversion failed with exit code {result.returncode}.\\n\"\n",
    "            f\"Error: {result.stderr}\\n\"\n",
    "            \"Check that the merged model format is compatible with llama.cpp.\"\n",
    "        )\n",
    "    \n",
    "    logger.info(\"✓ GGUF conversion completed\")\n",
    "    return output_path\n",
    "\n",
    "try:\n",
    "    llama_cpp_path = ensure_llama_cpp()\n",
    "    gguf_path = convert_to_gguf(MERGED_MODEL_PATH, OLLAMA_MODEL_PATH, llama_cpp_path)\n",
    "    \n",
    "    logger.info(f\"✓ GGUF model created: {gguf_path}\")\n",
    "    logger.info(f\"  File size: {gguf_path.stat().st_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"GGUF conversion failed: {str(e)}\")\n",
    "    logger.info(\"\\nTroubleshooting steps:\")\n",
    "    logger.info(\"1. Manually install: pip install sentencepiece protobuf gguf numpy\")\n",
    "    logger.info(\"2. Ensure merged model was saved correctly in previous step\")\n",
    "    logger.info(\"3. Check llama.cpp GitHub for latest compatibility updates\")\n",
    "    logger.info(\"4. Verify tokenizer files exist in merged-model directory\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249cfa7",
   "metadata": {},
   "source": [
    "## Step 6: Quantize Model & Import to Ollama\n",
    "\n",
    "Quantize the model to reduce size (recommended for production), then create a Modelfile and import to Ollama for serving.\n",
    "\n",
    "**Quantization Options:**\n",
    "- **Q4_K_M**: 4-bit, good quality/size balance (recommended)\n",
    "- **Q5_K_M**: 5-bit, better quality, larger size\n",
    "- **Q8_0**: 8-bit, best quality, largest size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "befa88de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:03:39,370 - INFO - Quantizing model with Q4_K_M...\n",
      "2026-01-12 21:03:39,371 - WARNING - Quantization tool not found, skipping quantization\n",
      "2026-01-12 21:03:39,371 - INFO - Creating Ollama model: lima-finetuned-model\n",
      "2026-01-12 21:03:39,384 - INFO -   Modelfile created: Modelfile\n",
      "2026-01-12 21:03:39,385 - INFO -   Importing model to Ollama...\n",
      "2026-01-12 21:04:44,526 - INFO - ✓ Model 'lima-finetuned-model' successfully imported to Ollama\n",
      "2026-01-12 21:04:44,526 - INFO - \n",
      "============================================================\n",
      "2026-01-12 21:04:44,527 - INFO - DEPLOYMENT COMPLETE!\n",
      "2026-01-12 21:04:44,527 - INFO - ============================================================\n",
      "2026-01-12 21:04:44,528 - INFO - Model Name: lima-finetuned-model\n",
      "2026-01-12 21:04:44,528 - INFO - Model Path: lima-finetuned-model.gguf\n",
      "2026-01-12 21:04:44,528 - INFO - \n",
      "Test with: ollama run lima-finetuned-model \"What is life insurance?\"\n",
      "2026-01-12 21:04:44,529 - INFO - \n",
      "To use in LIMA, update .env with:\n",
      "2026-01-12 21:04:44,529 - INFO -   LOCAL_MODEL_NAME=lima-finetuned-model\n",
      "2026-01-12 21:04:44,529 - INFO -   LOCAL_MODEL_URL=http://localhost:11434\n",
      "2026-01-12 21:04:44,529 - INFO -   LOCAL_MODEL_TYPE=ollama\n"
     ]
    }
   ],
   "source": [
    "def quantize_model(input_path: Path, output_path: Path, quantization: str, llama_cpp_path: Path):\n",
    "    \"\"\"Quantize GGUF model to reduce size\"\"\"\n",
    "    logger.info(f\"Quantizing model with {quantization}...\")\n",
    "    \n",
    "    quantize_tool = llama_cpp_path / \"llama-quantize\"\n",
    "    if not quantize_tool.exists():\n",
    "        logger.warning(\"Quantization tool not found, skipping quantization\")\n",
    "        return input_path\n",
    "    \n",
    "    cmd = [str(quantize_tool), str(input_path), str(output_path), quantization]\n",
    "    logger.info(f\"  Running: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        logger.error(f\"Quantization failed: {result.stderr}\")\n",
    "        raise RuntimeError(f\"Model quantization failed: {result.stderr}\")\n",
    "    \n",
    "    logger.info(\"✓ Model quantization completed\")\n",
    "    logger.info(f\"  Original size: {input_path.stat().st_size / (1024**3):.2f} GB\")\n",
    "    logger.info(f\"  Quantized size: {output_path.stat().st_size / (1024**3):.2f} GB\")\n",
    "    logger.info(f\"  Compression ratio: {input_path.stat().st_size / output_path.stat().st_size:.2f}x\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def create_ollama_model(model_path: Path, model_name: str):\n",
    "    \"\"\"Create Modelfile and import model to Ollama\"\"\"\n",
    "    logger.info(f\"Creating Ollama model: {model_name}\")\n",
    "    \n",
    "    # Check if Ollama is available\n",
    "    result = subprocess.run([\"which\", \"ollama\"], capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(\"Ollama not found. Please install Ollama from https://ollama.ai\")\n",
    "    \n",
    "    # Create Modelfile with proper configuration\n",
    "    modelfile_content = f'''FROM ./{model_path.name}\n",
    "\n",
    "# Template for prompt formatting\n",
    "TEMPLATE \"\"\"{{{{ .Prompt }}}}\"\"\"\n",
    "\n",
    "# Model parameters optimized for LIMA\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "PARAMETER top_k 40\n",
    "PARAMETER num_ctx 4096\n",
    "PARAMETER stop \"<|endoftext|>\"\n",
    "PARAMETER stop \"<|im_end|>\"\n",
    "\n",
    "# System message for LIMA context\n",
    "SYSTEM \"\"\"You are a helpful AI assistant integrated with LIMA (Life Insurance & Managed Accounts) system. Provide accurate, concise answers about insurance and financial products.\"\"\"\n",
    "'''\n",
    "    \n",
    "    modelfile_path = Path(\"./Modelfile\")\n",
    "    modelfile_path.write_text(modelfile_content)\n",
    "    logger.info(f\"  Modelfile created: {modelfile_path}\")\n",
    "    \n",
    "    # Import to Ollama\n",
    "    logger.info(\"  Importing model to Ollama...\")\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"create\", model_name, \"-f\", str(modelfile_path)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        logger.error(f\"Ollama import failed: {result.stderr}\")\n",
    "        raise RuntimeError(f\"Failed to import model to Ollama: {result.stderr}\")\n",
    "    \n",
    "    logger.info(f\"✓ Model '{model_name}' successfully imported to Ollama\")\n",
    "    return model_name\n",
    "\n",
    "try:\n",
    "    # Quantize if requested\n",
    "    if QUANTIZATION and QUANTIZATION != \"none\":\n",
    "        quantized_path = OLLAMA_MODEL_PATH.with_stem(\n",
    "            f\"{OLLAMA_MODEL_PATH.stem}-{QUANTIZATION.lower()}\"\n",
    "        )\n",
    "        final_model_path = quantize_model(\n",
    "            OLLAMA_MODEL_PATH, \n",
    "            quantized_path, \n",
    "            QUANTIZATION, \n",
    "            llama_cpp_path\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Skipping quantization (using full precision model)\")\n",
    "        final_model_path = OLLAMA_MODEL_PATH\n",
    "    \n",
    "    # Import to Ollama\n",
    "    ollama_model_name = create_ollama_model(final_model_path, OUTPUT_MODEL_NAME)\n",
    "    \n",
    "    # Display completion message\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(\"DEPLOYMENT COMPLETE!\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    logger.info(f\"Model Name: {ollama_model_name}\")\n",
    "    logger.info(f\"Model Path: {final_model_path}\")\n",
    "    logger.info(f\"\\nTest with: ollama run {ollama_model_name} \\\"What is life insurance?\\\"\")\n",
    "    logger.info(f\"\\nTo use in LIMA, update .env with:\")\n",
    "    logger.info(f\"  LOCAL_MODEL_NAME={ollama_model_name}\")\n",
    "    logger.info(f\"  LOCAL_MODEL_URL=http://localhost:11434\")\n",
    "    logger.info(f\"  LOCAL_MODEL_TYPE=ollama\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Ollama import failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b425366",
   "metadata": {},
   "source": [
    "## Step 7: Validate Deployment (Optional)\n",
    "\n",
    "Test the deployed model through Ollama to ensure it's working correctly before integrating with LIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e847822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_model(model_name: str, test_prompt: Optional[str] = None):\n",
    "    \"\"\"Test the deployed model with a sample prompt\"\"\"\n",
    "    \n",
    "    if test_prompt is None:\n",
    "        test_prompt = os.getenv(\"TEST_PROMPT\", \"What is life insurance?\")\n",
    "    \n",
    "    logger.info(f\"\\nTesting model: {model_name}\")\n",
    "    logger.info(f\"Test prompt: {test_prompt}\")\n",
    "    logger.info(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name, test_prompt],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            logger.error(f\"Model test failed: {result.stderr}\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(\"Model Response:\")\n",
    "        logger.info(result.stdout)\n",
    "        logger.info(\"-\" * 60)\n",
    "        logger.info(\"✓ Model test successful!\")\n",
    "        return True\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        logger.error(\"Model test timed out after 60 seconds\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run validation test\n",
    "try:\n",
    "    test_success = test_ollama_model(OUTPUT_MODEL_NAME)\n",
    "    \n",
    "    if test_success:\n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"✓ ALL DEPLOYMENT STEPS COMPLETED SUCCESSFULLY\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"\\nNext steps:\")\n",
    "        logger.info(\"1. Update LIMA's .env file with model configuration\")\n",
    "        logger.info(\"2. Restart LIMA services to pick up new model\")\n",
    "        logger.info(\"3. Run LIMA integration tests\")\n",
    "        logger.info(\"4. Monitor model performance in production\")\n",
    "        logger.info(\"\\nFor more details, see: LIMA_INTEGRATION.private.md\")\n",
    "    else:\n",
    "        logger.warning(\"Model test failed. Please check Ollama logs for details.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Validation failed: {str(e)}\")\n",
    "    logger.info(\"You can still use the model, but manual testing is recommended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFT(.trainingEnv)",
   "language": "python",
   "name": ".trainingenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
