{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fe4a68",
   "metadata": {},
   "source": [
    "# README — SFT Data Generation (supervised_fine_tuning)\n",
    "\n",
    "**Purpose:** Generate high-quality SFT training examples from PDF text using a teacher + (optional) auditor pipeline. This notebook supports caching, batching, and configurable concurrency so you can balance speed vs. strict auditing. \n",
    "\n",
    "### Quick start ✅\n",
    "- Install dependencies (local, no docker required):\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- Copy `.env.example` to `.env` and edit values (do NOT commit your `.env`):\n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "# edit .env to match your environment\n",
    "```\n",
    "\n",
    "- Prompts and templates are stored in `config/prompts.json` and are loaded by the notebook at runtime; edit that file to customize system prompts or templates.\n",
    "\n",
    "- `.env` recommendations:\n",
    "\n",
    "```text\n",
    "OLLAMA_URL=http://localhost:11434\n",
    "TEACHER_MODEL=qwen2.5:72b-instruct\n",
    "AUDITOR_MODEL=deepseek-r1:70b\n",
    "# Optional: run Redis for shared cache\n",
    "REDIS_URL=redis://localhost:6379/0\n",
    "MAX_LLM_CONCURRENCY=8\n",
    "USE_SINGLE_CALL=1         # recommended for speed\n",
    "USE_BATCHING=0            # optional: 0/1\n",
    "BATCH_SIZE=4\n",
    "AUDIT_SAMPLE_RATE=0.05    # sample strict audits when using single-call\n",
    "```\n",
    "\n",
    "### Prompts & Templates ✅\n",
    "- Location: `config/prompts.json`.\n",
    "- What it contains: system prompts and small templates used by the pipeline (keys include `system_gen`, `gen_prompt_template`, `audit_system`, `audit_prompt_template`, `single_call_system`, `single_call_prompt_template`, `batch_system`, `batch_block_template`).\n",
    "- How it works: the notebook loads `config/prompts.json` at runtime; if the file is missing the notebook falls back to safe built-in defaults so nothing breaks.\n",
    "- Editing tips:\n",
    "  - Edit `config/prompts.json` with your custom wording and keep values valid JSON.\n",
    "  - Templates use `{chunk}` and `{generated}` placeholders for prompt composition (these are substituted when the notebook runs).\n",
    "  - After editing, re-run the top configuration cells (or restart the kernel and run top cells) to pickup changes.\n",
    "- Example: modify the `system_gen` value to shift the teacher's style or constraints, or adjust `audit_prompt_template` to change strictness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e9363",
   "metadata": {},
   "source": [
    "# --- ORGANIZED ENTRYPOINTS (New ordering) ---\n",
    "# Use the cells that follow as the canonical, ordered implementation. Older, scattered cells remain below for history but **do not** run them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (one-off helper cell)\n",
    "# Run this in the notebook when you need to install packages for this project\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781f1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Configuration (canonical, single cell)\n",
    "import os, sys, json, time, re, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "load_dotenv()\n",
    "\n",
    "# Core endpoints & models\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n",
    "if OLLAMA_URL is None:\n",
    "    raise RuntimeError(\"Error: OLLAMA_URL is not set in the environment variables. Please add OLLAMA_URL to your .env file\")\n",
    "OLLAMA_URL = OLLAMA_URL.rstrip(\"/\")\n",
    "TEACHER_MODEL = os.getenv(\"TEACHER_MODEL\", \"qwen2.5:72b-instruct\")\n",
    "AUDITOR_MODEL = os.getenv(\"AUDITOR_MODEL\", \"deepseek-r1:70b\")\n",
    "\n",
    "# Cache & concurrency defaults\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\") or None\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\", str(Path.cwd() / \"cache\"))\n",
    "CHUNK_TTL = int(os.getenv(\"CHUNK_TTL\", 60 * 60 * 24))\n",
    "SFT_TTL = int(os.getenv(\"SFT_TTL\", 60 * 60 * 24 * 7))\n",
    "MAX_LLM_CONCURRENCY = int(os.getenv(\"MAX_LLM_CONCURRENCY\", 8))\n",
    "USE_SINGLE_CALL = os.getenv(\"USE_SINGLE_CALL\", \"1\") in [\"1\",\"true\",\"True\", True]\n",
    "AUDIT_SAMPLE_RATE = float(os.getenv(\"AUDIT_SAMPLE_RATE\", \"0.05\"))\n",
    "USE_BATCHING = os.getenv(\"USE_BATCHING\", \"0\") in [\"1\",\"true\",\"True\", True]\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"4\"))\n",
    "BATCH_CONCURRENCY = int(os.getenv(\"BATCH_CONCURRENCY\", \"2\"))\n",
    "MAX_BATCH_CHARS = int(os.getenv(\"MAX_BATCH_CHARS\", \"20000\"))\n",
    "# Chunking defaults (chars)\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"2000\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"200\"))\n",
    "# Optional override to force cache backend: 'redis' or 'disk'\n",
    "CACHE_BACKEND = os.getenv(\"CACHE_BACKEND\", \"\").lower()  # set to 'disk' to force DiskCache\n",
    "\n",
    "# Paths\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).parent.resolve()\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd().resolve()\n",
    "RAW_DATA_DIR = SCRIPT_DIR / \"data\" / \"raw\" / \"in-progress\"\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "PROCESSED_DIR = SCRIPT_DIR / \"data\" / \"processed\" / TIMESTAMP\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286cc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts (load or defaults)\n",
    "PROMPTS_PATH = Path(\"config\") / \"prompts.json\"\n",
    "if PROMPTS_PATH.exists():\n",
    "    with PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        PROMPTS = json.load(f)\n",
    "else:\n",
    "    PROMPTS = {\n",
    "        \"system_gen\": \"You are a senior Life Insurance Operations expert... Output ONLY valid JSON in this exact format: {\\\"instruction\\\": \\\"...\\\", \\\"output\\\": \\\"...\\\"}\",\n",
    "        \"gen_prompt_template\": \"Using only the following extract...\\\\n\\\\n{chunk}\",\n",
    "        \"audit_system\": \"You are a meticulous Life Insurance Regulatory Auditor... Output ONLY the final corrected JSON...\",\n",
    "        \"audit_prompt_template\": \"Source Text:\\\\n{chunk}\\\\n\\\\nGenerated Pair:\\\\n{generated}\\\\n\\\\nVerify factual accuracy...\",\n",
    "        \"single_call_system\": \"You are a senior Life Insurance Operations expert and a meticulous auditor... Output ONLY final JSON...\",\n",
    "        \"single_call_prompt_template\": \"Source Text:\\\\n{chunk}\\\\n\\\\nCreate the training example and self-audit it; return only the final JSON.\",\n",
    "        \"batch_system\": \"You are a senior Life Insurance Operations expert and a strict auditor. Return a JSON array or newline-separated JSON objects.\",\n",
    "        \"batch_block_template\": \"--- SOURCE {i} ---\\\\n{chunk}\\\\n\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94849bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache backend init + helper to detect Redis usage\n",
    "try:\n",
    "    import redis\n",
    "except Exception:\n",
    "    redis = None\n",
    "try:\n",
    "    from diskcache import Cache as DiskCache\n",
    "except Exception:\n",
    "    DiskCache = None\n",
    "\n",
    "_cache_client = None\n",
    "\n",
    "def init_cache():\n",
    "    \"\"\"Initialize a cache client. Prefer Redis (when available and healthy),\n",
    "    otherwise fall back to DiskCache. Honours CACHE_BACKEND env override.\n",
    "    \"\"\"\n",
    "    global _cache_client\n",
    "    if _cache_client is not None:\n",
    "        return\n",
    "    _info = globals().get(\"log\", print)\n",
    "\n",
    "    # Force disk backend when requested\n",
    "    if CACHE_BACKEND == \"disk\":\n",
    "        _info(\"CACHE_BACKEND=disk -> forcing DiskCache backend\")\n",
    "        if DiskCache is None:\n",
    "            _info(\"DiskCache not available. Install with `pip install diskcache`\")\n",
    "            raise RuntimeError(\"No cache backend available\")\n",
    "        _cache_client = DiskCache(CACHE_DIR)\n",
    "        _info(f\"Using DiskCache at {CACHE_DIR} (forced by CACHE_BACKEND)\")\n",
    "        return\n",
    "\n",
    "    # Try Redis when configured\n",
    "    if REDIS_URL and redis is not None:\n",
    "        try:\n",
    "            client = redis.from_url(REDIS_URL, socket_connect_timeout=2, socket_timeout=2, decode_responses=True)\n",
    "            # require a ping to verify health\n",
    "            client.ping()\n",
    "            _cache_client = client\n",
    "            _info(f\"Using Redis cache at {REDIS_URL}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            _info(f\"Could not use Redis at {REDIS_URL} ({type(e).__name__}: {e}). Falling back to DiskCache.\")\n",
    "            try:\n",
    "                client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "\n",
    "    # Fall back to DiskCache\n",
    "    if DiskCache is None:\n",
    "        _info(\"DiskCache not available. Install with `pip install diskcache` or set REDIS_URL to a running Redis server.\")\n",
    "        raise RuntimeError(\"No cache backend available\")\n",
    "\n",
    "    _cache_client = DiskCache(CACHE_DIR)\n",
    "    _info(f\"Using DiskCache at {CACHE_DIR}\")\n",
    "\n",
    "\n",
    "def _using_redis():\n",
    "    \"\"\"Return True when the active cache client is Redis-backed.\"\"\"\n",
    "    return bool(REDIS_URL and redis is not None and _cache_client is not None and not isinstance(_cache_client, DiskCache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e14794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing & cache helpers (robust to Redis bytes/str)\n",
    "import hashlib\n",
    "\n",
    "def pdf_sha256(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def chunk_sha256(chunk: str) -> str:\n",
    "    return hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def cache_chunks(pdf_hash: str, chunks, ttl=CHUNK_TTL):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            _cache_client.set(key, json.dumps(chunks, ensure_ascii=False), ex=ttl)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            # on redis failure, fall back to diskcache\n",
    "            print(f\"Redis cache_chunks error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "    # DiskCache path\n",
    "    _cache_client.set(key, chunks, expire=ttl)\n",
    "\n",
    "def get_cached_chunks(pdf_hash: str):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            v = _cache_client.get(key)\n",
    "        except Exception as e:\n",
    "            print(f\"Redis get_cached_chunks error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "            return _cache_client.get(key)\n",
    "        if not v:\n",
    "            return None\n",
    "        try:\n",
    "            return json.loads(v)\n",
    "        except Exception:\n",
    "            return None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "def cache_sft_pair(chunk_hash: str, pair, ttl=SFT_TTL):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            _cache_client.set(key, json.dumps(pair, ensure_ascii=False), ex=ttl)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Redis cache_sft_pair error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "    _cache_client.set(key, pair, expire=ttl)\n",
    "\n",
    "def get_cached_sft_pair(chunk_hash: str):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            v = _cache_client.get(key)\n",
    "        except Exception as e:\n",
    "            print(f\"Redis get_cached_sft_pair error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "            return _cache_client.get(key)\n",
    "        if not v:\n",
    "            return None\n",
    "        try:\n",
    "            return json.loads(v)\n",
    "        except Exception:\n",
    "            return None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "# Text chunking helper\n",
    "\n",
    "def chunk_text_to_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> list[str]:\n",
    "    \"\"\"Split `text` into chunks of approximately `chunk_size` characters with `overlap`.\n",
    "\n",
    "    Strategy:\n",
    "    - Split text into paragraphs on two newlines\n",
    "    - Accumulate paragraphs until adding would exceed chunk_size\n",
    "    - If a single paragraph is larger than chunk_size, split it into slices with overlap\n",
    "    - Return list of chunk strings (stripped)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_len = 0\n",
    "\n",
    "    def flush_current():\n",
    "        nonlocal current, current_len\n",
    "        if current:\n",
    "            chunk = \"\\n\\n\".join(current).strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        current = []\n",
    "        current_len = 0\n",
    "\n",
    "    for p in paragraphs:\n",
    "        p_len = len(p)\n",
    "        if current_len + p_len + (2 if current else 0) <= chunk_size:\n",
    "            current.append(p)\n",
    "            current_len += p_len + (2 if current else 0)\n",
    "        else:\n",
    "            flush_current()\n",
    "            if p_len <= chunk_size:\n",
    "                current.append(p)\n",
    "                current_len = p_len\n",
    "            else:\n",
    "                # paragraph itself is larger than chunk_size; split it\n",
    "                start = 0\n",
    "                while start < p_len:\n",
    "                    end = min(start + chunk_size, p_len)\n",
    "                    slice_ = p[start:end].strip()\n",
    "                    if slice_:\n",
    "                        chunks.append(slice_)\n",
    "                    if end >= p_len:\n",
    "                        break\n",
    "                    start = max(0, end - overlap)\n",
    "    # flush remaining\n",
    "    flush_current()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9642977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP session (pooling & retries) and METRICS\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "\n",
    "SESSION = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(pool_connections=MAX_LLM_CONCURRENCY*2, pool_maxsize=MAX_LLM_CONCURRENCY*2, max_retries=retries)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "METRICS = {\"calls\": []}\n",
    "def record_call(model: str, duration: float, success: bool, error: str | None = None):\n",
    "    METRICS[\"calls\"].append({\"model\": model, \"duration\": duration, \"success\": bool(success), \"error\": str(error) if error else None})\n",
    "def summarise_metrics():\n",
    "    import statistics\n",
    "    by_model = {}\n",
    "    for c in METRICS[\"calls\"]:\n",
    "        m = c[\"model\"]\n",
    "        by_model.setdefault(m, []).append(c)\n",
    "    lines = []\n",
    "    for m, calls in by_model.items():\n",
    "        durations = [c[\"duration\"] for c in calls if c[\"duration\"] is not None]\n",
    "        successes = sum(1 for c in calls if c[\"success\"])\n",
    "        total = len(calls)\n",
    "        mean = statistics.mean(durations) if durations else 0\n",
    "        p95 = sorted(durations)[int(len(durations) * 0.95)] if durations else 0\n",
    "        lines.append(f\"{m}: calls={total} success={successes} mean={mean:.2f}s p95={p95:.2f}s\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67e85c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM call + parsing helpers\n",
    "def call_ollama(model, prompt, system_prompt=\"\", session=None):\n",
    "    session = session or SESSION\n",
    "    url = f\"{OLLAMA_URL}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.1, \"num_predict\": 1024}\n",
    "    }\n",
    "    payload[\"messages\"] = [m for m in payload[\"messages\"] if m is not None]\n",
    "    for attempt in range(3):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = session.post(url, json=payload, timeout=300)\n",
    "            response.raise_for_status()\n",
    "            duration = time.time() - start\n",
    "            data = response.json()\n",
    "            raw_content = data[\"message\"][\"content\"]\n",
    "            content = raw_content.strip()\n",
    "            if content.startswith(\"```json\"):\n",
    "                content = content[7:]\n",
    "            if content.endswith(\"```\"):\n",
    "                content = content[:-3]\n",
    "            content = content.strip()\n",
    "            try:\n",
    "                parsed = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if match:\n",
    "                    parsed = json.loads(match.group(0))\n",
    "                else:\n",
    "                    record_call(model, duration, False, error=\"invalid-json\")\n",
    "                    return None\n",
    "            record_call(model, duration, True)\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            duration = time.time() - start\n",
    "            record_call(model, duration, False, error=str(e))\n",
    "            time.sleep(1)\n",
    "    record_call(model, None, False, error=\"all attempts failed\")\n",
    "    return None\n",
    "\n",
    "def _parse_json_array_or_objects(text: str):\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, dict):\n",
    "            return [parsed]\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "    except Exception:\n",
    "        objs = re.findall(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=re.DOTALL)\n",
    "        results = []\n",
    "        for o in objs:\n",
    "            try:\n",
    "                results.append(json.loads(o))\n",
    "            except Exception:\n",
    "                continue\n",
    "        if results:\n",
    "            return results\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf8758ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate & audit pipelines\n",
    "\n",
    "# PDF text extraction fallbacks (docling -> fitz -> pdfminer)\n",
    "\n",
    "def extract_text_with_fitz(pdf_path: Path):\n",
    "    try:\n",
    "        import fitz\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        texts = []\n",
    "        for p in doc:\n",
    "            texts.append(p.get_text(\"text\"))\n",
    "        return \"\\n\\n\".join(t.strip() for t in texts if t and t.strip())\n",
    "    except Exception as e:\n",
    "        print(\"fitz extraction failed:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text_with_pdfminer(pdf_path: Path):\n",
    "    try:\n",
    "        from pdfminer.high_level import extract_text\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        return extract_text(str(pdf_path))\n",
    "    except Exception as e:\n",
    "        print(\"pdfminer extraction failed:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_markdown_for_pdf(pdf_path: Path, converter=None):\n",
    "    \"\"\"Return markdown text for a PDF by trying docling conversion first, then fallbacks.\n",
    "\n",
    "    Returns empty string if no usable text is found.\n",
    "    \"\"\"\n",
    "    converter = converter or DocumentConverter()\n",
    "    # try docling conversion\n",
    "    try:\n",
    "        result = converter.convert(pdf_path)\n",
    "        md = result.document.export_to_markdown()\n",
    "        if md and len(md.strip()) > 50:\n",
    "            return md\n",
    "        print(\"Docling conversion produced insufficient text; trying fallbacks.\")\n",
    "    except Exception as e:\n",
    "        print(\"Docling conversion failed:\", e)\n",
    "\n",
    "    # try PyMuPDF (fitz)\n",
    "    md = extract_text_with_fitz(pdf_path)\n",
    "    if md and len(md.strip()) > 50:\n",
    "        return md\n",
    "\n",
    "    # try pdfminer\n",
    "    md = extract_text_with_pdfminer(pdf_path)\n",
    "    if md and len(md.strip()) > 50:\n",
    "        return md\n",
    "\n",
    "    print(\"Fallback extractors returned no usable text. Check OCR engines or install fitz/pdfminer.\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def generate_and_audit(chunk):\n",
    "    start = time.time()\n",
    "    system_gen = PROMPTS.get(\"system_gen\")\n",
    "    gen_prompt = PROMPTS.get(\"gen_prompt_template\").format(chunk=chunk)\n",
    "    raw_pair = call_ollama(TEACHER_MODEL, gen_prompt, system_gen)\n",
    "    if not raw_pair:\n",
    "        return None\n",
    "    audit_system = PROMPTS.get(\"audit_system\")\n",
    "    audit_prompt = PROMPTS.get(\"audit_prompt_template\").format(chunk=chunk, generated=json.dumps(raw_pair, indent=2))\n",
    "    final_pair = call_ollama(AUDITOR_MODEL, audit_prompt, audit_system)\n",
    "    record_call(\"pipeline:generate_and_audit\", time.time() - start, True if final_pair else False)\n",
    "    return final_pair\n",
    "\n",
    "def generate_and_audit_single(chunk):\n",
    "    system_prompt = PROMPTS.get(\"single_call_system\")\n",
    "    prompt = PROMPTS.get(\"single_call_prompt_template\").format(chunk=chunk)\n",
    "    start = time.time()\n",
    "    out = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    record_call(\"pipeline:single_generate_and_audit\", time.time() - start, True if out else False)\n",
    "    return out\n",
    "\n",
    "def generate_and_audit_batch(chunks: list[str]):\n",
    "    total_chars = sum(len(c) for c in chunks)\n",
    "    if total_chars > MAX_BATCH_CHARS:\n",
    "        print(\"Batch too large\")\n",
    "        return None\n",
    "    system_prompt = PROMPTS.get(\"batch_system\")\n",
    "    block_template = PROMPTS.get(\"batch_block_template\")\n",
    "    parts = [block_template.format(i=i, chunk=c) for i,c in enumerate(chunks, start=1)]\n",
    "    prompt = \"\\n\".join(parts)\n",
    "    start = time.time()\n",
    "    raw = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    record_call(\"pipeline:batch_generate_and_audit\", time.time() - start, True if raw else False)\n",
    "    if not raw:\n",
    "        return None\n",
    "    if isinstance(raw, (list, dict)):\n",
    "        return raw if isinstance(raw, list) else [raw]\n",
    "    return _parse_json_array_or_objects(str(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe1f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing orchestration (single-call and batching)\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_chunk(chunk: str, idx: int, semaphore: threading.BoundedSemaphore):\n",
    "    chunk_hash = chunk_sha256(chunk)\n",
    "    cached_pair = get_cached_sft_pair(chunk_hash)\n",
    "    if cached_pair:\n",
    "        return idx, json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "    with semaphore:\n",
    "        if USE_BATCHING:\n",
    "            raise RuntimeError(\"process_chunk shouldn't be used in BATCHING mode\")\n",
    "        if USE_SINGLE_CALL:\n",
    "            sft_pair = generate_and_audit_single(chunk)\n",
    "            if sft_pair and random.random() < AUDIT_SAMPLE_RATE:\n",
    "                strict_pair = generate_and_audit(chunk)\n",
    "                if strict_pair:\n",
    "                    sft_pair = strict_pair\n",
    "        else:\n",
    "            sft_pair = generate_and_audit(chunk)\n",
    "    if sft_pair and isinstance(sft_pair, dict) and \"instruction\" in sft_pair:\n",
    "        cache_sft_pair(chunk_hash, sft_pair)\n",
    "        return idx, json.dumps(sft_pair, ensure_ascii=False) + \"\\n\"\n",
    "    return idx, None\n",
    "\n",
    "def process_pdfs(max_chunks_per_pdf: int = None):\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "        md_content = get_markdown_for_pdf(pdf_path, converter)\n",
    "        if not md_content:\n",
    "            print(\"Conversion failed or produced no text for\", pdf_path)\n",
    "            continue\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        chunks = get_cached_chunks(pdf_hash) or chunk_text_to_chunks(md_content)\n",
    "        cache_chunks(pdf_hash, chunks)\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        uncached_indices = [i for i in range(process_count) if get_cached_sft_pair(chunk_sha256(chunks[i])) is None]\n",
    "        results_buffer = [None] * process_count\n",
    "        if uncached_indices:\n",
    "            semaphore = threading.BoundedSemaphore(MAX_LLM_CONCURRENCY)\n",
    "            with ThreadPoolExecutor(max_workers=MAX_LLM_CONCURRENCY) as ex:\n",
    "                futures = {ex.submit(process_chunk, chunks[i], i, semaphore): i for i in uncached_indices}\n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        idx, line = future.result()\n",
    "                    except Exception as e:\n",
    "                        print(\"Chunk job failed\", e)\n",
    "                        continue\n",
    "                    if line:\n",
    "                        results_buffer[idx] = line\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    f.write(line)\n",
    "        print(f\"Saved entries → {output_file}\")\n",
    "\n",
    "def process_pdfs_with_batching(max_chunks_per_pdf: int = None):\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "        md_content = get_markdown_for_pdf(pdf_path, converter)\n",
    "        if not md_content:\n",
    "            print(\"Conversion failed or produced no text for\", pdf_path)\n",
    "            continue\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        chunks = get_cached_chunks(pdf_hash) or chunk_text_to_chunks(md_content)\n",
    "        cache_chunks(pdf_hash, chunks)\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        uncached_indices = [i for i in range(process_count) if get_cached_sft_pair(chunk_sha256(chunks[i])) is None]\n",
    "        batches = []\n",
    "        current = []\n",
    "        current_chars = 0\n",
    "        for idx in uncached_indices:\n",
    "            c = chunks[idx]\n",
    "            if len(current) >= BATCH_SIZE or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "                batches.append(current)\n",
    "                current = []\n",
    "                current_chars = 0\n",
    "            current.append(idx)\n",
    "            current_chars += len(c)\n",
    "        if current:\n",
    "            batches.append(current)\n",
    "        results_buffer = [None] * process_count\n",
    "        with ThreadPoolExecutor(max_workers=BATCH_CONCURRENCY) as executor:\n",
    "            future_to_batch = {}\n",
    "            for batch_idxs in batches:\n",
    "                batch_chunks = [chunks[i] for i in batch_idxs]\n",
    "                future = executor.submit(generate_and_audit_batch, batch_chunks)\n",
    "                future_to_batch[future] = batch_idxs\n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_idxs = future_to_batch[future]\n",
    "                try:\n",
    "                    out_list = future.result()\n",
    "                except Exception as e:\n",
    "                    print(\"Batch failed\", e)\n",
    "                    out_list = None\n",
    "                if out_list and isinstance(out_list, list):\n",
    "                    for idx_in_batch, obj in enumerate(out_list):\n",
    "                        target_idx = batch_idxs[idx_in_batch] if idx_in_batch < len(batch_idxs) else None\n",
    "                        if target_idx is not None and isinstance(obj, dict) and \"instruction\" in obj:\n",
    "                            cache_sft_pair(chunk_sha256(chunks[target_idx]), obj)\n",
    "                            results_buffer[target_idx] = json.dumps(obj, ensure_ascii=False) + \"\\n\"\n",
    "                else:\n",
    "                    print(\"Batch returned invalid output\")\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    f.write(line)\n",
    "        print(f\"Saved entries → {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96630780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autotuner & benchmarks (single, batch)\n",
    "def _reconfigure_session_for_concurrency(concurrency: int):\n",
    "    global SESSION\n",
    "    import requests\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    SESSION = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(pool_connections=concurrency*2, pool_maxsize=concurrency*2, max_retries=retries)\n",
    "    SESSION.mount(\"http://\", adapter)\n",
    "    SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "def benchmark_single_call(chunks, concurrency=1, repeat=1):\n",
    "    if not chunks:\n",
    "        return {\"mode\":\"single_call\",\"concurrency\":concurrency,\"throughput\":0.0,\"total_processed\":0,\"total_time\":0.0}\n",
    "    _reconfigure_session_for_concurrency(concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_single, c) for c in chunks]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        success = sum(1 for r in results if r and isinstance(r, dict) and \"instruction\" in r)\n",
    "        return duration, success\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(s for _, s in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\": \"single_call\", \"concurrency\": concurrency, \"throughput\": throughput, \"total_processed\": total_processed, \"total_time\": total_time}\n",
    "\n",
    "def benchmark_batch(chunks, batch_size=4, batch_concurrency=1, repeat=1):\n",
    "    \"\"\"Benchmark batching pipeline using generate_and_audit_batch and return throughput.\"\"\"\n",
    "    if not chunks:\n",
    "        return {\"mode\":\"batch\",\"batch_size\":batch_size,\"batch_concurrency\":batch_concurrency,\"throughput\":0.0,\"total_processed\":0,\"total_time\":0.0}\n",
    "\n",
    "    # Build batches respecting batch_size and MAX_BATCH_CHARS\n",
    "    batches = []\n",
    "    current = []\n",
    "    current_chars = 0\n",
    "    for c in chunks:\n",
    "        if len(current) >= batch_size or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "            batches.append(current)\n",
    "            current = []\n",
    "            current_chars = 0\n",
    "        current.append(c)\n",
    "        current_chars += len(c)\n",
    "    if current:\n",
    "        batches.append(current)\n",
    "\n",
    "    _reconfigure_session_for_concurrency(batch_concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=batch_concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_batch, b) for b in batches]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        processed = 0\n",
    "        for res in results:\n",
    "            if isinstance(res, list):\n",
    "                processed += len(res)\n",
    "            elif isinstance(res, dict):\n",
    "                processed += 1\n",
    "        return duration, processed\n",
    "\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(p for _, p in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\":\"batch\",\"batch_size\":batch_size,\"batch_concurrency\":batch_concurrency,\"throughput\":throughput,\"total_processed\":total_processed,\"total_time\":total_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b1320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostics:\n",
      "ThreadPoolExecutor: <class 'concurrent.futures.thread.ThreadPoolExecutor'>\n",
      "CACHE_BACKEND=disk -> forcing DiskCache backend\n",
      "Using DiskCache at /home/rahul/dev/sft-data-gen/cache (forced by CACHE_BACKEND)\n",
      "Using Redis: False\n",
      "Cache client type: <class 'diskcache.core.Cache'>\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics cell (run immediately after imports if anything seems off)\n",
    "print(\"Diagnostics:\")\n",
    "print(\"ThreadPoolExecutor:\", ThreadPoolExecutor)\n",
    "init_cache()\n",
    "print(\"Using Redis:\", _using_redis())\n",
    "print(\"Cache client type:\", type(_cache_client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01547050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook loaded. Call run_all() to start processing.\n"
     ]
    }
   ],
   "source": [
    "# Main run helper and dry-run example\n",
    "def run_all(max_chunks_per_pdf: int = None):\n",
    "    if USE_BATCHING:\n",
    "        process_pdfs_with_batching(max_chunks_per_pdf)\n",
    "    else:\n",
    "        process_pdfs(max_chunks_per_pdf)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: run_all() or keep for CLI usage in notebooks\n",
    "    print(\"Notebook loaded. Call run_all() to start processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d3f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-04 01:03:26,622 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dry-run using: /home/rahul/dev/sft-data-gen/data/raw/in-progress/accelerated-protection-combined-pds.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 01:03:27,407 - INFO - Going to convert document batch...\n",
      "2026-01-04 01:03:27,409 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-04 01:03:27,422 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-04 01:03:27,430 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-04 01:03:27,450 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-04 01:03:27,456 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-04 01:03:27,475 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2026-01-04 01:03:27,478 - INFO - easyocr cannot be used because it is not installed.\n",
      "2026-01-04 01:03:28,203 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:28,277 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:28,288 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:28,403 [RapidOCR] download_file.py:60: File exists and is valid: /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:28,405 [RapidOCR] main.py:50: Using /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:29,667 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:29,668 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:29,675 [RapidOCR] download_file.py:60: File exists and is valid: /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:29,677 [RapidOCR] main.py:50: Using /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:29,964 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:29,967 [RapidOCR] device_config.py:57: Using GPU device with ID: 0\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:30,205 [RapidOCR] download_file.py:60: File exists and is valid: /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-04 01:03:30,207 [RapidOCR] main.py:50: Using /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2026-01-04 01:03:30,828 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2026-01-04 01:03:30,842 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-04 01:03:30,846 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-04 01:03:30,857 - INFO - Accelerator device: 'cuda:0'\n",
      "2026-01-04 01:03:32,070 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-04 01:03:32,073 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-04 01:03:32,331 - INFO - Accelerator device: 'cuda:0'\n",
      "2026-01-04 01:03:33,314 - INFO - Processing document accelerated-protection-combined-pds.pdf\n",
      "2026-01-04 01:03:41,495 - WARNING - RapidOCR returned empty result!\n"
     ]
    }
   ],
   "source": [
    "# Dry-run helper (process entire first PDF concurrently; writes .dryrun.jsonl to processed dir)\n",
    "try:\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    from tqdm import tqdm\n",
    "    import threading\n",
    "\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found for dry-run.\")\n",
    "    else:\n",
    "        pdf_path = pdf_files[0]\n",
    "        print(\"Dry-run using:\", pdf_path)\n",
    "        md = get_markdown_for_pdf(pdf_path, converter)\n",
    "        if not md:\n",
    "            print(\"Dry-run: no text extracted for\", pdf_path)\n",
    "        else:\n",
    "            chunks = chunk_text_to_chunks(md)\n",
    "            n_chunks = len(chunks)\n",
    "            print(f\"PDF produced {n_chunks} chunks\")\n",
    "            output_file = PROCESSED_DIR / f\"{pdf_path.stem}.dryrun.jsonl\"\n",
    "\n",
    "            # concurrency bounded by MAX_LLM_CONCURRENCY and number of chunks\n",
    "            concurrency = min(MAX_LLM_CONCURRENCY, max(1, n_chunks))\n",
    "            semaphore = threading.BoundedSemaphore(concurrency)\n",
    "            results_buffer = [None] * n_chunks\n",
    "            processed = 0\n",
    "            skipped = 0\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "                future_to_idx = {ex.submit(process_chunk, chunks[i], i, semaphore): i for i in range(n_chunks)}\n",
    "                for fut in tqdm(as_completed(future_to_idx), total=n_chunks, desc=\"Dry-run chunks\"):\n",
    "                    try:\n",
    "                        idx, line = fut.result()\n",
    "                    except Exception as e:\n",
    "                        print(\"Chunk job failed:\", e)\n",
    "                        continue\n",
    "                    if line:\n",
    "                        results_buffer[idx] = line\n",
    "                        processed += 1\n",
    "                    else:\n",
    "                        cached = get_cached_sft_pair(chunk_sha256(chunks[idx]))\n",
    "                        if cached:\n",
    "                            results_buffer[idx] = json.dumps(cached, ensure_ascii=False) + \"\\n\"\n",
    "                            skipped += 1\n",
    "\n",
    "            # Write results in order\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "                for line in results_buffer:\n",
    "                    if line:\n",
    "                        out_f.write(line)\n",
    "\n",
    "            print(f\"Dry-run complete: processed={processed} skipped_cached={skipped} written→{output_file}\")\n",
    "except Exception as e:\n",
    "    print(\"Dry-run skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be75c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sftEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
