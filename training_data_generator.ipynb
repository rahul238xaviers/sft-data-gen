{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fe4a68",
   "metadata": {},
   "source": [
    "# README ‚Äî SFT Data Generation (supervised_fine_tuning)\n",
    "\n",
    "**Purpose:** Generate high-quality SFT training examples from PDF text using a teacher + (optional) auditor pipeline. This notebook supports caching, batching, and configurable concurrency so you can balance speed vs. strict auditing. \n",
    "\n",
    "### Quick start ‚úÖ\n",
    "- Install dependencies (local, no docker required):\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- Copy `.env.example` to `.env` and edit values (do NOT commit your `.env`):\n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "# edit .env to match your environment\n",
    "```\n",
    "\n",
    "- Prompts and templates are loaded at runtime. By default the notebook reads `config/prompts.json`, but you can override this using the environment variable `SFT_PROMPTS_PATH` (for example: `SFT_PROMPTS_PATH=config/prompt.local.json` in your local `.env`). Do **not** commit your private prompts to the repository; use `config/prompts.example.json` as the editable, generic illustration.\n",
    "\n",
    "- `.env` recommendations:\n",
    "\n",
    "```text\n",
    "OLLAMA_URL=http://localhost:11434\n",
    "TEACHER_MODEL=qwen2.5:72b-instruct\n",
    "AUDITOR_MODEL=deepseek-r1:70b\n",
    "# Optional: run Redis for shared cache\n",
    "REDIS_URL=redis://localhost:6379/0\n",
    "MAX_LLM_CONCURRENCY=8\n",
    "USE_SINGLE_CALL=1         # recommended for speed\n",
    "USE_BATCHING=0            # optional: 0/1\n",
    "BATCH_SIZE=4\n",
    "AUDIT_SAMPLE_RATE=0.05    # sample strict audits when using single-call\n",
    "# Optional: override prompts file for local development\n",
    "# SFT_PROMPTS_PATH=config/prompt.local.json\n",
    "```\n",
    "\n",
    "### Prompts & Templates ‚úÖ\n",
    "- Location: `config/prompts.json` by default (override with `SFT_PROMPTS_PATH`).\n",
    "- What it contains: system prompts and small templates used by the pipeline (keys include `system_gen`, `gen_prompt_template`, `audit_system`, `audit_prompt_template`, `single_call_system`, `single_call_prompt_template`, `batch_system`, `batch_block_template`).\n",
    "- How it works: the notebook will first attempt to load the file at `SFT_PROMPTS_PATH` (if set) or `config/prompts.json`; if missing it will prefer `config/prompts.example.json` (editable example), and finally fall back to well‚Äëtested built-in defaults.\n",
    "- Editing tips:\n",
    "  - Copy `config/prompts.example.json` to `config/prompts.json` or create a `config/prompt.local.json` for local changes and set `SFT_PROMPTS_PATH` to point to it.\n",
    "  - **Do not** commit private prompt files ‚Äî add them to `.gitignore` (this repo already ignores `config/prompts.json` and `config/prompt.local.json`).\n",
    "  - Templates use `{chunk}` and `{generated}` placeholders for prompt composition (these are substituted when the notebook runs).\n",
    "  - After editing, re-run the top configuration cells (or restart the kernel and run top cells) to pick up changes.\n",
    "- Example: modify the `system_gen` value to shift the teacher's style or constraints, or adjust `audit_prompt_template` to change strictness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26199eae",
   "metadata": {},
   "source": [
    "## Install dependencies (helper) ‚öôÔ∏è\n",
    "\n",
    "**Purpose:** Install required Python packages for this project. Run this cell when setting up the environment or when `requirements.txt` changes.\n",
    "\n",
    "**Usage:** `!pip install -r requirements.txt` ‚Äî run once per environment. Avoid running in CI; prefer reproducible environments or lockfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (one-off helper cell)\n",
    "# Run this in the notebook when you need to install packages for this project\n",
    "# Preferably in a virtual environment. create a virtual environment using:\n",
    "# python3 -m venv .venv\n",
    "# source .venv/bin/activate\n",
    "#python -m ipykernel install --user --name .venv --display-name \"SFT Data Gen (.venv)\"\n",
    "\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72049703",
   "metadata": {},
   "source": [
    "## Imports & Configuration (overview) üîß\n",
    "\n",
    "**Purpose:** Load environment variables, set default configuration values, and establish paths used throughout the notebook (e.g., `OLLAMA_URL`, cache settings, chunking constants and data directories).\n",
    "\n",
    "**Usage:** Run this cell first after starting the kernel so all subsequent helper functions have access to these constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Configuration (canonical, single cell)\n",
    "import os, sys, json, time, re, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "load_dotenv()\n",
    "\n",
    "# Core endpoints & models\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n",
    "if OLLAMA_URL is None:\n",
    "    raise RuntimeError(\"Error: OLLAMA_URL is not set in the environment variables. Please add OLLAMA_URL to your .env file\")\n",
    "OLLAMA_URL = OLLAMA_URL.rstrip(\"/\")\n",
    "TEACHER_MODEL = os.getenv(\"TEACHER_MODEL\", \"qwen2.5:72b-instruct\")\n",
    "AUDITOR_MODEL = os.getenv(\"AUDITOR_MODEL\", \"deepseek-r1:70b\")\n",
    "\n",
    "# Cache & concurrency defaults\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\") or None\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\", str(Path.cwd() / \"cache\"))\n",
    "CHUNK_TTL = int(os.getenv(\"CHUNK_TTL\", 60 * 60 * 24))\n",
    "SFT_TTL = int(os.getenv(\"SFT_TTL\", 60 * 60 * 24 * 7))\n",
    "MAX_LLM_CONCURRENCY = int(os.getenv(\"MAX_LLM_CONCURRENCY\", 8))\n",
    "USE_SINGLE_CALL = os.getenv(\"USE_SINGLE_CALL\", \"1\") in [\"1\",\"true\",\"True\", True]\n",
    "AUDIT_SAMPLE_RATE = float(os.getenv(\"AUDIT_SAMPLE_RATE\", \"0.05\"))\n",
    "USE_BATCHING = os.getenv(\"USE_BATCHING\", \"0\") in [\"1\",\"true\",\"True\", True]\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"4\"))\n",
    "BATCH_CONCURRENCY = int(os.getenv(\"BATCH_CONCURRENCY\", \"2\"))\n",
    "MAX_BATCH_CHARS = int(os.getenv(\"MAX_BATCH_CHARS\", \"20000\"))\n",
    "# Chunking defaults (chars)\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"2000\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"200\"))\n",
    "# Optional override to force cache backend: 'redis' or 'disk'\n",
    "CACHE_BACKEND = os.getenv(\"CACHE_BACKEND\", \"\").lower()  # set to 'disk' to force DiskCache\n",
    "\n",
    "# Paths\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).parent.resolve()\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd().resolve()\n",
    "RAW_DATA_DIR = SCRIPT_DIR / \"data\" / \"raw\" / \"in-progress\"\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "PROCESSED_DIR = SCRIPT_DIR / \"data\" / \"processed\" / TIMESTAMP\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf95d7",
   "metadata": {},
   "source": [
    "## Prompts loader & defaults üßæ\n",
    "\n",
    "\n",
    "**Purpose:** The prompt file is to instruct model to generate as well as audit data where objective of dataset is defined. \n",
    "\n",
    "**Action:** Create a prompt templates in `config/` folder `prompt.local.json`. An illustrative example is provided `prompts.example.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts (load or defaults)\n",
    "# Respect SFT_PROMPTS_PATH if set and non-empty; otherwise use config/prompts.json\n",
    "_env_path = os.environ.get(\"SFT_PROMPTS_PATH\")\n",
    "PROMPTS_PATH = Path(_env_path) if _env_path else Path(\"config\") / \"prompts.json\"\n",
    "# If an environment override is provided (non-empty), surface it so users know where prompts are coming from\n",
    "if _env_path:\n",
    "    print(f\"SFT_PROMPTS_PATH set -> loading prompts from {PROMPTS_PATH}\")\n",
    "\n",
    "PROMPTS = None\n",
    "# Only load when PROMPTS_PATH is a file (guards against empty env -> current dir)\n",
    "if PROMPTS_PATH.is_file():\n",
    "    try:\n",
    "        with PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            PROMPTS = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read SFT prompts file {PROMPTS_PATH}: {e}. Falling back to example/defaults.\")\n",
    "\n",
    "if PROMPTS is None:\n",
    "    # Prefer an example prompts file if present (editable and safe to commit)\n",
    "    EXAMPLE_PATH = Path(\"config\") / \"prompts.example.json\"\n",
    "    if EXAMPLE_PATH.is_file():\n",
    "        with EXAMPLE_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            PROMPTS = json.load(f)\n",
    "        print(f\"Loaded prompts from example: {EXAMPLE_PATH}\")\n",
    "    else:\n",
    "        # Built-in safe defaults (generic templates) ‚Äî edit `config/prompts.example.json` to customize\n",
    "        PROMPTS = {\n",
    "            \"system_gen\": \"You are an expert assistant. Create ONE high-quality supervised fine-tuning example based only on the provided source text. Output ONLY valid JSON in this exact format: {\\\"instruction\\\": \\\"A clear task\\\", \\\"output\\\": \\\"A precise response\\\"}\",\n",
    "            \"gen_prompt_template\": \"Using only the following source text, create one high-quality training example for fine-tuning:\\\\n\\\\n{chunk}\",\n",
    "            \"audit_system\": \"You are a meticulous auditor. Verify that the generated instruction-output pair is factually accurate and faithful to the source text. Correct any errors and return only the final valid JSON.\",\n",
    "            \"audit_prompt_template\": \"Source Text:\\\\n{chunk}\\\\n\\\\nGenerated Pair:\\\\n{generated}\\\\n\\\\nVerify factual accuracy against the source and return only the corrected JSON.\",\n",
    "            \"single_call_system\": \"You are an expert assistant and auditor. Create and self-audit one high-quality example; output only the final JSON.\",\n",
    "            \"single_call_prompt_template\": \"Source Text:\\\\n{chunk}\\\\n\\\\nCreate the training example and self-audit it; return only the final JSON.\",\n",
    "            \"batch_system\": \"For each SOURCE block, create one high-quality example and ensure it is fact-checked. Return a JSON array or newline-separated JSON objects.\",\n",
    "            \"batch_block_template\": \"--- SOURCE {i} ---\\\\n{chunk}\\\\n\"\n",
    "        }\n",
    "        print(\"Loaded built-in prompt defaults\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648aa166",
   "metadata": {},
   "source": [
    "## Cache backend initialization üíæ\n",
    "\n",
    "**Purpose:** Initialize the caching client. Prefers Redis when available and healthy, otherwise falls back to DiskCache. Also provides `_using_redis()` for backend checks.\n",
    "\n",
    "**Notes:** Install `redis` or `diskcache` packages in your environment if you want the respective backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94849bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache backend init + helper to detect Redis usage\n",
    "try:\n",
    "    import redis\n",
    "except Exception:\n",
    "    redis = None\n",
    "try:\n",
    "    from diskcache import Cache as DiskCache\n",
    "except Exception:\n",
    "    DiskCache = None\n",
    "\n",
    "_cache_client = None\n",
    "\n",
    "def init_cache():\n",
    "    \"\"\"Initialize a cache client. Prefer Redis (when available and healthy),\n",
    "    otherwise fall back to DiskCache. Honours CACHE_BACKEND env override.\n",
    "    \"\"\"\n",
    "    global _cache_client\n",
    "    if _cache_client is not None:\n",
    "        return\n",
    "    _info = globals().get(\"log\", print)\n",
    "\n",
    "    # Force disk backend when requested\n",
    "    if CACHE_BACKEND == \"disk\":\n",
    "        _info(\"CACHE_BACKEND=disk -> forcing DiskCache backend\")\n",
    "        if DiskCache is None:\n",
    "            _info(\"DiskCache not available. Install with `pip install diskcache`\")\n",
    "            raise RuntimeError(\"No cache backend available\")\n",
    "        _cache_client = DiskCache(CACHE_DIR)\n",
    "        _info(f\"Using DiskCache at {CACHE_DIR} (forced by CACHE_BACKEND)\")\n",
    "        return\n",
    "\n",
    "    # Try Redis when configured\n",
    "    if REDIS_URL and redis is not None:\n",
    "        try:\n",
    "            client = redis.from_url(REDIS_URL, socket_connect_timeout=2, socket_timeout=2, decode_responses=True)\n",
    "            # require a ping to verify health\n",
    "            client.ping()\n",
    "            _cache_client = client\n",
    "            _info(f\"Using Redis cache at {REDIS_URL}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            _info(f\"Could not use Redis at {REDIS_URL} ({type(e).__name__}: {e}). Falling back to DiskCache.\")\n",
    "            try:\n",
    "                client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "\n",
    "    # Fall back to DiskCache\n",
    "    if DiskCache is None:\n",
    "        _info(\"DiskCache not available. Install with `pip install diskcache` or set REDIS_URL to a running Redis server.\")\n",
    "        raise RuntimeError(\"No cache backend available\")\n",
    "\n",
    "    _cache_client = DiskCache(CACHE_DIR)\n",
    "    _info(f\"Using DiskCache at {CACHE_DIR}\")\n",
    "\n",
    "\n",
    "def _using_redis():\n",
    "    \"\"\"Return True when the active cache client is Redis-backed.\"\"\"\n",
    "    return bool(REDIS_URL and redis is not None and _cache_client is not None and not isinstance(_cache_client, DiskCache))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79536a",
   "metadata": {},
   "source": [
    "## Hashing, caching helpers & chunking üß©\n",
    "\n",
    "**Purpose:** Provide utilities to compute PDF and chunk hashes (`pdf_sha256`, `chunk_sha256`), store and retrieve cached chunks/SFT pairs, and split long text into reasonable chunks using paragraph-aware logic.\n",
    "\n",
    "**Usage:** These helpers are used by the processing pipeline to avoid reprocessing work and ensure chunk stability across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e14794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing & cache helpers (robust to Redis bytes/str)\n",
    "import hashlib\n",
    "\n",
    "def pdf_sha256(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def chunk_sha256(chunk: str) -> str:\n",
    "    return hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def cache_chunks(pdf_hash: str, chunks, ttl=CHUNK_TTL):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            _cache_client.set(key, json.dumps(chunks, ensure_ascii=False), ex=ttl)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            # on redis failure, fall back to diskcache\n",
    "            print(f\"Redis cache_chunks error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "    # DiskCache path\n",
    "    _cache_client.set(key, chunks, expire=ttl)\n",
    "\n",
    "def get_cached_chunks(pdf_hash: str):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            v = _cache_client.get(key)\n",
    "        except Exception as e:\n",
    "            print(f\"Redis get_cached_chunks error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "            return _cache_client.get(key)\n",
    "        if not v:\n",
    "            return None\n",
    "        try:\n",
    "            return json.loads(v)\n",
    "        except Exception:\n",
    "            return None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "def cache_sft_pair(chunk_hash: str, pair, ttl=SFT_TTL):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            _cache_client.set(key, json.dumps(pair, ensure_ascii=False), ex=ttl)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Redis cache_sft_pair error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "    _cache_client.set(key, pair, expire=ttl)\n",
    "\n",
    "def get_cached_sft_pair(chunk_hash: str):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            v = _cache_client.get(key)\n",
    "        except Exception as e:\n",
    "            print(f\"Redis get_cached_sft_pair error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "            return _cache_client.get(key)\n",
    "        if not v:\n",
    "            return None\n",
    "        try:\n",
    "            return json.loads(v)\n",
    "        except Exception:\n",
    "            return None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "# Text chunking helper\n",
    "\n",
    "def chunk_text_to_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> list[str]:\n",
    "    \"\"\"Split `text` into chunks of approximately `chunk_size` characters with `overlap`.\n",
    "\n",
    "    Strategy:\n",
    "    - Split text into paragraphs on two newlines\n",
    "    - Accumulate paragraphs until adding would exceed chunk_size\n",
    "    - If a single paragraph is larger than chunk_size, split it into slices with overlap\n",
    "    - Return list of chunk strings (stripped)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_len = 0\n",
    "\n",
    "    def flush_current():\n",
    "        nonlocal current, current_len\n",
    "        if current:\n",
    "            chunk = \"\\n\\n\".join(current).strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        current = []\n",
    "        current_len = 0\n",
    "\n",
    "    for p in paragraphs:\n",
    "        p_len = len(p)\n",
    "        if current_len + p_len + (2 if current else 0) <= chunk_size:\n",
    "            current.append(p)\n",
    "            current_len += p_len + (2 if current else 0)\n",
    "        else:\n",
    "            flush_current()\n",
    "            if p_len <= chunk_size:\n",
    "                current.append(p)\n",
    "                current_len = p_len\n",
    "            else:\n",
    "                # paragraph itself is larger than chunk_size; split it\n",
    "                start = 0\n",
    "                while start < p_len:\n",
    "                    end = min(start + chunk_size, p_len)\n",
    "                    slice_ = p[start:end].strip()\n",
    "                    if slice_:\n",
    "                        chunks.append(slice_)\n",
    "                    if end >= p_len:\n",
    "                        break\n",
    "                    start = max(0, end - overlap)\n",
    "    # flush remaining\n",
    "    flush_current()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270e769",
   "metadata": {},
   "source": [
    "## HTTP session pooling & METRICS üì°\n",
    "\n",
    "**Purpose:** Set up a shared `requests.Session` with retries and connection pooling to reuse HTTP connections for Ollama calls. Also provides `record_call` and `summarise_metrics` for basic instrumentation and debugging.\n",
    "\n",
    "**Usage:** Reconfigure session concurrency via `_reconfigure_session_for_concurrency()` when benchmarking or autotuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9642977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP session (pooling & retries) and METRICS\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "\n",
    "SESSION = requests.Session()\n",
    "OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\")\n",
    "if OLLAMA_API_KEY:\n",
    "    SESSION.headers.update({\"Authorization\": f\"Bearer {OLLAMA_API_KEY}\"})\n",
    "retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(pool_connections=MAX_LLM_CONCURRENCY*2, pool_maxsize=MAX_LLM_CONCURRENCY*2, max_retries=retries)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "METRICS = {\"calls\": []}\n",
    "def record_call(model: str, duration: float, success: bool, error: str | None = None):\n",
    "    METRICS[\"calls\"].append({\"model\": model, \"duration\": duration, \"success\": bool(success), \"error\": str(error) if error else None})\n",
    "def summarise_metrics():\n",
    "    import statistics\n",
    "    by_model = {}\n",
    "    for c in METRICS[\"calls\"]:\n",
    "        m = c[\"model\"]\n",
    "        by_model.setdefault(m, []).append(c)\n",
    "    lines = []\n",
    "    for m, calls in by_model.items():\n",
    "        durations = [c[\"duration\"] for c in calls if c[\"duration\"] is not None]\n",
    "        successes = sum(1 for c in calls if c[\"success\"])\n",
    "        total = len(calls)\n",
    "        mean = statistics.mean(durations) if durations else 0\n",
    "        p95 = sorted(durations)[int(len(durations) * 0.95)] if durations else 0\n",
    "        lines.append(f\"{m}: calls={total} success={successes} mean={mean:.2f}s p95={p95:.2f}s\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c74c44",
   "metadata": {},
   "source": [
    "## LLM call wrapper & JSON parsing ü§ñ\n",
    "\n",
    "**Purpose:** `call_ollama` performs robust calls to the Ollama API (with retries and timeout), strips surrounding markdown fences, and attempts to parse JSON. `_parse_json_array_or_objects` extracts JSON objects from potentially messy outputs.\n",
    "\n",
    "**Notes:** The helper records metrics and returns `None` on repeated failures; callers should handle `None` results accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e85c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM call + parsing helpers\n",
    "def call_ollama(model, prompt, system_prompt=\"\", session=None):\n",
    "    session = session or SESSION\n",
    "    url = f\"{OLLAMA_URL}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.1, \"num_predict\": 1024}\n",
    "    }\n",
    "    payload[\"messages\"] = [m for m in payload[\"messages\"] if m is not None]\n",
    "    for attempt in range(3):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = session.post(url, json=payload, timeout=300)\n",
    "            response.raise_for_status()\n",
    "            duration = time.time() - start\n",
    "            data = response.json()\n",
    "            raw_content = data[\"message\"][\"content\"]\n",
    "            print(\"call_ollama: raw response (truncated):\", raw_content[:2000])\n",
    "            content = raw_content.strip()\n",
    "            if content.startswith(\"```json\"):\n",
    "                content = content[7:]\n",
    "            if content.endswith(\"```\"):\n",
    "                content = content[:-3]\n",
    "            content = content.strip()\n",
    "            try:\n",
    "                parsed = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if match:\n",
    "                    parsed = json.loads(match.group(0))\n",
    "                else:\n",
    "                    record_call(model, duration, False, error=\"invalid-json\")\n",
    "                    return None\n",
    "            record_call(model, duration, True)\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            duration = time.time() - start\n",
    "            record_call(model, duration, False, error=str(e))\n",
    "            time.sleep(1)\n",
    "    record_call(model, None, False, error=\"all attempts failed\")\n",
    "    return None\n",
    "\n",
    "def _parse_json_array_or_objects(text: str):\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, dict):\n",
    "            return [parsed]\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "    except Exception:\n",
    "        objs = re.findall(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=re.DOTALL)\n",
    "        results = []\n",
    "        for o in objs:\n",
    "            try:\n",
    "                results.append(json.loads(o))\n",
    "            except Exception:\n",
    "                continue\n",
    "        if results:\n",
    "            return results\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61298c32",
   "metadata": {},
   "source": [
    "## Generation & Auditing Pipelines üß™\n",
    "\n",
    "**Purpose:** Provide three generation modes:\n",
    "- `generate_and_audit`: two-step teacher ‚Üí auditor (strict accuracy)\n",
    "- `generate_and_audit_single`: single-call with self-audit (faster)\n",
    "- `generate_and_audit_batch`: batch multiple chunks into one request\n",
    "\n",
    "**Also includes:** PDF extraction helpers with fallbacks (`docling`, `fitz`, `pdfminer`) to ensure robust text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8758ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate & audit pipelines\n",
    "\n",
    "# PDF text extraction fallbacks (docling -> fitz -> pdfminer)\n",
    "\n",
    "def extract_text_with_fitz(pdf_path: Path):\n",
    "    try:\n",
    "        import fitz\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        texts = []\n",
    "        for p in doc:\n",
    "            texts.append(p.get_text(\"text\"))\n",
    "        return \"\\n\\n\".join(t.strip() for t in texts if t and t.strip())\n",
    "    except Exception as e:\n",
    "        print(\"fitz extraction failed:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text_with_pdfminer(pdf_path: Path):\n",
    "    try:\n",
    "        from pdfminer.high_level import extract_text\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        return extract_text(str(pdf_path))\n",
    "    except Exception as e:\n",
    "        print(\"pdfminer extraction failed:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_markdown_for_pdf(pdf_path: Path, converter=None):\n",
    "    \"\"\"Return markdown text for a PDF by trying docling conversion first, then fallbacks.\n",
    "\n",
    "    Returns empty string if no usable text is found.\n",
    "    \"\"\"\n",
    "    converter = converter or DocumentConverter()\n",
    "    # try docling conversion\n",
    "    try:\n",
    "        result = converter.convert(pdf_path)\n",
    "        md = result.document.export_to_markdown()\n",
    "        if md and len(md.strip()) > 50:\n",
    "            return md\n",
    "        print(\"Docling conversion produced insufficient text; trying fallbacks.\")\n",
    "    except Exception as e:\n",
    "        print(\"Docling conversion failed:\", e)\n",
    "\n",
    "    # try PyMuPDF (fitz)\n",
    "    md = extract_text_with_fitz(pdf_path)\n",
    "    if md and len(md.strip()) > 50:\n",
    "        return md\n",
    "\n",
    "    # try pdfminer\n",
    "    md = extract_text_with_pdfminer(pdf_path)\n",
    "    if md and len(md.strip()) > 50:\n",
    "        return md\n",
    "\n",
    "    print(\"Fallback extractors returned no usable text. Check OCR engines or install fitz/pdfminer.\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def generate_and_audit(chunk):\n",
    "    start = time.time()\n",
    "    system_gen = PROMPTS.get(\"system_gen\")\n",
    "    gen_prompt = PROMPTS.get(\"gen_prompt_template\").format(chunk=chunk)\n",
    "    raw_pair = call_ollama(TEACHER_MODEL, gen_prompt, system_gen)\n",
    "    if not raw_pair:\n",
    "        return None\n",
    "    audit_system = PROMPTS.get(\"audit_system\")\n",
    "    audit_prompt = PROMPTS.get(\"audit_prompt_template\").format(chunk=chunk, generated=json.dumps(raw_pair, indent=2))\n",
    "    final_pair = call_ollama(AUDITOR_MODEL, audit_prompt, audit_system)\n",
    "    record_call(\"pipeline:generate_and_audit\", time.time() - start, True if final_pair else False)\n",
    "    return final_pair\n",
    "\n",
    "def generate_and_audit_single(chunk):\n",
    "    system_prompt = PROMPTS.get(\"single_call_system\")\n",
    "    prompt = PROMPTS.get(\"single_call_prompt_template\").format(chunk=chunk)\n",
    "    start = time.time()\n",
    "    out = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    record_call(\"pipeline:single_generate_and_audit\", time.time() - start, True if out else False)\n",
    "    return out\n",
    "\n",
    "def generate_and_audit_batch(chunks: list[str]):\n",
    "    total_chars = sum(len(c) for c in chunks)\n",
    "    if total_chars > MAX_BATCH_CHARS:\n",
    "        print(\"Batch too large\")\n",
    "        return None\n",
    "    system_prompt = PROMPTS.get(\"batch_system\")\n",
    "    block_template = PROMPTS.get(\"batch_block_template\")\n",
    "    parts = [block_template.format(i=i, chunk=c) for i,c in enumerate(chunks, start=1)]\n",
    "    prompt = \"\\n\".join(parts)\n",
    "    start = time.time()\n",
    "    raw = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    record_call(\"pipeline:batch_generate_and_audit\", time.time() - start, True if raw else False)\n",
    "    if not raw:\n",
    "        raise RuntimeError(\"generate_and_audit_batch: call_ollama returned no result\")\n",
    "    if isinstance(raw, (list, dict)):\n",
    "        return raw if isinstance(raw, list) else [raw]\n",
    "    parsed = _parse_json_array_or_objects(str(raw))\n",
    "\n",
    "    if parsed is None:\n",
    "        # Log truncated raw content for debugging and raise so callers see a traceback\n",
    "        print(\"generate_and_audit_batch: could not parse LLM response. Raw response (truncated):\", repr(raw)[:2000])\n",
    "        raise ValueError(f\"generate_and_audit_batch: could not parse LLM response: {repr(raw)[:2000]}\")\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51a9b4",
   "metadata": {},
   "source": [
    "## Processing orchestration & helpers ‚öôÔ∏è\n",
    "\n",
    "**Purpose:** Orchestrate end-to-end processing across PDFs: `process_chunk`, `process_pdfs` (single-call or two-step), and `process_pdfs_with_batching`. Manages concurrency, caching lookups, and writes results in deterministic order.\n",
    "\n",
    "**Usage:** Use `process_pdfs(max_chunks_per_pdf)` for regular runs or the batching variant when `USE_BATCHING` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing orchestration (single-call and batching)\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import shutil\n",
    "\n",
    "def process_chunk(chunk: str, idx: int, semaphore: threading.BoundedSemaphore):\n",
    "    chunk_hash = chunk_sha256(chunk)\n",
    "    cached_pair = get_cached_sft_pair(chunk_hash)\n",
    "    if cached_pair:\n",
    "        return idx, json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "    with semaphore:\n",
    "        if USE_BATCHING:\n",
    "            raise RuntimeError(\"process_chunk shouldn't be used in BATCHING mode\")\n",
    "        if USE_SINGLE_CALL:\n",
    "            sft_pair = generate_and_audit_single(chunk)\n",
    "            if sft_pair and random.random() < AUDIT_SAMPLE_RATE:\n",
    "                strict_pair = generate_and_audit(chunk)\n",
    "                if strict_pair:\n",
    "                    sft_pair = strict_pair\n",
    "        else:\n",
    "            sft_pair = generate_and_audit(chunk)\n",
    "    if sft_pair and isinstance(sft_pair, dict) and \"instruction\" in sft_pair:\n",
    "        cache_sft_pair(chunk_hash, sft_pair)\n",
    "        return idx, json.dumps(sft_pair, ensure_ascii=False) + \"\\n\"\n",
    "    return idx, None\n",
    "\n",
    "def process_pdfs(max_chunks_per_pdf: int = None):\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "        md_content = get_markdown_for_pdf(pdf_path, converter)\n",
    "        if not md_content:\n",
    "            print(\"Conversion failed or produced no text for\", pdf_path)\n",
    "            continue\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        chunks = get_cached_chunks(pdf_hash) or chunk_text_to_chunks(md_content)\n",
    "        cache_chunks(pdf_hash, chunks)\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        uncached_indices = [i for i in range(process_count) if get_cached_sft_pair(chunk_sha256(chunks[i])) is None]\n",
    "        results_buffer = [None] * process_count\n",
    "        if uncached_indices:\n",
    "            semaphore = threading.BoundedSemaphore(MAX_LLM_CONCURRENCY)\n",
    "            with ThreadPoolExecutor(max_workers=MAX_LLM_CONCURRENCY) as ex:\n",
    "                futures = {ex.submit(process_chunk, chunks[i], i, semaphore): i for i in uncached_indices}\n",
    "                processed = 0\n",
    "                total = len(uncached_indices)\n",
    "                PRINT_EVERY = 10 \n",
    "                TIME_INTERVAL = 30 \n",
    "                last_print = time.time()\n",
    "                for future in as_completed(futures):\n",
    "                    idx, line = future.result()\n",
    "                    if line:\n",
    "                        results_buffer[idx] = line\n",
    "                    processed += 1\n",
    "                    if processed % PRINT_EVERY == 0 or (time.time() - last_print) >= TIME_INTERVAL:\n",
    "                        print(f\"Progress: processed {processed}/{total} chunks\")\n",
    "                        last_print = time.time()\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "        # Ensure processed directory exists before writing output\n",
    "        PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    f.write(line)\n",
    "        print(f\"Saved entries ‚Üí {output_file}\")\n",
    "\n",
    "        try:\n",
    "            destination = PROCESSED_DIR / pdf_path.name\n",
    "            shutil.move(str(pdf_path), destination)\n",
    "            print(f\"Moved PDF: {pdf_path.name} ‚Üí {PROCESSED_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to move PDF {pdf_path.name}: {e}\")\n",
    "\n",
    "def process_pdfs_with_batching(max_chunks_per_pdf: int = None):\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "        md_content = get_markdown_for_pdf(pdf_path, converter)\n",
    "        if not md_content:\n",
    "            print(\"Conversion failed or produced no text for\", pdf_path)\n",
    "            continue\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        chunks = get_cached_chunks(pdf_hash) or chunk_text_to_chunks(md_content)\n",
    "        cache_chunks(pdf_hash, chunks)\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        uncached_indices = [i for i in range(process_count) if get_cached_sft_pair(chunk_sha256(chunks[i])) is None]\n",
    "        batches = []\n",
    "        current = []\n",
    "        current_chars = 0\n",
    "        for idx in uncached_indices:\n",
    "            c = chunks[idx]\n",
    "            if len(current) >= BATCH_SIZE or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "                batches.append(current)\n",
    "                current = []\n",
    "                current_chars = 0\n",
    "            current.append(idx)\n",
    "            current_chars += len(c)\n",
    "        if current:\n",
    "            batches.append(current)\n",
    "        results_buffer = [None] * process_count\n",
    "        if batches:\n",
    "            processed_batches = 0\n",
    "            total_batches = len(batches)\n",
    "            PRINT_EVERY_BATCH = 1      # print every N batches\n",
    "            TIME_INTERVAL_BATCH = 30   # or every T seconds\n",
    "            last_print_batch = time.time()\n",
    "            with ThreadPoolExecutor(max_workers=BATCH_CONCURRENCY) as executor:\n",
    "                future_to_batch = {}\n",
    "                for batch_idxs in batches:\n",
    "                    batch_chunks = [chunks[i] for i in batch_idxs]\n",
    "                    future = executor.submit(generate_and_audit_batch, batch_chunks)\n",
    "                    future_to_batch[future] = batch_idxs\n",
    "                for future in as_completed(future_to_batch):\n",
    "                    batch_idxs = future_to_batch[future]\n",
    "                    # Let exceptions propagate so you get a full traceback when a batch fails\n",
    "                    out_list = future.result()\n",
    "                    if out_list and isinstance(out_list, list):\n",
    "                        for idx_in_batch, obj in enumerate(out_list):\n",
    "                            target_idx = batch_idxs[idx_in_batch] if idx_in_batch < len(batch_idxs) else None\n",
    "                            if target_idx is not None and isinstance(obj, dict) and \"instruction\" in obj:\n",
    "                                cache_sft_pair(chunk_sha256(chunks[target_idx]), obj)\n",
    "                                results_buffer[target_idx] = json.dumps(obj, ensure_ascii=False) + \"\\n\"\n",
    "                    else:\n",
    "                        print(\"Batch returned invalid output\")\n",
    "                    processed_batches += 1\n",
    "                    if processed_batches % PRINT_EVERY_BATCH == 0 or (time.time() - last_print_batch) >= TIME_INTERVAL_BATCH:\n",
    "                        print(f\"Progress: processed {processed_batches}/{total_batches} batches\")\n",
    "                        last_print_batch = time.time()\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "        # Ensure processed directory exists before writing output\n",
    "        PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    f.write(line)\n",
    "        print(f\"Saved entries ‚Üí {output_file}\")\n",
    "\n",
    "        try:\n",
    "            destination = PROCESSED_DIR / pdf_path.name\n",
    "            shutil.move(str(pdf_path), destination)\n",
    "            print(f\"Moved PDF: {pdf_path.name} ‚Üí {PROCESSED_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to move PDF {pdf_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0af9cb",
   "metadata": {},
   "source": [
    "## Autotuner & Benchmarks ‚öñÔ∏è\n",
    "\n",
    "**Purpose:** Micro-benchmarks for the single-call and batching pipelines and a helper to reconfigure `SESSION` concurrency. Use these to measure throughput and pick sensible concurrency/batching settings for your environment.\n",
    "\n",
    "**Tip:** Run small probes with realistic chunks to get meaningful recommendations before full-scale runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96630780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autotuner & benchmarks (single, batch)\n",
    "def _reconfigure_session_for_concurrency(concurrency: int):\n",
    "    global SESSION\n",
    "    import requests\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    SESSION = requests.Session()\n",
    "    OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\")\n",
    "    if OLLAMA_API_KEY:\n",
    "        SESSION.headers.update({\"Authorization\": f\"Bearer {OLLAMA_API_KEY}\"})\n",
    "    retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(pool_connections=concurrency*2, pool_maxsize=concurrency*2, max_retries=retries)\n",
    "    SESSION.mount(\"http://\", adapter)\n",
    "    SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "def benchmark_single_call(chunks, concurrency=1, repeat=1):\n",
    "    if not chunks:\n",
    "        return {\"mode\":\"single_call\",\"concurrency\":concurrency,\"throughput\":0.0,\"total_processed\":0,\"total_time\":0.0}\n",
    "    _reconfigure_session_for_concurrency(concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_single, c) for c in chunks]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        success = sum(1 for r in results if r and isinstance(r, dict) and \"instruction\" in r)\n",
    "        return duration, success\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(s for _, s in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\": \"single_call\", \"concurrency\": concurrency, \"throughput\": throughput, \"total_processed\": total_processed, \"total_time\": total_time}\n",
    "\n",
    "def benchmark_batch(chunks, batch_size=4, batch_concurrency=1, repeat=1):\n",
    "    \"\"\"Benchmark batching pipeline using generate_and_audit_batch and return throughput.\"\"\"\n",
    "    if not chunks:\n",
    "        return {\"mode\":\"batch\",\"batch_size\":batch_size,\"batch_concurrency\":batch_concurrency,\"throughput\":0.0,\"total_processed\":0,\"total_time\":0.0}\n",
    "\n",
    "    # Build batches respecting batch_size and MAX_BATCH_CHARS\n",
    "    batches = []\n",
    "    current = []\n",
    "    current_chars = 0\n",
    "    for c in chunks:\n",
    "        if len(current) >= batch_size or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "            batches.append(current)\n",
    "            current = []\n",
    "            current_chars = 0\n",
    "        current.append(c)\n",
    "        current_chars += len(c)\n",
    "    if current:\n",
    "        batches.append(current)\n",
    "\n",
    "    _reconfigure_session_for_concurrency(batch_concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=batch_concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_batch, b) for b in batches]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        processed = 0\n",
    "        for res in results:\n",
    "            if isinstance(res, list):\n",
    "                processed += len(res)\n",
    "            elif isinstance(res, dict):\n",
    "                processed += 1\n",
    "        return duration, processed\n",
    "\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(p for _, p in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\":\"batch\",\"batch_size\":batch_size,\"batch_concurrency\":batch_concurrency,\"throughput\":throughput,\"total_processed\":total_processed,\"total_time\":total_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9648178",
   "metadata": {},
   "source": [
    "## Diagnostics üîç\n",
    "\n",
    "**Purpose:** Quick checks to validate the cache client and session health (Redis/DiskCache availability). Useful to run after starting the kernel or when the pipeline behaves unexpectedly.\n",
    "\n",
    "**Usage:** Run this cell after the imports/configuration cell to confirm environment readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b1320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics cell (run immediately after imports if anything seems off)\n",
    "print(\"Diagnostics:\")\n",
    "print(\"ThreadPoolExecutor:\", ThreadPoolExecutor)\n",
    "init_cache()\n",
    "print(\"Using Redis:\", _using_redis())\n",
    "print(\"Cache client type:\", type(_cache_client))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb55e9",
   "metadata": {},
   "source": [
    "## Main runner & CLI helper ‚ñ∂Ô∏è\n",
    "\n",
    "**Purpose:** `run_all()` selects the appropriate pipeline (batching or single-call) and starts processing. This wrapper is convenient for notebook and CLI usage.\n",
    "\n",
    "**Usage:** Call `run_all()` with optional `max_chunks_per_pdf` to limit processing for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry-run helper (process entire first PDF concurrently; writes .dryrun.jsonl to processed dir)\n",
    "def dry_run_process_chunk(chunk: str, idx: int, semaphore: threading.BoundedSemaphore):\n",
    "    try:\n",
    "        from docling.document_converter import DocumentConverter\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        from tqdm import tqdm\n",
    "        import threading\n",
    "\n",
    "        converter = DocumentConverter()\n",
    "        pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            print(\"No PDFs found for dry-run.\")\n",
    "        else:\n",
    "            pdf_path = pdf_files[0]\n",
    "            print(\"Dry-run using:\", pdf_path)\n",
    "            md = get_markdown_for_pdf(pdf_path, converter)\n",
    "            if not md:\n",
    "                print(\"Dry-run: no text extracted for\", pdf_path)\n",
    "            else:\n",
    "                chunks = chunk_text_to_chunks(md)\n",
    "                n_chunks = len(chunks)\n",
    "                print(f\"PDF produced {n_chunks} chunks\")\n",
    "                output_file = PROCESSED_DIR / f\"{pdf_path.stem}.dryrun.jsonl\"\n",
    "\n",
    "                # concurrency bounded by MAX_LLM_CONCURRENCY and number of chunks\n",
    "                concurrency = min(MAX_LLM_CONCURRENCY, max(1, n_chunks))\n",
    "                semaphore = threading.BoundedSemaphore(concurrency)\n",
    "                results_buffer = [None] * n_chunks\n",
    "                processed = 0\n",
    "                skipped = 0\n",
    "\n",
    "                with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "                    future_to_idx = {ex.submit(process_chunk, chunks[i], i, semaphore): i for i in range(n_chunks)}\n",
    "                    for fut in tqdm(as_completed(future_to_idx), total=n_chunks, desc=\"Dry-run chunks\"):\n",
    "                        try:\n",
    "                            idx, line = fut.result()\n",
    "                        except Exception as e:\n",
    "                            print(\"Chunk job failed:\", e)\n",
    "                            continue\n",
    "                        if line:\n",
    "                            results_buffer[idx] = line\n",
    "                            processed += 1\n",
    "                        else:\n",
    "                            cached = get_cached_sft_pair(chunk_sha256(chunks[idx]))\n",
    "                            if cached:\n",
    "                                results_buffer[idx] = json.dumps(cached, ensure_ascii=False) + \"\\n\"\n",
    "                                skipped += 1\n",
    "\n",
    "                # Write results in order\n",
    "                # Ensure processed directory exists before writing output\n",
    "                PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "                    for line in results_buffer:\n",
    "                        if line:\n",
    "                            out_f.write(line)\n",
    "\n",
    "                print(f\"Dry-run complete: processed={processed} skipped_cached={skipped} written‚Üí{output_file}\")\n",
    "    except Exception as e:\n",
    "        print(\"Dry-run skipped:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544227f",
   "metadata": {},
   "source": [
    "## Main runner & CLI helper ‚ñ∂Ô∏è\n",
    "\n",
    "**Purpose:** `run_all()` selects the appropriate pipeline (batching or single-call) and starts processing. This wrapper is convenient for notebook and CLI usage.\n",
    "\n",
    "**Usage:** Call `run_all()` with optional `max_chunks_per_pdf` to limit processing for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01547050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 14:40:51,831 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-07 14:40:51,852 - INFO - Going to convert document batch...\n",
      "2026-01-07 14:40:51,856 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-07 14:40:51,864 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,004 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,034 [RapidOCR] download_file.py:60: File exists and is valid: /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,035 [RapidOCR] main.py:53: Using /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,221 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,235 [RapidOCR] download_file.py:60: File exists and is valid: /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,240 [RapidOCR] main.py:53: Using /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,370 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,434 [RapidOCR] download_file.py:60: File exists and is valid: /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-07 14:40:52,435 [RapidOCR] main.py:53: Using /home/rahul/dev/sft-data-gen/.sftEnv/lib/python3.12/site-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "2026-01-07 14:40:52,616 - INFO - Auto OCR model selected rapidocr with onnxruntime.\n",
      "2026-01-07 14:40:52,617 - INFO - Accelerator device: 'cuda:0'\n",
      "2026-01-07 14:40:55,264 - INFO - Accelerator device: 'cuda:0'\n",
      "2026-01-07 14:40:57,021 - INFO - Processing document LPS_20230_20180306.pdf\n",
      "2026-01-07 14:41:01,623 - INFO - Finished converting document LPS_20230_20180306.pdf in 9.79 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, what is the legislative authority that establishes the Prudential Standard?\", \"output\": \"The Prudential Standard is made under subsection 230A(1) of the Life Insurance Act 1995.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"According to the provided regulatory text, what are the two specific conditions that define a Referable Reinsurance Arrangement under Attachment B, and how is the scope of such arrangements described in terms of contract structure?\", \"output\": \"Referable Reinsurance Arrangements are defined by two conditions: (a) they typically do not involve significant transfer of insurance risk; and/or (b) they involve significant financing elements. The scope includes arrangements that may consist of a single contract or a combination of two or more individual contracts, as well as side letters or other forms of agreement.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"According to the provided regulatory text, what is the required timeframe for a life company to submit a reinsurance report to APRA after the end of each financial year, and what must this report include?\", \"output\": \"The reinsurance report must be submitted within 3 months after the end of each financial year and must set out the particulars of each reinsurance contract or group of reinsurance contracts in force between the company and a reinsurer during the financial year, as outlined under Attachment A.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, what are two key characteristics that may indicate a Referable Reinsurance Arrangement, and how might such arrangements affect the financial presentation of a life company?\", \"output\": \"Two key characteristics that may indicate a Referable Reinsurance Arrangement are: (1) the presence of features that limit the extent of risk transfer at certain points over the life of the contract or on the happening of certain contingencies, and (2) requirements placed on the life company to limit loss experienced by the reinsurer over a certain period of time, including arrangements designed to allow the reinsurer to achieve a specified level of return or benefit regardless of actual experience. Such arrangements can affect the presentation of financial results by potentially leading to a misrepresentation of the true financial position of the life company, which may ultimately pose risks to policy owners.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"According to the provided regulatory text, what must an insurer do if it seeks to rely on a previous approval or other exercise of discretion by APRA under a previous version of LPS 230 Reinsurance for a reinsurance contract entered into before the commencement of this Prudential Standard?\", \"output\": \"The insurer must notify APRA in the reinsurance report required under paragraph 6.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain why a reinsurer cannot be required to obtain APRA approval for a reinsurance arrangement, and describe the conditions under which a life company may be required to modify its proposed arrangement to result in separate reinsurance and financing arrangements.\", \"output\": \"APRA approval is required only of the cedant life company, not the reinsurer, because the regulatory focus is on the life company's risk transfer and capital impact. A life company may be required to modify its proposed arrangement if APRA determines that the arrangement must be split into two distinct structures‚Äîone approved as a reinsurance arrangement and another as a financing arrangement‚Äîso that the reinsurance component genuinely transfers significant insurance risk and the financing component meets prudential requirements without misrepresenting risk or distorting capital calculations.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Extract the reinsurer's details as specified in paragraph 2(a)(i) through (v) from the provided regulatory text.\", \"output\": \"The reinsurer's details include: (i) its Australian Company Number or Australian Registered Body Number (if any); (ii) if it was a foreign entity, its place of business and any number under which it was registered in its place of business; (iii) the address of its registered office or, if it had no registered office, the address of its principal place of business; (iv) any business name it used in Australia; and (v) the name of any life company registered under the Act to which it was related (within the meaning of section 16 of the Act).\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"According to the provided regulatory text, what are the three key requirements that a life company must meet regarding reinsurance arrangements under Prudential Standard LPS 230?\", \"output\": \"1. Report on prescribed matters in relation to its reinsurance arrangements annually; 2. Not enter into a Referable Reinsurance Arrangement unless prior written approval has been granted by APRA; 3. Comply with any condition imposed by APRA in respect of an approved reinsurance or financing arrangement.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, what specific documents must be included in a minimum application for approval under clause 5.2(a)?\", \"output\": \"A draft contract wording or other draft proposed agreement and collateral or 'side' agreements, and any other documentation or information relevant to the transaction (including a written description of any verbal understandings and/or undertakings that are material to the operation of the arrangement).\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 14:41:05,460 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-07 14:41:05,500 - INFO - Going to convert document batch...\n",
      "2026-01-07 14:41:05,504 - INFO - Processing document EziCoverZurichLifeInsurance.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_ollama: raw response (truncated): {\"instruction\": \"According to the regulatory text, what must a life company do if clause 5(a) is not available, and what specific elements must be included in the submission to APRA?\", \"output\": \"The life company must submit to APRA a comprehensive description of the proposed arrangement, including details of any risk transfer and financing elements, prepared with regard to reporting standards made under the Financial Sector (Collection of Data) Act 2001, and must ensure that the arrangement will not have an inappropriate adverse effect on the company's balance sheet and capital position in any one period or over the entire term, will not adversely affect the interests of policy owners, has been reviewed within the context of its overall risk management and control systems and its Internal Capital Adequacy Assessment Process, and will not overall adversely affect the interests of policy owners.\"}\n",
      "Progress: processed 10/11 chunks\n",
      "call_ollama: raw response (truncated): {\"instruction\":\"Explain the purpose and effect of a Referable Reinsurance Arrangement and why APRA requires a life company to provide actuarial advice, internal approvals, and documentation scrutiny before approval\",\"output\":\"The purpose of a Referable Reinsurance Arrangement is to transfer specific risks from a life insurer to a reinsurer in a way that is transparent, compliant with the Insurance Contracts Act 1984, and does not obscure material financial or solvency risks. APRA requires the life company to provide actuarial advice to ensure the arrangement‚Äôs financial impact is accurately modelled, to confirm it meets statutory criteria (e.g., sections 32 and 48), and to demonstrate that risks are properly identified and mitigated. Internal approvals must be documented to show governance oversight, and all documentation must be reviewed by qualified personnel to ensure compliance with APRA‚Äôs criteria that the arrangement has a legitimate purpose, does not misrepresent financial position, and accurately reflects costs, benefits, and potential risks to policy owners.\"}\n",
      "Saved entries ‚Üí /home/rahul/dev/sft-data-gen/data/processed/2026-01-07_14-33-35/LPS_20230_20180306.train.jsonl\n",
      "Moved PDF: LPS_20230_20180306.pdf ‚Üí /home/rahul/dev/sft-data-gen/data/processed/2026-01-07_14-33-35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 14:41:07,710 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2026-01-07 14:41:10,811 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2026-01-07 14:41:10,814 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2026-01-07 14:41:11,404 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2026-01-07 14:41:11,409 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2026-01-07 14:41:12,987 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2026-01-07 14:41:12,994 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2026-01-07 14:41:15,660 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2026-01-07 14:41:15,664 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2026-01-07 14:41:17,200 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2026-01-07 14:41:17,227 - WARNING - RapidOCR returned empty result!\n",
      "\u001b[33m[WARNING] 2026-01-07 14:41:18,440 [RapidOCR] main.py:125: The text detection result is empty\u001b[0m\n",
      "2026-01-07 14:41:18,445 - WARNING - RapidOCR returned empty result!\n",
      "2026-01-07 14:41:26,972 - INFO - Finished converting document EziCoverZurichLifeInsurance.pdf in 21.51 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, what are the key eligibility requirements and application methods for Ezicover Life Insurance?\", \"output\": \"You must be eligible to apply. You can apply by phone or online.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory excerpt, what are the specific additional features of Ezicover Life Insurance that are listed after the initial benefit amounts?\", \"output\": \"Inflation protection, Future insurability, Cover suspension\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"What is the issue date of the Ezicover ¬Æ Life Insurance Product Disclosure Statement and Policy Document?\", \"output\": \"6 October 2023\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain the key information that must be disclosed to a customer about Ezicover Life Insurance before they apply, focusing on the claims process and policy benefits.\", \"output\": \"The PDS discloses that Ezicover Life Insurance provides a lump sum payment upon the insured's death or diagnosis of a terminal illness, which can be used to clear debts and support the family. It also outlines the process for making a claim, including the information required, when a claim may be denied, and an example of how the Death Benefit is paid. Additionally, the document covers premium-related details, policy changes, cancellation, reinstatement, privacy, and compliance with the Life Insurance Code of Practice, ensuring the customer is fully informed before applying.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain the eligibility criteria for applying to the Ezicover Life Insurance policy.\", \"output\": \"You must be aged between 19 and 69 years, be an Australian citizen or hold an Australian Permanent Residency Visa, and be residing in Australia on a permanent basis.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain the purpose of the welcome pack and list its components.\", \"output\": \"The welcome pack is provided after the insurer agrees to cover the policy owner. It contains a welcome letter, the policy schedule, any amended terms applied to the policy, and the policy owner's answers to the health and lifestyle questions. The policy owner should read all documents and report any missing or incorrect information promptly.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain the key obligations of a referrer under the provided regulatory text regarding the Ezicover Life Insurance product.\", \"output\": \"The referrer is acting only as a referrer for Zurich Australia Limited ABN 92 000 010 195 and does not provide advice or recommendations concerning the product or its suitability for you.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain the legal duty of an applicant when applying for insurance under Australian consumer insurance law, including what constitutes a misrepresentation and the consequences of failing to meet this duty.\", \"output\": \"The applicant has a legal duty to take reasonable care not to make a misrepresentation to the insurer before entering into a consumer insurance contract. A misrepresentation includes a false answer, a partially true answer, or an answer that does not fairly reflect the truth. This duty also applies when extending, changing, or reinstating insurance. Failure to meet this duty may result in the insurer avoiding the cover (treating it as if it never existed), modifying its terms, or declining a claim or reducing a benefit. The insurer may later investigate the truthfulness of the information provided, such as during a claim.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"What must the applicant do before their cover starts regarding changes that would affect their answers?\", \"output\": \"Tell the insurer about any changes that would cause them to answer the questions differently, as this may require further assessment or investigation.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain the eligibility requirements and application methods for Ezicover Life Insurance as stated in the provided documentation.\", \"output\": \"To be eligible for Ezicover Life Insurance, you must meet the eligibility criteria specified in the policy. You can apply either by phone or online. Additionally, you have a duty to take reasonable care when applying for insurance, and coverage may be affected by certain risks or circumstances, including changes in your situation or applicable laws.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain the conditions under which the Death benefit is paid to a representative and the conditions under which the estate receives a proportion of benefits allocated to a beneficiary.\", \"output\": \"The Death benefit is paid to your representative if you choose not to make a nomination, do not make a valid nomination, cancel your existing nomination, or if a court order overrides the nomination. The estate receives any proportion of benefits allocated to a beneficiary if the beneficiary dies before you do or if your nomination is invalid.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain the conditions under which a misrepresentation may be considered fraudulent and the potential remedies available to the insurer, including the effect of the first month premium waiver.\", \"output\": \"A misrepresentation is deemed fraudulent when the insured knowingly provides false or misleading information despite having a duty to answer questions truthfully, and the insurer would have acted differently had the truth been disclosed‚Äîsuch as denying cover or offering it on different terms. Remedies include refusing a claim, avoiding the policy, or applying other lawful remedies, provided the insurer first explains its reasons, allows the insured to respond, and supplies further information. The first month of Ezicover Life Insurance is complimentary, waiving the premium for the initial month so that only 11 months of premiums are payable in the first year.\"}\n",
      "Progress: processed 10/39 chunks\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain the specific timeframe and conditions under which a life insurance benefit claim would be denied due to suicide, including what events trigger the exclusion and where this is referenced in the policy.\", \"output\": \"A life insurance benefit claim is denied if the insured dies by suicide within 13 months of any of the following: the start date of cover, the most recent reinstatement of cover, the date cover was increased (for that increased amount), or the date cover resumes after suspension. This exclusion applies specifically to suicide occurring within that 13-month window and is referenced under 'Cover suspension' on page 19 of the policy.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, what are the specific conditions that cause a life insurance policy to end, and under what circumstances will the insurer notify the policyholder before termination?\", \"output\": \"The policy ends if the policyholder cancels it, dies, is diagnosed with a terminal illness and the Terminal Illness benefit is paid, the insurer cancels or avoids the policy under legal rights, the policyholder fails to pay premiums after at least 30 days' notice, or the policy anniversary following the policyholder's 99th birthday. The insurer will notify the policyholder before termination except when the policyholder dies or cancels the policy.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain the conditions under which a policy may be refused or modified, and describe the rules regarding coverage for events occurring shortly after a cover suspension ends.\", \"output\": \"A policy may be refused or modified only if the insurer agrees to the specific condition or exclusion with the policyholder before cover begins, and the condition or exclusion is shown on the policy schedule. Additionally, if cover is suspended and then reinstated, the insurer will not cover any insured event that occurs or becomes apparent within 90 days of the new cover start date.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"According to the provided regulatory text, what are the conditions under which Zurich will cancel a policy due to sanctions, and what opportunity is given to the policyholder to restore coverage?\", \"output\": \"Zurich will cancel a policy if it reasonably considers that the policyholder, a life insured, or a policy beneficiary is a sanctioned person or is conducting an activity sanctioned by Australian or overseas trade and economic sanctions laws. In such cases, the policyholder is given 14 days to demonstrate that the person is not sanctioned, after which coverage may be restored.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain how an insurer may respond when a policyholder's death is linked to an excluded activity, such as skydiving, and what legal constraints may affect the insurer's ability to process claims or maintain coverage.\", \"output\": \"The insurer may deny a claim if the policyholder's death is directly or indirectly related to an activity that was excluded at the time of underwriting, such as skydiving, even if the activity was disclosed and coverage was otherwise granted. However, the insurer must still comply with applicable laws, which may restrict its ability to deny claims, accept premiums, or maintain coverage‚Äîespecially where foreign or Australian regulations impose conditions on policy administration, financial transactions, or cancellation. In such cases, the insurer may need to suspend or cancel coverage only after ensuring compliance with legal requirements, potentially providing notice and an opportunity to respond before taking action.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain the eligibility criteria for the Terminal Illness Benefit and specify the maximum benefit amount payable.\", \"output\": \"The Terminal Illness Benefit applies when the insured is diagnosed with a terminal illness and has less than 12 months to live. The benefit amount payable is up to $1,500,000.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain how the death benefit is paid out and specify the maximum amount payable.\", \"output\": \"The death benefit is paid to your representative, and the benefit amount can be up to $1,500,000.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain how Ezicover Life Insurance adjusts the Death and Terminal Illness benefit amount each policy anniversary to protect against inflation.\", \"output\": \"Each policy anniversary, Ezicover automatically increases the Death and Terminal Illness benefit by the greater of 5% or the annual percentage change in the Consumer Price Index (CPI), which is based on the most recent CPI figure published at least three months prior to the anniversary (e.g., for a September anniversary, the March quarter CPI is used).\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain the conditions under which a cash advance payment is made and clarify what this payment does not entail regarding claim acceptance.\", \"output\": \"The insurer may pay a $15,000 cash advance while the claim for the Death benefit is being assessed, which can be used for expenses such as funeral costs. However, this payment does not constitute acceptance of the claim, nor does it guarantee that the claim will be approved. The cash advance is only paid to the policyholder‚Äôs representative after a death certificate or other proof of death showing the cause of death is received and it is confirmed that the death is not due to an excluded condition. Additionally, the amount of the cash advance is subtracted from the Death benefit amount specified in the policy schedule.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain how a policyholder can respond to an automatic increase in cover and what the consequences are of each response option.\", \"output\": \"The policyholder can either reject the increase, which delays the cover increase for 12 months and may lead to a future offer at the next anniversary; request a lower increase amount for the current anniversary; or reject both the current and all future increases, which stops any further automatic increases unless the policyholder later asks to reinstate them, in which case written confirmation will be provided. If the policyholder does not contact the insurer before the policy anniversary, the increase will be automatically applied.\"}\n",
      "Progress: processed 20/39 chunks\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"What are the eligibility criteria and procedural requirements for increasing cover under the Life Insurance feature following a life milestone, including timeframes, evidence requirements, and age-based limits?\", \"output\": \"You may increase your cover within 30 days of a qualifying life milestone such as marriage, divorce, becoming a parent (including adoption), becoming a full-time carer, or becoming a widow/widower, or on the next policy anniversary, whichever occurs first. You must provide evidence of the milestone. The increase is limited to the lesser of $100,000 or 50% of your Death and Terminal Illness benefit per year, with a total cap of $1,500,000 if aged 19‚Äì45 or $1,000,000 if aged 46‚Äì54. Cover must be reinstated after suspension, and increases cannot occur while premiums are overdue or during suspension periods.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, what are the required documents for a Terminal Illness claim?\", \"output\": \"A completed claim form, medical evidence from a specialist medical practitioner treating the condition who can provide supporting evidence of the condition, possible medical treatment and the prognosis, and who must state that the condition is likely to lead to death within 12 months from the date the opinion is provided, proof of your age, and the insurer may ask you to consult with a second specialist medical practitioner.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain how to suspend and resume life insurance cover, including the required notice period, premium payment, and the 90‚Äëday exclusion that applies when cover is reinstated.\", \"output\": \"To suspend cover you must notify the insurer at least 14 days before the intended suspension ends; the suspension can be extended or terminated only with that same notice. When you wish to resume cover you must pay the overdue premium on the next due date, after which the policy becomes effective on the day the suspension ends. If you end the suspension early, any insured event occurring within 90 days of the new effective date is excluded, and the suicide exclusion period restarts, requiring another 12 months of continuous cover before another suspension is permitted.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain the conditions under which a policyholder can suspend their life insurance cover, the maximum duration of suspension, and the implications of suspension on coverage and claims, based on the provided regulatory text.\", \"output\": \"A policyholder may suspend their life insurance cover for up to 12 months in total over the life of the policy, provided the policy has been continuously in force for at least 12 months. Suspension can only begin from the date the last unpaid premiums were due. During suspension, no premiums are payable and no refunds are issued for premiums already paid. Cover is not in force while suspended, meaning the policy does not provide benefit payments for death or terminal illness occurring during this period, except for events that occurred before the suspension start date and met the benefit conditions. If the policyholder was aware of a health condition or symptom before suspension, any claim arising from that condition will be excluded. Upon completion of the suspension period, the policy automatically reinstates, but the suicide exclusion period restarts and may affect claim eligibility. The policyholder should review their cover details before suspending to understand these effects.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, calculate the benefit amount payable to Dana's representative after accounting for the cash advance payment, given that Dana died two months after her 2nd policy anniversary.\", \"output\": \"$370,875\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain the required documents for a death claim and the conditions under which the insurer may require additional medical verification and cost coverage.\", \"output\": \"The insurer requires a completed claim form, an interim death certificate or other valid proof of death, and proof of age. All submitted documents must be legible, unaltered, and support the claim. If additional verification of a terminal illness diagnosis is needed, the insurer may require certification by a second specialist medical practitioner and will cover the specialist's costs and reasonable travel expenses. The insurer may decline the claim or reduce the payout if required information is not provided or if policy exclusions apply.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\":\"Based on the provided regulatory text, explain how a policyholder's occupation and pastimes can influence their life insurance premium.\", \"output\":\"Your premium is generally higher if your occupation includes hazardous duties or higher occupational risk, or if you participate in hazardous pastimes.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain how age, gender, selected death and terminal illness benefit amount, and discounts influence life insurance premiums according to the provided regulatory text.\", \"output\": \"Age directly impacts premiums, with older ages generally resulting in higher premiums. Gender also influences premiums, as males typically face higher premiums than females of the same age. The selected death and terminal illness benefit amount affects the premium, where a larger benefit amount leads to a higher premium. Discounts may be available and are displayed on the policy schedule, though they are not guaranteed and may be removed or changed.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, what is the required smoker status review condition for an applicant who stopped smoking for 12 months or more?\", \"output\": \"The applicant must have stopped smoking for 12 months or more and is requesting a review of their smoker status.\"}\n",
      "Progress: processed 30/39 chunks\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain how to cancel a policy during the cooling-off period, including required details.\", \"output\": \"To cancel your policy during the cooling-off period, you must write to Zurich Australia Limited with your name and address, policy number, the date you want the policy to end, and your signature or authority.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain how premium rates can change and what notice period is required.\", \"output\": \"Premium rates can change due to factors such as claim costs, benefit payments, industry trends, commission costs, operating expenses, reinsurance costs, capital and regulatory requirements, policyholder behaviour, economic conditions, taxes, government charges, or other factors affecting the insurer's ability to provide cover and meet claims. These changes may increase the premium, and such changes apply to all policies in the same category. The insurer must provide at least 30 days' notice of any premium change, and the premium amount shown on the policy schedule will not change before the policy anniversary.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain how premium loading is applied and how policyholders can make premium payments according to the provided regulatory text.\", \"output\": \"Premium loading is applied when answers about occupation, pastimes, or health increase the premium, and these loadings are shown on the policy schedule. Premiums can be paid by direct debit (annually, half-yearly, or monthly), by BPAY (annually), or by Buy Now Pay Later (BNPL) (all frequencies). Direct debit requires prior notice of any changes to bank details and may incur fees from the financial institution; stopping direct debit requires following the Direct Debit Request Service Agreement. BNPL payments are subject to the provider‚Äôs terms, possible fees, and a 14‚Äëday notice to cease; payment methods or frequencies may change only with prior notice. Overpayments are refunded unless under $5, and premiums are refunded after death once notified.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain the conditions under which a policyholder can receive a pro rata premium refund after the cooling-off period when paying premiums annually, and specify when the policy will end in that scenario.\", \"output\": \"A pro rata premium refund is provided only if the policyholder pays premiums annually and the next premium payment is not due within 30 days. In this case, the refund is calculated based on the number of whole months remaining before the next premium due date. The policy will end according to the period of cover that has been paid for but not refunded.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"According to the provided regulatory text, what are the timeframes for insurers to respond to customers about claims, complaints and requests for information?\", \"output\": \"The provided regulatory text states that it sets out timeframes for insurers to respond to customers about claims, complaints and requests for information, but it does not specify the actual timeframes.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Explain the steps a customer must follow to have their complaint reviewed by AFCA, including any prerequisites and time limits mentioned.\", \"output\": \"The customer must first give Zurich the opportunity to resolve the complaint. After that, they can request a free review by the Australian Financial Complaints Authority (AFCA). AFCA requires the complaint to be lodged within the time limits set by AFCA, which can be confirmed by contacting AFCA directly.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain what personal information may be used for and how a customer can opt out of that use.\", \"output\": \"Personal information (but not sensitive information) may be used to notify you of other products and services we offer. If you do not want your personal information to be used in this way, please contact us.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Based on the provided regulatory text, explain what constitutes a 'Policy schedule' and under what circumstances it may be updated.\", \"output\": \"A Policy schedule is the document that contains details of the life insured, the benefit amount, the cover type, and other important policy details. It may be updated when you make changes to your policy that we agree to, or when we make changes to the policy in accordance with the policy terms.\"}\n",
      "call_ollama: raw response (truncated): {\"instruction\": \"Extract the company name, ABN, AFSL, address, telephone number, and website URL from the provided regulatory text.\", \"output\": \"Company Name: Zurich Australia Limited\\nABN: 92 000 010 195\\nAFSL: 232510\\nAddress: Locked Bag 994 North Sydney NSW 2060\\nTelephone: 1800 025 015\\nWebsite: https://zurich.com.au/ezicover\"}\n",
      "Saved entries ‚Üí /home/rahul/dev/sft-data-gen/data/processed/2026-01-07_14-33-35/EziCoverZurichLifeInsurance.train.jsonl\n",
      "Moved PDF: EziCoverZurichLifeInsurance.pdf ‚Üí /home/rahul/dev/sft-data-gen/data/processed/2026-01-07_14-33-35\n"
     ]
    }
   ],
   "source": [
    "# Main run helper and dry-run example\n",
    "def run_all(max_chunks_per_pdf: int = None):\n",
    "    if USE_BATCHING:\n",
    "        process_pdfs_with_batching(max_chunks_per_pdf)\n",
    "    else:\n",
    "        process_pdfs(max_chunks_per_pdf)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Generate SFT training data from PDFs\")\n",
    "    parser.add_argument(\n",
    "        \"--max-chunks-per-pdf\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Limit number of chunks processed per PDF (for testing)\"\n",
    "    )\n",
    "    args = parser.parse_known_args()\n",
    "    run_all(args[0].max_chunks_per_pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sftEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
