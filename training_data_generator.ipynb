{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80fe4a68",
   "metadata": {},
   "source": [
    "# README ‚Äî SFT Data Generation (supervised_fine_tuning)\n",
    "\n",
    "**Purpose:** Generate high-quality SFT training examples from PDF text using a teacher + (optional) auditor pipeline. This notebook supports caching, batching, and configurable concurrency so you can balance speed vs. strict auditing. \n",
    "\n",
    "### Quick start ‚úÖ\n",
    "- Install dependencies (local, no docker required):\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- Copy `.env.example` to `.env` and edit values (do NOT commit your `.env`):\n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "# edit .env to match your environment\n",
    "```\n",
    "\n",
    "- Prompts and templates are loaded at runtime. By default the notebook reads `config/prompts.json`, but you can override this using the environment variable `SFT_PROMPTS_PATH` (for example: `SFT_PROMPTS_PATH=config/prompt.local.json` in your local `.env`). Do **not** commit your private prompts to the repository; use `config/prompts.example.json` as the editable, generic illustration.\n",
    "\n",
    "- `.env` recommendations:\n",
    "\n",
    "```text\n",
    "OLLAMA_URL=http://localhost:11434\n",
    "TEACHER_MODEL=qwen2.5:72b-instruct\n",
    "AUDITOR_MODEL=deepseek-r1:70b\n",
    "# Optional: run Redis for shared cache\n",
    "REDIS_URL=redis://localhost:6379/0\n",
    "MAX_LLM_CONCURRENCY=8\n",
    "USE_SINGLE_CALL=1         # recommended for speed\n",
    "USE_BATCHING=0            # optional: 0/1\n",
    "BATCH_SIZE=4\n",
    "AUDIT_SAMPLE_RATE=0.05    # sample strict audits when using single-call\n",
    "# Optional: override prompts file for local development\n",
    "# SFT_PROMPTS_PATH=config/prompt.local.json\n",
    "```\n",
    "\n",
    "### Prompts & Templates ‚úÖ\n",
    "- Location: `config/prompts.json` by default (override with `SFT_PROMPTS_PATH`).\n",
    "- What it contains: system prompts and small templates used by the pipeline (keys include `system_gen`, `gen_prompt_template`, `audit_system`, `audit_prompt_template`, `single_call_system`, `single_call_prompt_template`, `batch_system`, `batch_block_template`).\n",
    "- How it works: the notebook will first attempt to load the file at `SFT_PROMPTS_PATH` (if set) or `config/prompts.json`; if missing it will prefer `config/prompts.example.json` (editable example), and finally fall back to well‚Äëtested built-in defaults.\n",
    "- Editing tips:\n",
    "  - Copy `config/prompts.example.json` to `config/prompts.json` or create a `config/prompt.local.json` for local changes and set `SFT_PROMPTS_PATH` to point to it.\n",
    "  - **Do not** commit private prompt files ‚Äî add them to `.gitignore` (this repo already ignores `config/prompts.json` and `config/prompt.local.json`).\n",
    "  - Templates use `{chunk}` and `{generated}` placeholders for prompt composition (these are substituted when the notebook runs).\n",
    "  - After editing, re-run the top configuration cells (or restart the kernel and run top cells) to pick up changes.\n",
    "- Example: modify the `system_gen` value to shift the teacher's style or constraints, or adjust `audit_prompt_template` to change strictness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26199eae",
   "metadata": {},
   "source": [
    "## Install dependencies (helper) ‚öôÔ∏è\n",
    "\n",
    "**Purpose:** Install required Python packages for this project. Run this cell when setting up the environment or when `requirements.txt` changes.\n",
    "\n",
    "**Usage:** `!pip install -r requirements.txt` ‚Äî run once per environment. Avoid running in CI; prefer reproducible environments or lockfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (one-off helper cell)\n",
    "# Run this in the notebook when you need to install packages for this project\n",
    "# Preferably in a virtual environment. create a virtual environment using:\n",
    "# python3 -m venv .venv\n",
    "# source .venv/bin/activate\n",
    "#python -m ipykernel install --user --name .venv --display-name \"SFT Data Gen (.venv)\"\n",
    "\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72049703",
   "metadata": {},
   "source": [
    "## Imports & Configuration (overview) üîß\n",
    "\n",
    "**Purpose:** Load environment variables, set default configuration values, and establish paths used throughout the notebook (e.g., `OLLAMA_URL`, cache settings, chunking constants and data directories).\n",
    "\n",
    "**Usage:** Run this cell first after starting the kernel so all subsequent helper functions have access to these constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Configuration (canonical, single cell)\n",
    "import os, sys, json, time, re, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "load_dotenv()\n",
    "\n",
    "# Core endpoints & models\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n",
    "if OLLAMA_URL is None:\n",
    "    raise RuntimeError(\"Error: OLLAMA_URL is not set in the environment variables. Please add OLLAMA_URL to your .env file\")\n",
    "OLLAMA_URL = OLLAMA_URL.rstrip(\"/\")\n",
    "TEACHER_MODEL = os.getenv(\"TEACHER_MODEL\", \"qwen2.5:72b-instruct\")\n",
    "AUDITOR_MODEL = os.getenv(\"AUDITOR_MODEL\", \"deepseek-r1:70b\")\n",
    "\n",
    "# Cache & concurrency defaults\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\") or None\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\", str(Path.cwd() / \"cache\"))\n",
    "CHUNK_TTL = int(os.getenv(\"CHUNK_TTL\", 60 * 60 * 24))\n",
    "SFT_TTL = int(os.getenv(\"SFT_TTL\", 60 * 60 * 24 * 7))\n",
    "MAX_LLM_CONCURRENCY = int(os.getenv(\"MAX_LLM_CONCURRENCY\", 8))\n",
    "USE_SINGLE_CALL = os.getenv(\"USE_SINGLE_CALL\", \"1\") in [\"1\",\"true\",\"True\", True]\n",
    "AUDIT_SAMPLE_RATE = float(os.getenv(\"AUDIT_SAMPLE_RATE\", \"0.05\"))\n",
    "USE_BATCHING = os.getenv(\"USE_BATCHING\", \"0\") in [\"1\",\"true\",\"True\", True]\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"4\"))\n",
    "BATCH_CONCURRENCY = int(os.getenv(\"BATCH_CONCURRENCY\", \"2\"))\n",
    "MAX_BATCH_CHARS = int(os.getenv(\"MAX_BATCH_CHARS\", \"20000\"))\n",
    "# Chunking defaults (chars)\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"2000\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", \"200\"))\n",
    "# Optional override to force cache backend: 'redis' or 'disk'\n",
    "CACHE_BACKEND = os.getenv(\"CACHE_BACKEND\", \"\").lower()  # set to 'disk' to force DiskCache\n",
    "\n",
    "# Paths\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).parent.resolve()\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd().resolve()\n",
    "RAW_DATA_DIR = SCRIPT_DIR / \"data\" / \"raw\" / \"in-progress\"\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "PROCESSED_DIR = SCRIPT_DIR / \"data\" / \"processed\" / TIMESTAMP\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf95d7",
   "metadata": {},
   "source": [
    "## Prompts loader & defaults üßæ\n",
    "\n",
    "\n",
    "**Purpose:** The prompt file is to instruct model to generate as well as audit data where objective of dataset is defined. \n",
    "\n",
    "**Action:** Create a prompt templates in `config/` folder `prompt.local.json`. An illustrative example is provided `prompts.example.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts (load or defaults)\n",
    "# Respect SFT_PROMPTS_PATH if set and non-empty; otherwise use config/prompts.json\n",
    "_env_path = os.environ.get(\"SFT_PROMPTS_PATH\")\n",
    "PROMPTS_PATH = Path(_env_path) if _env_path else Path(\"config\") / \"prompts.json\"\n",
    "# If an environment override is provided (non-empty), surface it so users know where prompts are coming from\n",
    "if _env_path:\n",
    "    print(f\"SFT_PROMPTS_PATH set -> loading prompts from {PROMPTS_PATH}\")\n",
    "\n",
    "PROMPTS = None\n",
    "# Only load when PROMPTS_PATH is a file (guards against empty env -> current dir)\n",
    "if PROMPTS_PATH.is_file():\n",
    "    try:\n",
    "        with PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            PROMPTS = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read SFT prompts file {PROMPTS_PATH}: {e}. Falling back to example/defaults.\")\n",
    "\n",
    "if PROMPTS is None:\n",
    "    # Prefer an example prompts file if present (editable and safe to commit)\n",
    "    EXAMPLE_PATH = Path(\"config\") / \"prompts.example.json\"\n",
    "    if EXAMPLE_PATH.is_file():\n",
    "        with EXAMPLE_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            PROMPTS = json.load(f)\n",
    "        print(f\"Loaded prompts from example: {EXAMPLE_PATH}\")\n",
    "    else:\n",
    "        # Built-in safe defaults (generic templates) ‚Äî edit `config/prompts.example.json` to customize\n",
    "        PROMPTS = {\n",
    "            \"system_gen\": \"You are an expert assistant. Create ONE high-quality supervised fine-tuning example based only on the provided source text. Output ONLY valid JSON in this exact format: {\\\"instruction\\\": \\\"A clear task\\\", \\\"output\\\": \\\"A precise response\\\"}\",\n",
    "            \"gen_prompt_template\": \"Using only the following source text, create one high-quality training example for fine-tuning:\\\\n\\\\n{chunk}\",\n",
    "            \"audit_system\": \"You are a meticulous auditor. Verify that the generated instruction-output pair is factually accurate and faithful to the source text. Correct any errors and return only the final valid JSON.\",\n",
    "            \"audit_prompt_template\": \"Source Text:\\\\n{chunk}\\\\n\\\\nGenerated Pair:\\\\n{generated}\\\\n\\\\nVerify factual accuracy against the source and return only the corrected JSON.\",\n",
    "            \"single_call_system\": \"You are an expert assistant and auditor. Create and self-audit one high-quality example; output only the final JSON.\",\n",
    "            \"single_call_prompt_template\": \"Source Text:\\\\n{chunk}\\\\n\\\\nCreate the training example and self-audit it; return only the final JSON.\",\n",
    "            \"batch_system\": \"For each SOURCE block, create one high-quality example and ensure it is fact-checked. Return a JSON array or newline-separated JSON objects.\",\n",
    "            \"batch_block_template\": \"--- SOURCE {i} ---\\\\n{chunk}\\\\n\"\n",
    "        }\n",
    "        print(\"Loaded built-in prompt defaults\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648aa166",
   "metadata": {},
   "source": [
    "## Cache backend initialization üíæ\n",
    "\n",
    "**Purpose:** Initialize the caching client. Prefers Redis when available and healthy, otherwise falls back to DiskCache. Also provides `_using_redis()` for backend checks.\n",
    "\n",
    "**Notes:** Install `redis` or `diskcache` packages in your environment if you want the respective backends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94849bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache backend init + helper to detect Redis usage\n",
    "try:\n",
    "    import redis\n",
    "except Exception:\n",
    "    redis = None\n",
    "try:\n",
    "    from diskcache import Cache as DiskCache\n",
    "except Exception:\n",
    "    DiskCache = None\n",
    "\n",
    "_cache_client = None\n",
    "\n",
    "def init_cache():\n",
    "    \"\"\"Initialize a cache client. Prefer Redis (when available and healthy),\n",
    "    otherwise fall back to DiskCache. Honours CACHE_BACKEND env override.\n",
    "    \"\"\"\n",
    "    global _cache_client\n",
    "    if _cache_client is not None:\n",
    "        return\n",
    "    _info = globals().get(\"log\", print)\n",
    "\n",
    "    # Force disk backend when requested\n",
    "    if CACHE_BACKEND == \"disk\":\n",
    "        _info(\"CACHE_BACKEND=disk -> forcing DiskCache backend\")\n",
    "        if DiskCache is None:\n",
    "            _info(\"DiskCache not available. Install with `pip install diskcache`\")\n",
    "            raise RuntimeError(\"No cache backend available\")\n",
    "        _cache_client = DiskCache(CACHE_DIR)\n",
    "        _info(f\"Using DiskCache at {CACHE_DIR} (forced by CACHE_BACKEND)\")\n",
    "        return\n",
    "\n",
    "    # Try Redis when configured\n",
    "    if REDIS_URL and redis is not None:\n",
    "        try:\n",
    "            client = redis.from_url(REDIS_URL, socket_connect_timeout=2, socket_timeout=2, decode_responses=True)\n",
    "            # require a ping to verify health\n",
    "            client.ping()\n",
    "            _cache_client = client\n",
    "            _info(f\"Using Redis cache at {REDIS_URL}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            _info(f\"Could not use Redis at {REDIS_URL} ({type(e).__name__}: {e}). Falling back to DiskCache.\")\n",
    "            try:\n",
    "                client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "\n",
    "    # Fall back to DiskCache\n",
    "    if DiskCache is None:\n",
    "        _info(\"DiskCache not available. Install with `pip install diskcache` or set REDIS_URL to a running Redis server.\")\n",
    "        raise RuntimeError(\"No cache backend available\")\n",
    "\n",
    "    _cache_client = DiskCache(CACHE_DIR)\n",
    "    _info(f\"Using DiskCache at {CACHE_DIR}\")\n",
    "\n",
    "\n",
    "def _using_redis():\n",
    "    \"\"\"Return True when the active cache client is Redis-backed.\"\"\"\n",
    "    return bool(REDIS_URL and redis is not None and _cache_client is not None and not isinstance(_cache_client, DiskCache))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79536a",
   "metadata": {},
   "source": [
    "## Hashing, caching helpers & chunking üß©\n",
    "\n",
    "**Purpose:** Provide utilities to compute PDF and chunk hashes (`pdf_sha256`, `chunk_sha256`), store and retrieve cached chunks/SFT pairs, and split long text into reasonable chunks using paragraph-aware logic.\n",
    "\n",
    "**Usage:** These helpers are used by the processing pipeline to avoid reprocessing work and ensure chunk stability across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e14794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing & cache helpers (robust to Redis bytes/str)\n",
    "import hashlib\n",
    "\n",
    "def pdf_sha256(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def chunk_sha256(chunk: str) -> str:\n",
    "    return hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def cache_chunks(pdf_hash: str, chunks, ttl=CHUNK_TTL):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            _cache_client.set(key, json.dumps(chunks, ensure_ascii=False), ex=ttl)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            # on redis failure, fall back to diskcache\n",
    "            print(f\"Redis cache_chunks error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "    # DiskCache path\n",
    "    _cache_client.set(key, chunks, expire=ttl)\n",
    "\n",
    "def get_cached_chunks(pdf_hash: str):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            v = _cache_client.get(key)\n",
    "        except Exception as e:\n",
    "            print(f\"Redis get_cached_chunks error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "            return _cache_client.get(key)\n",
    "        if not v:\n",
    "            return None\n",
    "        try:\n",
    "            return json.loads(v)\n",
    "        except Exception:\n",
    "            return None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "def cache_sft_pair(chunk_hash: str, pair, ttl=SFT_TTL):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            _cache_client.set(key, json.dumps(pair, ensure_ascii=False), ex=ttl)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Redis cache_sft_pair error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "    _cache_client.set(key, pair, expire=ttl)\n",
    "\n",
    "def get_cached_sft_pair(chunk_hash: str):\n",
    "    global _cache_client\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if _using_redis():\n",
    "        try:\n",
    "            v = _cache_client.get(key)\n",
    "        except Exception as e:\n",
    "            print(f\"Redis get_cached_sft_pair error: {e}; falling back to DiskCache\")\n",
    "            try:\n",
    "                _cache_client.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            _cache_client = None\n",
    "            init_cache()\n",
    "            return _cache_client.get(key)\n",
    "        if not v:\n",
    "            return None\n",
    "        try:\n",
    "            return json.loads(v)\n",
    "        except Exception:\n",
    "            return None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "# Text chunking helper\n",
    "\n",
    "def chunk_text_to_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> list[str]:\n",
    "    \"\"\"Split `text` into chunks of approximately `chunk_size` characters with `overlap`.\n",
    "\n",
    "    Strategy:\n",
    "    - Split text into paragraphs on two newlines\n",
    "    - Accumulate paragraphs until adding would exceed chunk_size\n",
    "    - If a single paragraph is larger than chunk_size, split it into slices with overlap\n",
    "    - Return list of chunk strings (stripped)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_len = 0\n",
    "\n",
    "    def flush_current():\n",
    "        nonlocal current, current_len\n",
    "        if current:\n",
    "            chunk = \"\\n\\n\".join(current).strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        current = []\n",
    "        current_len = 0\n",
    "\n",
    "    for p in paragraphs:\n",
    "        p_len = len(p)\n",
    "        if current_len + p_len + (2 if current else 0) <= chunk_size:\n",
    "            current.append(p)\n",
    "            current_len += p_len + (2 if current else 0)\n",
    "        else:\n",
    "            flush_current()\n",
    "            if p_len <= chunk_size:\n",
    "                current.append(p)\n",
    "                current_len = p_len\n",
    "            else:\n",
    "                # paragraph itself is larger than chunk_size; split it\n",
    "                start = 0\n",
    "                while start < p_len:\n",
    "                    end = min(start + chunk_size, p_len)\n",
    "                    slice_ = p[start:end].strip()\n",
    "                    if slice_:\n",
    "                        chunks.append(slice_)\n",
    "                    if end >= p_len:\n",
    "                        break\n",
    "                    start = max(0, end - overlap)\n",
    "    # flush remaining\n",
    "    flush_current()\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270e769",
   "metadata": {},
   "source": [
    "## HTTP session pooling & METRICS üì°\n",
    "\n",
    "**Purpose:** Set up a shared `requests.Session` with retries and connection pooling to reuse HTTP connections for Ollama calls. Also provides `record_call` and `summarise_metrics` for basic instrumentation and debugging.\n",
    "\n",
    "**Usage:** Reconfigure session concurrency via `_reconfigure_session_for_concurrency()` when benchmarking or autotuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9642977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP session (pooling & retries) and METRICS\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "\n",
    "SESSION = requests.Session()\n",
    "OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\")\n",
    "if OLLAMA_API_KEY:\n",
    "    SESSION.headers.update({\"Authorization\": f\"Bearer {OLLAMA_API_KEY}\"})\n",
    "retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(pool_connections=MAX_LLM_CONCURRENCY*2, pool_maxsize=MAX_LLM_CONCURRENCY*2, max_retries=retries)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "METRICS = {\"calls\": []}\n",
    "def record_call(model: str, duration: float, success: bool, error: str | None = None):\n",
    "    METRICS[\"calls\"].append({\"model\": model, \"duration\": duration, \"success\": bool(success), \"error\": str(error) if error else None})\n",
    "def summarise_metrics():\n",
    "    import statistics\n",
    "    by_model = {}\n",
    "    for c in METRICS[\"calls\"]:\n",
    "        m = c[\"model\"]\n",
    "        by_model.setdefault(m, []).append(c)\n",
    "    lines = []\n",
    "    for m, calls in by_model.items():\n",
    "        durations = [c[\"duration\"] for c in calls if c[\"duration\"] is not None]\n",
    "        successes = sum(1 for c in calls if c[\"success\"])\n",
    "        total = len(calls)\n",
    "        mean = statistics.mean(durations) if durations else 0\n",
    "        p95 = sorted(durations)[int(len(durations) * 0.95)] if durations else 0\n",
    "        lines.append(f\"{m}: calls={total} success={successes} mean={mean:.2f}s p95={p95:.2f}s\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c74c44",
   "metadata": {},
   "source": [
    "## LLM call wrapper & JSON parsing ü§ñ\n",
    "\n",
    "**Purpose:** `call_ollama` performs robust calls to the Ollama API (with retries and timeout), strips surrounding markdown fences, and attempts to parse JSON. `_parse_json_array_or_objects` extracts JSON objects from potentially messy outputs.\n",
    "\n",
    "**Notes:** The helper records metrics and returns `None` on repeated failures; callers should handle `None` results accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e85c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM call + parsing helpers\n",
    "def call_ollama(model, prompt, system_prompt=\"\", session=None):\n",
    "    session = session or SESSION\n",
    "    url = f\"{OLLAMA_URL}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.1, \"num_predict\": 1024}\n",
    "    }\n",
    "    payload[\"messages\"] = [m for m in payload[\"messages\"] if m is not None]\n",
    "    for attempt in range(3):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = session.post(url, json=payload, timeout=300)\n",
    "            response.raise_for_status()\n",
    "            duration = time.time() - start\n",
    "            data = response.json()\n",
    "            raw_content = data[\"message\"][\"content\"]\n",
    "            print(\"call_ollama: raw response (truncated):\", raw_content[:2000])\n",
    "            content = raw_content.strip()\n",
    "            if content.startswith(\"```json\"):\n",
    "                content = content[7:]\n",
    "            if content.endswith(\"```\"):\n",
    "                content = content[:-3]\n",
    "            content = content.strip()\n",
    "            try:\n",
    "                parsed = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if match:\n",
    "                    parsed = json.loads(match.group(0))\n",
    "                else:\n",
    "                    record_call(model, duration, False, error=\"invalid-json\")\n",
    "                    return None\n",
    "            record_call(model, duration, True)\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            duration = time.time() - start\n",
    "            record_call(model, duration, False, error=str(e))\n",
    "            time.sleep(1)\n",
    "    record_call(model, None, False, error=\"all attempts failed\")\n",
    "    return None\n",
    "\n",
    "def _parse_json_array_or_objects(text: str):\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, dict):\n",
    "            return [parsed]\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "    except Exception:\n",
    "        objs = re.findall(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=re.DOTALL)\n",
    "        results = []\n",
    "        for o in objs:\n",
    "            try:\n",
    "                results.append(json.loads(o))\n",
    "            except Exception:\n",
    "                continue\n",
    "        if results:\n",
    "            return results\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61298c32",
   "metadata": {},
   "source": [
    "## Generation & Auditing Pipelines üß™\n",
    "\n",
    "**Purpose:** Provide three generation modes:\n",
    "- `generate_and_audit`: two-step teacher ‚Üí auditor (strict accuracy)\n",
    "- `generate_and_audit_single`: single-call with self-audit (faster)\n",
    "- `generate_and_audit_batch`: batch multiple chunks into one request\n",
    "\n",
    "**Also includes:** PDF extraction helpers with fallbacks (`docling`, `fitz`, `pdfminer`) to ensure robust text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8758ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate & audit pipelines\n",
    "\n",
    "# PDF text extraction fallbacks (docling -> fitz -> pdfminer)\n",
    "\n",
    "def extract_text_with_fitz(pdf_path: Path):\n",
    "    try:\n",
    "        import fitz\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        texts = []\n",
    "        for p in doc:\n",
    "            texts.append(p.get_text(\"text\"))\n",
    "        return \"\\n\\n\".join(t.strip() for t in texts if t and t.strip())\n",
    "    except Exception as e:\n",
    "        print(\"fitz extraction failed:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text_with_pdfminer(pdf_path: Path):\n",
    "    try:\n",
    "        from pdfminer.high_level import extract_text\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        return extract_text(str(pdf_path))\n",
    "    except Exception as e:\n",
    "        print(\"pdfminer extraction failed:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_markdown_for_pdf(pdf_path: Path, converter=None):\n",
    "    \"\"\"Return markdown text for a PDF by trying docling conversion first, then fallbacks.\n",
    "\n",
    "    Returns empty string if no usable text is found.\n",
    "    \"\"\"\n",
    "    converter = converter or DocumentConverter()\n",
    "    # try docling conversion\n",
    "    try:\n",
    "        result = converter.convert(pdf_path)\n",
    "        md = result.document.export_to_markdown()\n",
    "        if md and len(md.strip()) > 50:\n",
    "            return md\n",
    "        print(\"Docling conversion produced insufficient text; trying fallbacks.\")\n",
    "    except Exception as e:\n",
    "        print(\"Docling conversion failed:\", e)\n",
    "\n",
    "    # try PyMuPDF (fitz)\n",
    "    md = extract_text_with_fitz(pdf_path)\n",
    "    if md and len(md.strip()) > 50:\n",
    "        return md\n",
    "\n",
    "    # try pdfminer\n",
    "    md = extract_text_with_pdfminer(pdf_path)\n",
    "    if md and len(md.strip()) > 50:\n",
    "        return md\n",
    "\n",
    "    print(\"Fallback extractors returned no usable text. Check OCR engines or install fitz/pdfminer.\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def generate_and_audit(chunk):\n",
    "    start = time.time()\n",
    "    system_gen = PROMPTS.get(\"system_gen\")\n",
    "    gen_prompt = PROMPTS.get(\"gen_prompt_template\").format(chunk=chunk)\n",
    "    raw_pair = call_ollama(TEACHER_MODEL, gen_prompt, system_gen)\n",
    "    if not raw_pair:\n",
    "        return None\n",
    "    audit_system = PROMPTS.get(\"audit_system\")\n",
    "    audit_prompt = PROMPTS.get(\"audit_prompt_template\").format(chunk=chunk, generated=json.dumps(raw_pair, indent=2))\n",
    "    final_pair = call_ollama(AUDITOR_MODEL, audit_prompt, audit_system)\n",
    "    record_call(\"pipeline:generate_and_audit\", time.time() - start, True if final_pair else False)\n",
    "    return final_pair\n",
    "\n",
    "def generate_and_audit_single(chunk):\n",
    "    system_prompt = PROMPTS.get(\"single_call_system\")\n",
    "    prompt = PROMPTS.get(\"single_call_prompt_template\").format(chunk=chunk)\n",
    "    start = time.time()\n",
    "    out = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    record_call(\"pipeline:single_generate_and_audit\", time.time() - start, True if out else False)\n",
    "    return out\n",
    "\n",
    "def generate_and_audit_batch(chunks: list[str]):\n",
    "    total_chars = sum(len(c) for c in chunks)\n",
    "    if total_chars > MAX_BATCH_CHARS:\n",
    "        print(\"Batch too large\")\n",
    "        return None\n",
    "    system_prompt = PROMPTS.get(\"batch_system\")\n",
    "    block_template = PROMPTS.get(\"batch_block_template\")\n",
    "    parts = [block_template.format(i=i, chunk=c) for i,c in enumerate(chunks, start=1)]\n",
    "    prompt = \"\\n\".join(parts)\n",
    "    start = time.time()\n",
    "    raw = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    record_call(\"pipeline:batch_generate_and_audit\", time.time() - start, True if raw else False)\n",
    "    if not raw:\n",
    "        raise RuntimeError(\"generate_and_audit_batch: call_ollama returned no result\")\n",
    "    if isinstance(raw, (list, dict)):\n",
    "        return raw if isinstance(raw, list) else [raw]\n",
    "    parsed = _parse_json_array_or_objects(str(raw))\n",
    "\n",
    "    if parsed is None:\n",
    "        # Log truncated raw content for debugging and raise so callers see a traceback\n",
    "        print(\"generate_and_audit_batch: could not parse LLM response. Raw response (truncated):\", repr(raw)[:2000])\n",
    "        raise ValueError(f\"generate_and_audit_batch: could not parse LLM response: {repr(raw)[:2000]}\")\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf51a9b4",
   "metadata": {},
   "source": [
    "## Processing orchestration & helpers ‚öôÔ∏è\n",
    "\n",
    "**Purpose:** Orchestrate end-to-end processing across PDFs: `process_chunk`, `process_pdfs` (single-call or two-step), and `process_pdfs_with_batching`. Manages concurrency, caching lookups, and writes results in deterministic order.\n",
    "\n",
    "**Usage:** Use `process_pdfs(max_chunks_per_pdf)` for regular runs or the batching variant when `USE_BATCHING` is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing orchestration (single-call and batching)\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import shutil\n",
    "\n",
    "def process_chunk(chunk: str, idx: int, semaphore: threading.BoundedSemaphore):\n",
    "    chunk_hash = chunk_sha256(chunk)\n",
    "    cached_pair = get_cached_sft_pair(chunk_hash)\n",
    "    if cached_pair:\n",
    "        return idx, json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "    with semaphore:\n",
    "        if USE_BATCHING:\n",
    "            raise RuntimeError(\"process_chunk shouldn't be used in BATCHING mode\")\n",
    "        if USE_SINGLE_CALL:\n",
    "            sft_pair = generate_and_audit_single(chunk)\n",
    "            if sft_pair and random.random() < AUDIT_SAMPLE_RATE:\n",
    "                strict_pair = generate_and_audit(chunk)\n",
    "                if strict_pair:\n",
    "                    sft_pair = strict_pair\n",
    "        else:\n",
    "            sft_pair = generate_and_audit(chunk)\n",
    "    if sft_pair and isinstance(sft_pair, dict) and \"instruction\" in sft_pair:\n",
    "        cache_sft_pair(chunk_hash, sft_pair)\n",
    "        return idx, json.dumps(sft_pair, ensure_ascii=False) + \"\\n\"\n",
    "    return idx, None\n",
    "\n",
    "def process_pdfs(max_chunks_per_pdf: int = None):\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "        md_content = get_markdown_for_pdf(pdf_path, converter)\n",
    "        if not md_content:\n",
    "            print(\"Conversion failed or produced no text for\", pdf_path)\n",
    "            continue\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        chunks = get_cached_chunks(pdf_hash) or chunk_text_to_chunks(md_content)\n",
    "        cache_chunks(pdf_hash, chunks)\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        uncached_indices = [i for i in range(process_count) if get_cached_sft_pair(chunk_sha256(chunks[i])) is None]\n",
    "        results_buffer = [None] * process_count\n",
    "        if uncached_indices:\n",
    "            semaphore = threading.BoundedSemaphore(MAX_LLM_CONCURRENCY)\n",
    "            with ThreadPoolExecutor(max_workers=MAX_LLM_CONCURRENCY) as ex:\n",
    "                futures = {ex.submit(process_chunk, chunks[i], i, semaphore): i for i in uncached_indices}\n",
    "                processed = 0\n",
    "                total = len(uncached_indices)\n",
    "                PRINT_EVERY = 10 \n",
    "                TIME_INTERVAL = 30 \n",
    "                last_print = time.time()\n",
    "                for future in as_completed(futures):\n",
    "                    idx, line = future.result()\n",
    "                    if line:\n",
    "                        results_buffer[idx] = line\n",
    "                    processed += 1\n",
    "                    if processed % PRINT_EVERY == 0 or (time.time() - last_print) >= TIME_INTERVAL:\n",
    "                        print(f\"Progress: processed {processed}/{total} chunks\")\n",
    "                        last_print = time.time()\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "        # Ensure processed directory exists before writing output\n",
    "        PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    f.write(line)\n",
    "        #print(f\"Saved entries ‚Üí {output_file}\")\n",
    "\n",
    "        try:\n",
    "            destination = PROCESSED_DIR / pdf_path.name\n",
    "            shutil.move(str(pdf_path), destination)\n",
    "            print(f\"Moved PDF: {pdf_path.name} ‚Üí {PROCESSED_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to move PDF {pdf_path.name}: {e}\")\n",
    "\n",
    "def process_pdfs_with_batching(max_chunks_per_pdf: int = None):\n",
    "    from docling.document_converter import DocumentConverter\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found\")\n",
    "        return\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "        md_content = get_markdown_for_pdf(pdf_path, converter)\n",
    "        if not md_content:\n",
    "            print(\"Conversion failed or produced no text for\", pdf_path)\n",
    "            continue\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        chunks = get_cached_chunks(pdf_hash) or chunk_text_to_chunks(md_content)\n",
    "        cache_chunks(pdf_hash, chunks)\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        uncached_indices = [i for i in range(process_count) if get_cached_sft_pair(chunk_sha256(chunks[i])) is None]\n",
    "        batches = []\n",
    "        current = []\n",
    "        current_chars = 0\n",
    "        for idx in uncached_indices:\n",
    "            c = chunks[idx]\n",
    "            if len(current) >= BATCH_SIZE or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "                batches.append(current)\n",
    "                current = []\n",
    "                current_chars = 0\n",
    "            current.append(idx)\n",
    "            current_chars += len(c)\n",
    "        if current:\n",
    "            batches.append(current)\n",
    "        results_buffer = [None] * process_count\n",
    "        if batches:\n",
    "            processed_batches = 0\n",
    "            total_batches = len(batches)\n",
    "            PRINT_EVERY_BATCH = 1      # print every N batches\n",
    "            TIME_INTERVAL_BATCH = 30   # or every T seconds\n",
    "            last_print_batch = time.time()\n",
    "            with ThreadPoolExecutor(max_workers=BATCH_CONCURRENCY) as executor:\n",
    "                future_to_batch = {}\n",
    "                for batch_idxs in batches:\n",
    "                    batch_chunks = [chunks[i] for i in batch_idxs]\n",
    "                    future = executor.submit(generate_and_audit_batch, batch_chunks)\n",
    "                    future_to_batch[future] = batch_idxs\n",
    "                for future in as_completed(future_to_batch):\n",
    "                    batch_idxs = future_to_batch[future]\n",
    "                    # Let exceptions propagate so you get a full traceback when a batch fails\n",
    "                    out_list = future.result()\n",
    "                    if out_list and isinstance(out_list, list):\n",
    "                        for idx_in_batch, obj in enumerate(out_list):\n",
    "                            target_idx = batch_idxs[idx_in_batch] if idx_in_batch < len(batch_idxs) else None\n",
    "                            if target_idx is not None and isinstance(obj, dict) and \"instruction\" in obj:\n",
    "                                cache_sft_pair(chunk_sha256(chunks[target_idx]), obj)\n",
    "                                results_buffer[target_idx] = json.dumps(obj, ensure_ascii=False) + \"\\n\"\n",
    "                    else:\n",
    "                        print(\"Batch returned invalid output\")\n",
    "                    processed_batches += 1\n",
    "                    if processed_batches % PRINT_EVERY_BATCH == 0 or (time.time() - last_print_batch) >= TIME_INTERVAL_BATCH:\n",
    "                        print(f\"Progress: processed {processed_batches}/{total_batches} batches\")\n",
    "                        last_print_batch = time.time()\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "        # Ensure processed directory exists before writing output\n",
    "        PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    f.write(line)\n",
    "        #print(f\"Saved entries ‚Üí {output_file}\")\n",
    "\n",
    "        try:\n",
    "            destination = PROCESSED_DIR / pdf_path.name\n",
    "            shutil.move(str(pdf_path), destination)\n",
    "            print(f\"Moved PDF: {pdf_path.name} ‚Üí {PROCESSED_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to move PDF {pdf_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0af9cb",
   "metadata": {},
   "source": [
    "## Autotuner & Benchmarks ‚öñÔ∏è\n",
    "\n",
    "**Purpose:** Micro-benchmarks for the single-call and batching pipelines and a helper to reconfigure `SESSION` concurrency. Use these to measure throughput and pick sensible concurrency/batching settings for your environment.\n",
    "\n",
    "**Tip:** Run small probes with realistic chunks to get meaningful recommendations before full-scale runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96630780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autotuner & benchmarks (single, batch)\n",
    "def _reconfigure_session_for_concurrency(concurrency: int):\n",
    "    global SESSION\n",
    "    import requests\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    SESSION = requests.Session()\n",
    "    OLLAMA_API_KEY = os.getenv(\"OLLAMA_API_KEY\")\n",
    "    if OLLAMA_API_KEY:\n",
    "        SESSION.headers.update({\"Authorization\": f\"Bearer {OLLAMA_API_KEY}\"})\n",
    "    retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(pool_connections=concurrency*2, pool_maxsize=concurrency*2, max_retries=retries)\n",
    "    SESSION.mount(\"http://\", adapter)\n",
    "    SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "def benchmark_single_call(chunks, concurrency=1, repeat=1):\n",
    "    if not chunks:\n",
    "        return {\"mode\":\"single_call\",\"concurrency\":concurrency,\"throughput\":0.0,\"total_processed\":0,\"total_time\":0.0}\n",
    "    _reconfigure_session_for_concurrency(concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_single, c) for c in chunks]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        success = sum(1 for r in results if r and isinstance(r, dict) and \"instruction\" in r)\n",
    "        return duration, success\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(s for _, s in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\": \"single_call\", \"concurrency\": concurrency, \"throughput\": throughput, \"total_processed\": total_processed, \"total_time\": total_time}\n",
    "\n",
    "def benchmark_batch(chunks, batch_size=4, batch_concurrency=1, repeat=1):\n",
    "    \"\"\"Benchmark batching pipeline using generate_and_audit_batch and return throughput.\"\"\"\n",
    "    if not chunks:\n",
    "        return {\"mode\":\"batch\",\"batch_size\":batch_size,\"batch_concurrency\":batch_concurrency,\"throughput\":0.0,\"total_processed\":0,\"total_time\":0.0}\n",
    "\n",
    "    # Build batches respecting batch_size and MAX_BATCH_CHARS\n",
    "    batches = []\n",
    "    current = []\n",
    "    current_chars = 0\n",
    "    for c in chunks:\n",
    "        if len(current) >= batch_size or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "            batches.append(current)\n",
    "            current = []\n",
    "            current_chars = 0\n",
    "        current.append(c)\n",
    "        current_chars += len(c)\n",
    "    if current:\n",
    "        batches.append(current)\n",
    "\n",
    "    _reconfigure_session_for_concurrency(batch_concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=batch_concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_batch, b) for b in batches]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        processed = 0\n",
    "        for res in results:\n",
    "            if isinstance(res, list):\n",
    "                processed += len(res)\n",
    "            elif isinstance(res, dict):\n",
    "                processed += 1\n",
    "        return duration, processed\n",
    "\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(p for _, p in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\":\"batch\",\"batch_size\":batch_size,\"batch_concurrency\":batch_concurrency,\"throughput\":throughput,\"total_processed\":total_processed,\"total_time\":total_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9648178",
   "metadata": {},
   "source": [
    "## Diagnostics üîç\n",
    "\n",
    "**Purpose:** Quick checks to validate the cache client and session health (Redis/DiskCache availability). Useful to run after starting the kernel or when the pipeline behaves unexpectedly.\n",
    "\n",
    "**Usage:** Run this cell after the imports/configuration cell to confirm environment readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b1320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics cell (run immediately after imports if anything seems off)\n",
    "print(\"Diagnostics:\")\n",
    "print(\"ThreadPoolExecutor:\", ThreadPoolExecutor)\n",
    "init_cache()\n",
    "print(\"Using Redis:\", _using_redis())\n",
    "print(\"Cache client type:\", type(_cache_client))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb55e9",
   "metadata": {},
   "source": [
    "## Main runner & CLI helper ‚ñ∂Ô∏è\n",
    "\n",
    "**Purpose:** `run_all()` selects the appropriate pipeline (batching or single-call) and starts processing. This wrapper is convenient for notebook and CLI usage.\n",
    "\n",
    "**Usage:** Call `run_all()` with optional `max_chunks_per_pdf` to limit processing for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry-run helper (process entire first PDF concurrently; writes .dryrun.jsonl to processed dir)\n",
    "def dry_run_process_chunk(chunk: str, idx: int, semaphore: threading.BoundedSemaphore):\n",
    "    try:\n",
    "        from docling.document_converter import DocumentConverter\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        from tqdm import tqdm\n",
    "        import threading\n",
    "\n",
    "        converter = DocumentConverter()\n",
    "        pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            print(\"No PDFs found for dry-run.\")\n",
    "        else:\n",
    "            pdf_path = pdf_files[0]\n",
    "            print(\"Dry-run using:\", pdf_path)\n",
    "            md = get_markdown_for_pdf(pdf_path, converter)\n",
    "            if not md:\n",
    "                print(\"Dry-run: no text extracted for\", pdf_path)\n",
    "            else:\n",
    "                chunks = chunk_text_to_chunks(md)\n",
    "                n_chunks = len(chunks)\n",
    "                print(f\"PDF produced {n_chunks} chunks\")\n",
    "                output_file = PROCESSED_DIR / f\"{pdf_path.stem}.dryrun.jsonl\"\n",
    "\n",
    "                # concurrency bounded by MAX_LLM_CONCURRENCY and number of chunks\n",
    "                concurrency = min(MAX_LLM_CONCURRENCY, max(1, n_chunks))\n",
    "                semaphore = threading.BoundedSemaphore(concurrency)\n",
    "                results_buffer = [None] * n_chunks\n",
    "                processed = 0\n",
    "                skipped = 0\n",
    "\n",
    "                with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "                    future_to_idx = {ex.submit(process_chunk, chunks[i], i, semaphore): i for i in range(n_chunks)}\n",
    "                    for fut in tqdm(as_completed(future_to_idx), total=n_chunks, desc=\"Dry-run chunks\"):\n",
    "                        try:\n",
    "                            idx, line = fut.result()\n",
    "                        except Exception as e:\n",
    "                            print(\"Chunk job failed:\", e)\n",
    "                            continue\n",
    "                        if line:\n",
    "                            results_buffer[idx] = line\n",
    "                            processed += 1\n",
    "                        else:\n",
    "                            cached = get_cached_sft_pair(chunk_sha256(chunks[idx]))\n",
    "                            if cached:\n",
    "                                results_buffer[idx] = json.dumps(cached, ensure_ascii=False) + \"\\n\"\n",
    "                                skipped += 1\n",
    "\n",
    "                # Write results in order\n",
    "                # Ensure processed directory exists before writing output\n",
    "                PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "                    for line in results_buffer:\n",
    "                        if line:\n",
    "                            out_f.write(line)\n",
    "\n",
    "                print(f\"Dry-run complete: processed={processed} skipped_cached={skipped} written‚Üí{output_file}\")\n",
    "    except Exception as e:\n",
    "        print(\"Dry-run skipped:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544227f",
   "metadata": {},
   "source": [
    "## Main runner & CLI helper ‚ñ∂Ô∏è\n",
    "\n",
    "**Purpose:** `run_all()` selects the appropriate pipeline (batching or single-call) and starts processing. This wrapper is convenient for notebook and CLI usage.\n",
    "\n",
    "**Usage:** Call `run_all()` with optional `max_chunks_per_pdf` to limit processing for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01547050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main run helper and dry-run example\n",
    "def run_all(max_chunks_per_pdf: int = None):\n",
    "    if USE_BATCHING:\n",
    "        process_pdfs_with_batching(max_chunks_per_pdf)\n",
    "    else:\n",
    "        process_pdfs(max_chunks_per_pdf)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"Generate SFT training data from PDFs\")\n",
    "    parser.add_argument(\n",
    "        \"--max-chunks-per-pdf\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Limit number of chunks processed per PDF (for testing)\"\n",
    "    )\n",
    "    args = parser.parse_known_args()\n",
    "    run_all(args[0].max_chunks_per_pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sftEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
