{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864f5241",
   "metadata": {},
   "source": [
    "# README — SFT Data Generation (supervised_fine_tuning)\n",
    "\n",
    "**Purpose:** Generate high-quality SFT training examples from PDF text using a teacher + (optional) auditor pipeline. This notebook supports caching, batching, and configurable concurrency so you can balance speed vs. strict auditing. \n",
    "\n",
    "### Quick start ✅\n",
    "- Install dependencies (local, no docker required):\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- Copy `.env.example` to `.env` and edit values (do NOT commit your `.env`):\n",
    "\n",
    "```bash\n",
    "cp .env.example .env\n",
    "# edit .env to match your environment\n",
    "```\n",
    "\n",
    "- Prompts and templates are stored in `config/prompts.json` and are loaded by the notebook at runtime; edit that file to customize system prompts or templates.\n",
    "\n",
    "- `.env` recommendations:\n",
    "\n",
    "```text\n",
    "OLLAMA_URL=http://localhost:11434\n",
    "TEACHER_MODEL=qwen2.5:72b-instruct\n",
    "AUDITOR_MODEL=deepseek-r1:70b\n",
    "# Optional: run Redis for shared cache\n",
    "REDIS_URL=redis://localhost:6379/0\n",
    "MAX_LLM_CONCURRENCY=8\n",
    "USE_SINGLE_CALL=1         # recommended for speed\n",
    "USE_BATCHING=0            # optional: 0/1\n",
    "BATCH_SIZE=4\n",
    "AUDIT_SAMPLE_RATE=0.05    # sample strict audits when using single-call\n",
    "```\n",
    "\n",
    "### Prompts & Templates ✅\n",
    "- Location: `config/prompts.json`.\n",
    "- What it contains: system prompts and small templates used by the pipeline (keys include `system_gen`, `gen_prompt_template`, `audit_system`, `audit_prompt_template`, `single_call_system`, `single_call_prompt_template`, `batch_system`, `batch_block_template`).\n",
    "- How it works: the notebook loads `config/prompts.json` at runtime; if the file is missing the notebook falls back to safe built-in defaults so nothing breaks.\n",
    "- Editing tips:\n",
    "  - Edit `config/prompts.json` with your custom wording and keep values valid JSON.\n",
    "  - Templates use `{chunk}` and `{generated}` placeholders for prompt composition (these are substituted when the notebook runs).\n",
    "  - After editing, re-run the top configuration cells (or restart the kernel and run top cells) to pickup changes.\n",
    "- Example: modify the `system_gen` value to shift the teacher's style or constraints, or adjust `audit_prompt_template` to change strictness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e9363",
   "metadata": {},
   "source": [
    "# --- ORGANIZED ENTRYPOINTS (New ordering) ---\n",
    "# Use the cells that follow as the canonical, ordered implementation. Older, scattered cells remain below for history but **do not** run them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Configuration (ordered)\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\".env exists?\", os.path.exists(\".env\"))\n",
    "\n",
    "# Core endpoints & models\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n",
    "if OLLAMA_URL is None:\n",
    "    raise RuntimeError(\"Error: OLLAMA_URL is not set in the environment variables. Please add OLLAMA_URL to your .env file\")\n",
    "OLLAMA_URL = OLLAMA_URL.rstrip(\"/\")\n",
    "TEACHER_MODEL = os.getenv(\"TEACHER_MODEL\", \"qwen2.5:72b-instruct\")\n",
    "AUDITOR_MODEL = os.getenv(\"AUDITOR_MODEL\", \"deepseek-r1:70b\")\n",
    "\n",
    "# Cache & concurrency defaults\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\") or None\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\", str(Path.cwd() / \"cache\"))\n",
    "CHUNK_TTL = int(os.getenv(\"CHUNK_TTL\", 60 * 60 * 24))\n",
    "SFT_TTL = int(os.getenv(\"SFT_TTL\", 60 * 60 * 24 * 7))\n",
    "MAX_LLM_CONCURRENCY = int(os.getenv(\"MAX_LLM_CONCURRENCY\", 8))\n",
    "USE_SINGLE_CALL = os.getenv(\"USE_SINGLE_CALL\", \"1\") in [\"1\", \"true\", \"True\", True]\n",
    "AUDIT_SAMPLE_RATE = float(os.getenv(\"AUDIT_SAMPLE_RATE\", \"0.05\"))\n",
    "USE_BATCHING = os.getenv(\"USE_BATCHING\", \"0\") in [\"1\", \"true\", \"True\", True]\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"4\"))\n",
    "BATCH_CONCURRENCY = int(os.getenv(\"BATCH_CONCURRENCY\", \"2\"))\n",
    "MAX_BATCH_CHARS = int(os.getenv(\"MAX_BATCH_CHARS\", \"20000\"))\n",
    "\n",
    "# PATH setup\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).parent.resolve()\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd().resolve()\n",
    "RAW_DATA_DIR = SCRIPT_DIR / \"data\" / \"raw\" / \"in-progress\"\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "PROCESSED_DIR = SCRIPT_DIR / \"data\" / \"processed\" / TIMESTAMP\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input PDFs: {RAW_DATA_DIR}\")\n",
    "print(f\"Outputs will be saved to: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fbb1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts, Cache backend, HTTP session & METRICS (ordered)\n",
    "import warnings\n",
    "\n",
    "# Prompts loader\n",
    "PROMPTS_PATH = Path(\"config\") / \"prompts.json\"\n",
    "if PROMPTS_PATH.exists():\n",
    "    with PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        PROMPTS = json.load(f)\n",
    "    print(f\"Loaded prompts from {PROMPTS_PATH}\")\n",
    "else:\n",
    "    print(f\"Prompts config not found at {PROMPTS_PATH}; using built-in defaults\")\n",
    "    PROMPTS = {}\n",
    "\n",
    "# Cache backend init (prefer Redis, fallback to disk)\n",
    "try:\n",
    "    import redis\n",
    "except Exception:\n",
    "    redis = None\n",
    "try:\n",
    "    from diskcache import Cache as DiskCache\n",
    "except Exception:\n",
    "    DiskCache = None\n",
    "\n",
    "_cache_client = None\n",
    "\n",
    "def init_cache():\n",
    "    global _cache_client\n",
    "    if _cache_client is not None:\n",
    "        return\n",
    "    if REDIS_URL and redis is not None:\n",
    "        try:\n",
    "            _cache_client = redis.from_url(REDIS_URL)\n",
    "            _cache_client.ping()\n",
    "            print(f\"Using Redis cache at {REDIS_URL}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Could not connect to Redis ({e}), falling back to DiskCache\")\n",
    "    if DiskCache is None:\n",
    "        raise RuntimeError(\"No cache backend available (install diskcache or provide REDIS_URL)\")\n",
    "    _cache_client = DiskCache(CACHE_DIR)\n",
    "    print(f\"Using DiskCache at {CACHE_DIR}\")\n",
    "\n",
    "# HTTP session & retries\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "\n",
    "SESSION = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(pool_connections=MAX_LLM_CONCURRENCY * 2, pool_maxsize=MAX_LLM_CONCURRENCY * 2, max_retries=retries)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "# METRICS\n",
    "METRICS = {\"calls\": []}\n",
    "\n",
    "def record_call(model: str, duration: float, success: bool, error: str | None = None):\n",
    "    METRICS[\"calls\"].append({\"model\": model, \"duration\": duration, \"success\": bool(success), \"error\": str(error) if error else None})\n",
    "\n",
    "def summarise_metrics():\n",
    "    import statistics\n",
    "    by_model = {}\n",
    "    for c in METRICS[\"calls\"]:\n",
    "        m = c[\"model\"]\n",
    "        by_model.setdefault(m, []).append(c)\n",
    "    lines = []\n",
    "    for m, calls in by_model.items():\n",
    "        durations = [c[\"duration\"] for c in calls if c[\"duration\"] is not None]\n",
    "        successes = sum(1 for c in calls if c[\"success\"])\n",
    "        total = len(calls)\n",
    "        mean = statistics.mean(durations) if durations else 0\n",
    "        p95 = sorted(durations)[int(len(durations) * 0.95)] if durations else 0\n",
    "        lines.append(f\"{m}: calls={total} success={successes} mean={mean:.2f}s p95={p95:.2f}s\")\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers (hashing, caching helpers, chunking, parsing)\n",
    "import hashlib\n",
    "\n",
    "# Hashing\n",
    "def pdf_sha256(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def chunk_sha256(chunk: str) -> str:\n",
    "    return hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Cache get/set helpers\n",
    "import json\n",
    "\n",
    "def cache_chunks(pdf_hash: str, chunks, ttl=CHUNK_TTL):\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        _cache_client.set(key, json.dumps(chunks, ensure_ascii=False), ex=ttl)\n",
    "    else:\n",
    "        _cache_client.set(key, chunks, expire=ttl)\n",
    "\n",
    "\n",
    "def get_cached_chunks(pdf_hash: str):\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        v = _cache_client.get(key)\n",
    "        return json.loads(v) if v else None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "\n",
    "def cache_sft_pair(chunk_hash: str, pair, ttl=SFT_TTL):\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        _cache_client.set(key, json.dumps(pair, ensure_ascii=False), ex=ttl)\n",
    "    else:\n",
    "        _cache_client.set(key, pair, expire=ttl)\n",
    "\n",
    "\n",
    "def get_cached_sft_pair(chunk_hash: str):\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        v = _cache_client.get(key)\n",
    "        return json.loads(v) if v else None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "# Chunking and parsing helpers\n",
    "\n",
    "def chunk_text_to_chunks(md_content: str, min_len: int = 300, max_len: int = 3000):\n",
    "    chunks = [c.strip() for c in md_content.split(\"\\n\\n\") if min_len < len(c.strip()) < max_len]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _parse_json_array_or_objects(text: str):\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, dict):\n",
    "            return [parsed]\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "    except Exception:\n",
    "        objs = re.findall(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=re.DOTALL)\n",
    "        results = []\n",
    "        for o in objs:\n",
    "            try:\n",
    "                results.append(json.loads(o))\n",
    "            except Exception:\n",
    "                continue\n",
    "        if results:\n",
    "            return results\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ecafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM call + pipeline functions (ordered)\n",
    "\n",
    "def call_ollama(model, prompt, system_prompt=\"\", session=None):\n",
    "    session = session or SESSION\n",
    "    url = f\"{OLLAMA_URL}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.1, \"num_predict\": 1024}\n",
    "    }\n",
    "    payload[\"messages\"] = [m for m in payload[\"messages\"] if m is not None]\n",
    "    print(f\"      > Sending to {model}...\", end=\"\")\n",
    "    for attempt in range(3):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = session.post(url, json=payload, timeout=300)\n",
    "            response.raise_for_status()\n",
    "            duration = time.time() - start\n",
    "            data = response.json()\n",
    "            raw_content = data[\"message\"][\"content\"]\n",
    "            print(\" [Done]\")\n",
    "            content = raw_content.strip()\n",
    "            if content.startswith(\"```json\"):\n",
    "                content = content[7:]\n",
    "            if content.endswith(\"```\"):\n",
    "                content = content[:-3]\n",
    "            content = content.strip()\n",
    "            try:\n",
    "                parsed = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"      ! Direct parse failed. Trying regex extraction...\")\n",
    "                match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if match:\n",
    "                    parsed = json.loads(match.group(0))\n",
    "                else:\n",
    "                    record_call(model, duration, False, error=\"invalid-json\")\n",
    "                    return None\n",
    "            record_call(model, duration, True)\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            duration = time.time() - start\n",
    "            record_call(model, duration, False, error=str(e))\n",
    "            print(f\"\\n      ! Attempt {attempt+1} failed: {str(e)[:200]}\")\n",
    "            time.sleep(3)\n",
    "    record_call(model, None, False, error=\"all attempts failed\")\n",
    "    print(f\"      ! All attempts failed for {model}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_and_audit(chunk):\n",
    "    start = time.time()\n",
    "    system_gen = PROMPTS.get(\"system_gen\")\n",
    "    gen_prompt = PROMPTS.get(\"gen_prompt_template\", \"Using only the following extract from Australian life insurance regulatory documentation, create one high-quality training example...\\n\\n{chunk}\").format(chunk=chunk)\n",
    "    raw_pair = call_ollama(TEACHER_MODEL, gen_prompt, system_gen)\n",
    "    if not raw_pair:\n",
    "        return None\n",
    "    audit_system = PROMPTS.get(\"audit_system\")\n",
    "    audit_prompt = PROMPTS.get(\"audit_prompt_template\", \"Source Text:\\n{chunk}\\n\\nGenerated Pair:\\n{generated}\\n\\nVerify factual accuracy...\").format(chunk=chunk, generated=json.dumps(raw_pair, indent=2))\n",
    "    final_pair = call_ollama(AUDITOR_MODEL, audit_prompt, audit_system)\n",
    "    pipeline_duration = time.time() - start\n",
    "    record_call(\"pipeline:generate_and_audit\", pipeline_duration, True if final_pair else False)\n",
    "    return final_pair\n",
    "\n",
    "\n",
    "def generate_and_audit_single(chunk):\n",
    "    system_prompt = PROMPTS.get(\"single_call_system\")\n",
    "    prompt = PROMPTS.get(\"single_call_prompt_template\", \"Source Text:\\n{chunk}\\n\\nCreate the training example and self-audit it; return only the final JSON.\").format(chunk=chunk)\n",
    "    start = time.time()\n",
    "    out = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    record_call(\"pipeline:single_generate_and_audit\", time.time() - start, True if out else False)\n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_and_audit_batch(chunks: list[str]):\n",
    "    total_chars = sum(len(c) for c in chunks)\n",
    "    if total_chars > MAX_BATCH_CHARS:\n",
    "        print(f\"      ! Batch too large ({total_chars} chars) — reduce BATCH_SIZE\")\n",
    "        return None\n",
    "    system_prompt = PROMPTS.get(\"batch_system\")\n",
    "    block_template = PROMPTS.get(\"batch_block_template\", \"--- SOURCE {i} ---\\\\n{chunk}\\\\n\")\n",
    "    parts = [block_template.format(i=i, chunk=c) for i,c in enumerate(chunks, start=1)]\n",
    "    prompt = \"\\n\".join(parts)\n",
    "    start = time.time()\n",
    "    raw = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    duration = time.time() - start\n",
    "    record_call(\"pipeline:batch_generate_and_audit\", duration, True if raw else False)\n",
    "    if not raw:\n",
    "        return None\n",
    "    if isinstance(raw, list):\n",
    "        return raw\n",
    "    if isinstance(raw, dict):\n",
    "        return [raw]\n",
    "    parsed = _parse_json_array_or_objects(str(raw))\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing orchestration (process_chunk, process_pdfs, process_pdfs_with_batching)\n",
    "import random\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def process_chunk(chunk: str, idx: int, semaphore: threading.BoundedSemaphore):\n",
    "    chunk_hash = chunk_sha256(chunk)\n",
    "    cached_pair = get_cached_sft_pair(chunk_hash)\n",
    "    if cached_pair:\n",
    "        print(f\"      > Cache hit for chunk {idx+1}\")\n",
    "        return idx, json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "    with semaphore:\n",
    "        if USE_BATCHING:\n",
    "            raise RuntimeError(\"process_chunk shouldn't be used in BATCHING mode\")\n",
    "        if USE_SINGLE_CALL:\n",
    "            sft_pair = generate_and_audit_single(chunk)\n",
    "            if sft_pair and random.random() < AUDIT_SAMPLE_RATE:\n",
    "                strict_pair = generate_and_audit(chunk)\n",
    "                if strict_pair:\n",
    "                    sft_pair = strict_pair\n",
    "        else:\n",
    "            sft_pair = generate_and_audit(chunk)\n",
    "    if sft_pair and isinstance(sft_pair, dict) and \"instruction\" in sft_pair:\n",
    "        cache_sft_pair(chunk_hash, sft_pair)\n",
    "        return idx, json.dumps(sft_pair, ensure_ascii=False) + \"\\n\"\n",
    "    else:\n",
    "        print(f\"      ! Failed chunk {idx+1}\")\n",
    "        return idx, None\n",
    "\n",
    "\n",
    "def process_pdfs(max_chunks_per_pdf: int = None):\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"Error: No PDF files found in {RAW_DATA_DIR}\")\n",
    "        return\n",
    "    print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "        print(f\"\\n--- Processing: {pdf_path.name} → {output_file.name} ---\")\n",
    "        try:\n",
    "            result = converter.convert(pdf_path)\n",
    "            md_content = result.document.export_to_markdown()\n",
    "        except Exception as e:\n",
    "            print(f\"      ! Failed to convert PDF: {e}\")\n",
    "            continue\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        cached = get_cached_chunks(pdf_hash)\n",
    "        if cached:\n",
    "            chunks = cached\n",
    "            print(f\"Using {len(chunks)} cached chunks for {pdf_path.name}\")\n",
    "        else:\n",
    "            chunks = chunk_text_to_chunks(md_content)\n",
    "            cache_chunks(pdf_hash, chunks)\n",
    "            print(f\"Extracted and cached {len(chunks)} chunks.\")\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        print(f\"Processing {process_count} chunks (single-call={USE_SINGLE_CALL}, batching={USE_BATCHING})...\")\n",
    "        uncached_indices = []\n",
    "        for i in range(process_count):\n",
    "            chh = chunk_sha256(chunks[i])\n",
    "            if get_cached_sft_pair(chh) is None:\n",
    "                uncached_indices.append(i)\n",
    "        results_buffer = [None] * process_count\n",
    "        successful_entries = 0\n",
    "        file_lock = threading.Lock()\n",
    "        if uncached_indices:\n",
    "            semaphore = threading.BoundedSemaphore(MAX_LLM_CONCURRENCY)\n",
    "            with ThreadPoolExecutor(max_workers=MAX_LLM_CONCURRENCY) as executor:\n",
    "                future_to_idx = {executor.submit(process_chunk, chunks[i], i, semaphore): i for i in uncached_indices}\n",
    "                for future in tqdm(as_completed(future_to_idx), total=len(future_to_idx), desc=f\"SFT [{pdf_stem}]\"):\n",
    "                    try:\n",
    "                        idx, line = future.result()\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ! Chunk job failed: {e}\")\n",
    "                        continue\n",
    "                    if line:\n",
    "                        results_buffer[idx] = line\n",
    "        # Fill from cache\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    with file_lock:\n",
    "                        f.write(line)\n",
    "                        f.flush()\n",
    "                        successful_entries += 1\n",
    "        print(f\"Saved {successful_entries}/{process_count} entries → {output_file}\")\n",
    "    print(f\"\\nAll processing complete! Outputs in: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autotuner & Benchmarks (ordered)\n",
    "def _reconfigure_session_for_concurrency(concurrency: int):\n",
    "    global SESSION\n",
    "    import requests\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    SESSION = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(pool_connections=concurrency * 2, pool_maxsize=concurrency * 2, max_retries=retries)\n",
    "    SESSION.mount(\"http://\", adapter)\n",
    "    SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "def benchmark_single_call(chunks, concurrency=1, repeat=1):\n",
    "    _reconfigure_session_for_concurrency(concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_single, c) for c in chunks]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        success = sum(1 for r in results if r and isinstance(r, dict) and \"instruction\" in r)\n",
    "        return duration, success\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(s for _, s in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\": \"single_call\", \"concurrency\": concurrency, \"throughput\": throughput, \"total_processed\": total_processed, \"total_time\": total_time}\n",
    "\n",
    "\n",
    "def benchmark_batch(chunks, batch_size=4, batch_concurrency=1, repeat=1):\n",
    "    batches = []\n",
    "    current = []\n",
    "    current_chars = 0\n",
    "    for c in chunks:\n",
    "        if len(current) >= batch_size or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "            batches.append(current)\n",
    "            current = []\n",
    "            current_chars = 0\n",
    "        current.append(c)\n",
    "        current_chars += len(c)\n",
    "    if current:\n",
    "        batches.append(current)\n",
    "    _reconfigure_session_for_concurrency(batch_concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=batch_concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_batch, b) for b in batches]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        processed = 0\n",
    "        for res in results:\n",
    "            if isinstance(res, list):\n",
    "                processed += len(res)\n",
    "        return duration, processed\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(p for _, p in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\": \"batch\", \"batch_size\": batch_size, \"batch_concurrency\": batch_concurrency, \"throughput\": throughput, \"total_processed\": total_processed, \"total_time\": total_time}\n",
    "\n",
    "\n",
    "def autotune(chunks, concurrency_options=[1,2,4], batch_sizes=[None,2,4], batch_concurrency_options=[1,2], repeat=1):\n",
    "    results = []\n",
    "    print(f\"Autotune: testing {len(chunks)} chunks | single-call conc: {concurrency_options} | batch_sizes: {batch_sizes} | batch_conc: {batch_concurrency_options}\")\n",
    "    for conc in concurrency_options:\n",
    "        r = benchmark_single_call(chunks, concurrency=conc, repeat=repeat)\n",
    "        print(f\"single_call conc={conc} -> throughput={r['throughput']:.3f} chunks/sec\")\n",
    "        results.append(r)\n",
    "    for bsize in [b for b in batch_sizes if b is not None]:\n",
    "        for bconc in batch_concurrency_options:\n",
    "            r = benchmark_batch(chunks, batch_size=bsize, batch_concurrency=bconc, repeat=repeat)\n",
    "            print(f\"batch bsize={bsize} bconc={bconc} -> throughput={r['throughput']:.3f} chunks/sec\")\n",
    "            results.append(r)\n",
    "    best = max(results, key=lambda x: x[\"throughput\"]) if results else None\n",
    "    sorted_results = sorted(results, key=lambda x: x[\"throughput\"], reverse=True)\n",
    "    print(\"\\nTop configs:\")\n",
    "    for s in sorted_results[:5]:\n",
    "        print(s)\n",
    "    print(\"\\nBest config:\", best)\n",
    "    return {\"best\": best, \"all\": sorted_results}\n",
    "\n",
    "# Probe helper (keeps the probe cell minimal)\n",
    "PROBE_CHUNKS = 6\n",
    "pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "if pdf_files:\n",
    "    pdf_path = pdf_files[0]\n",
    "    pdf_hash = pdf_sha256(pdf_path)\n",
    "    probe_chunks = (get_cached_chunks(pdf_hash) or [])[:PROBE_CHUNKS]\n",
    "    print(f\"Probe chunks ready: {len(probe_chunks)} from {pdf_path.name}\")\n",
    "else:\n",
    "    probe_chunks = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preview & Main run (ordered)\n",
    "import random, statistics\n",
    "from collections import Counter\n",
    "\n",
    "def load_jsonl(path):\n",
    "    valid = []\n",
    "    invalid = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, raw in enumerate(f):\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                valid.append(obj)\n",
    "            except Exception as e:\n",
    "                invalid.append((i, str(e), line[:300]))\n",
    "    return valid, invalid\n",
    "\n",
    "# Main run helper\n",
    "def run_all(max_chunks_per_pdf: int = None):\n",
    "    if USE_BATCHING:\n",
    "        process_pdfs_with_batching(max_chunks_per_pdf)\n",
    "    else:\n",
    "        process_pdfs(max_chunks_per_pdf)\n",
    "\n",
    "# Quick preview: latest processed file\n",
    "def quick_preview():\n",
    "    files = sorted(PROCESSED_DIR.glob(\"*.train.jsonl\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        print(\"No processed files found in:\", PROCESSED_DIR)\n",
    "        return\n",
    "    path = files[0]\n",
    "    print(\"Previewing:\", path)\n",
    "    entries, invalid = load_jsonl(path)\n",
    "    print(f\"Total lines: {sum(1 for _ in open(path,'r',encoding='utf-8'))}\")\n",
    "    print(f\"Valid: {len(entries)} | Invalid: {len(invalid)}\")\n",
    "    print(\"Basic stats:\", {\"count\": len(entries)})\n",
    "    print(\"LLM metrics:\\n\", summarise_metrics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\".env exists?\", os.path.exists(\".env\"))\n",
    "load_dotenv()\n",
    "\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n",
    "print(\"OLLAMA_URL:\", OLLAMA_URL)\n",
    "TEACHER_MODEL = os.getenv(\"TEACHER_MODEL\")\n",
    "print(\"TEACHER_MODEL:\", TEACHER_MODEL)\n",
    "AUDITOR_MODEL = os.getenv(\"AUDITOR_MODEL\")\n",
    "print(\"AUDITOR_MODEL:\", AUDITOR_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7583246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg, end=\"\\n\"):\n",
    "    \"\"\"Lightweight logger with immediate flush used across the notebook.\"\"\"\n",
    "    import sys\n",
    "    sys.stdout.write(f\"{msg}{end}\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from docling.document_converter import DocumentConverter\n",
    "# Safe tqdm import: prefer notebook tqdm when ipywidgets is available, otherwise fallback to console tqdm\n",
    "import warnings\n",
    "try:\n",
    "    import ipywidgets  # type: ignore\n",
    "    from tqdm.notebook import tqdm as tqdm\n",
    "except Exception:\n",
    "    from tqdm import tqdm as tqdm\n",
    "try:\n",
    "    from tqdm.std import TqdmWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION (lightweight) ---\n",
    "load_dotenv()\n",
    "\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n",
    "if OLLAMA_URL is None:\n",
    "    print(\"Error: OLLAMA_URL is not set in the environment variables.\")\n",
    "    print(\"Please add OLLAMA_URL=http://your-ip:11434 to your .env file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Strip trailing slash if present\n",
    "OLLAMA_URL = OLLAMA_URL.rstrip(\"/\")\n",
    "\n",
    "TEACHER_MODEL = os.getenv(\"TEACHER_MODEL\", \"qwen2.5:72b-instruct\")\n",
    "AUDITOR_MODEL = os.getenv(\"AUDITOR_MODEL\", \"deepseek-r1:70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache and concurrency settings (separate, lightweight)\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\") or None  # e.g. redis://localhost:6379/0\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\", str(Path.cwd() / \"cache\"))\n",
    "CHUNK_TTL = int(os.getenv(\"CHUNK_TTL\", 60 * 60 * 24))  # 1 day\n",
    "SFT_TTL = int(os.getenv(\"SFT_TTL\", 60 * 60 * 60 * 24 * 7))  # 7 days\n",
    "MAX_LLM_CONCURRENCY = int(os.getenv(\"MAX_LLM_CONCURRENCY\", 8))\n",
    "# Strategy: 'two_step' (teacher + auditor) or 'single_call' (teacher self-audits)\n",
    "USE_SINGLE_CALL = os.getenv(\"USE_SINGLE_CALL\", \"1\") in [\"1\", \"true\", \"True\", True]\n",
    "# Audit sampling: when using single-call, run the two-step auditor on a small random sample to detect regressions\n",
    "AUDIT_SAMPLE_RATE = float(os.getenv(\"AUDIT_SAMPLE_RATE\", \"0.05\"))\n",
    "# Batching options (optional speed optimization):\n",
    "USE_BATCHING = os.getenv(\"USE_BATCHING\", \"0\") in [\"1\", \"true\", \"True\", True]\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"4\"))  # number of chunks to send in a single prompt\n",
    "BATCH_CONCURRENCY = int(os.getenv(\"BATCH_CONCURRENCY\", \"2\"))  # concurrent batch jobs\n",
    "# Safety: max batch chars to avoid overly large prompts\n",
    "MAX_BATCH_CHARS = int(os.getenv(\"MAX_BATCH_CHARS\", \"20000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac365843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompts configuration (config/prompts.json). If the file is missing, fall back to built-in defaults.\n",
    "PROMPTS_PATH = Path(\"config\") / \"prompts.json\"\n",
    "if PROMPTS_PATH.exists():\n",
    "    with PROMPTS_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        PROMPTS = json.load(f)\n",
    "    log(f\"Loaded prompts from {PROMPTS_PATH}\")\n",
    "else:\n",
    "    log(f\"Prompts config not found at {PROMPTS_PATH}; using built-in defaults\")\n",
    "    PROMPTS = {\n",
    "        \"system_gen\": (\n",
    "            \"You are a senior Life Insurance Operations expert with deep knowledge of Australian life insurance processes, \"\n",
    "            \"including policy administration, underwriting, claims handling, reinsurance, customer service, and regulatory compliance (APRA standards).\\n\\n\"\n",
    "            \"Your task is to create ONE high-quality supervised fine-tuning example based solely on the provided regulatory text.\\n\"\n",
    "            \"The example should reflect real-world operational tasks that a life insurance company employee or specialized LLM would perform, \"\n",
    "            \"such as explaining requirements, summarizing obligations, answering procedural questions, or guiding compliance actions.\\n\\n\"\n",
    "            \"Output ONLY valid JSON in this exact format:\\n{\\\"instruction\\\": \\\"A clear, realistic user query or task related to life insurance operations\\\", \\\"output\\\": \\\"A precise, professional, and accurate response based strictly on the source text\\\"}\\n\\n\"\n",
    "            \"No explanations, no markdown, no extra text.\"\n",
    "        ),\n",
    "        \"gen_prompt_template\": (\n",
    "            \"Using only the following extract from Australian life insurance regulatory documentation, \"\n",
    "            \"create one high-quality training example for fine-tuning an LLM to excel in life insurance operations:\\n\\n{chunk}\"\n",
    "        ),\n",
    "        \"audit_system\": (\n",
    "            \"You are a meticulous Life Insurance Regulatory Auditor.\\n\"\n",
    "            \"Your role is to verify that the generated instruction-output pair is factually accurate, complete, and faithfully represents the source text with no hallucinations or additions. \"\n",
    "            \"Correct any inaccuracies, improve clarity if needed, but preserve the original intent.\\n\\n\"\n",
    "            \"Output ONLY the final corrected JSON in this exact format:\\n{\\\"instruction\\\": \\\"...\\\", \\\"output\\\": \\\"...\\\"}\\n\\n\"\n",
    "            \"No thinking steps, no preamble, no markdown.\"\n",
    "        ),\n",
    "        \"audit_prompt_template\": \"Source Text:\\n{chunk}\\n\\nGenerated Pair:\\n{generated}\\n\\nVerify factual accuracy against the source. Correct errors. Return only the final valid JSON.\",\n",
    "        \"single_call_system\": (\n",
    "            \"You are a senior Life Insurance Operations expert with deep knowledge of Australian life insurance processes and a meticulous auditor.\\n\\n\"\n",
    "            \"Create ONE high-quality supervised fine-tuning example based ONLY on the provided regulatory text. Also verify and audit the example yourself to ensure factual accuracy and strict fidelity to the source.\\n\\n\"\n",
    "            \"Output ONLY the final valid JSON in this exact format:\\n{\\\"instruction\\\": \\\"...\\\", \\\"output\\\": \\\"...\\\"}\\n\\n\"\n",
    "            \"No explanations, no markdown, no extra text.\"\n",
    "        ),\n",
    "        \"single_call_prompt_template\": \"Source Text:\\n{chunk}\\n\\nCreate the training example and self-audit it; return only the final JSON.\",\n",
    "        \"batch_system\": (\n",
    "            \"You are a senior Life Insurance Operations expert and a strict auditor.\\n\\n\"\n",
    "            \"For each of the following SOURCE TEXT blocks, create ONE high-quality supervised fine-tuning example and ensure each example is strictly fact-checked. Return a JSON array where each item corresponds to the source blocks in the same order. \"\n",
    "            \"Each item must be an object of the form: {\\\"instruction\\\": \\\"...\\\", \\\"output\\\": \\\"...\\\"}.\\nOutput only valid JSON — either a JSON array or newline-separated JSON objects.\"\n",
    "        ),\n",
    "        \"batch_block_template\": \"--- SOURCE {i} ---\\n{chunk}\\n\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64434420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPTS loaded from: config/prompts.json\n",
      "PROMPTS keys: system_gen, gen_prompt_template, audit_system, audit_prompt_template, single_call_system, single_call_prompt_template, batch_system, batch_block_template\n",
      "\n",
      "PROMPTS preview (truncated):\n",
      "{ 'audit_prompt_template': 'Source Text:\\n'\n",
      "                           '{chunk}\\n'\n",
      "                           '\\n'\n",
      "                           'Generated Pair:\\n'\n",
      "                           '{generated}\\n'\n",
      "                           '\\n'\n",
      "                           'Verify factual accuracy against the source. Correct errors. Return only the final valid '\n",
      "                           'JSON.',\n",
      "  'audit_system': 'You are a meticulous Life Insurance Regulatory Auditor.\\n'\n",
      "                  'Your role is to verify that the generated instruction-output pair is factually accurate, complete, '\n",
      "                  'and faithfully represents the source text with no hallucinations or additions. Correct any '\n",
      "                  'inaccuracies, improve clarity if needed, but preserve the original intent.\\n'\n",
      "                  '\\n'\n",
      "                  'Output ONLY the final corrected JSON in this exact format:\\n'\n",
      "                  '{\"instruction\": \"..... [truncated]',\n",
      "  'batch_block_template': '--- SOURCE {i} ---\\n{chunk}\\n',\n",
      "  'batch_system': 'You are a senior Life Insurance Operations expert and a strict auditor.\\n'\n",
      "                  '\\n'\n",
      "                  'For each of the following SOURCE TEXT blocks, create ONE high-quality supervised fine-tuning '\n",
      "                  'example and ensure each example is strictly fact-checked. Return a JSON array where each item '\n",
      "                  'corresponds to the source blocks in the same order. Each item must be an object of the form: '\n",
      "                  '{\"instruction\": \"...\", \"output\": \"...\"}.\\n'\n",
      "                  'Output... [truncated]',\n",
      "  'gen_prompt_template': 'Using only the following extract from Australian life insurance regulatory documentation, '\n",
      "                         'create one high-quality training example for fine-tuning an LLM to excel in life insurance '\n",
      "                         'operations:\\n'\n",
      "                         '\\n'\n",
      "                         '{chunk}',\n",
      "  'single_call_prompt_template': 'Source Text:\\n'\n",
      "                                 '{chunk}\\n'\n",
      "                                 '\\n'\n",
      "                                 'Create the training example and self-audit it; return only the final JSON.',\n",
      "  'single_call_system': 'You are a senior Life Insurance Operations expert with deep knowledge of Australian life '\n",
      "                        'insurance processes and a meticulous auditor.\\n'\n",
      "                        '\\n'\n",
      "                        'Create ONE high-quality supervised fine-tuning example based ONLY on the provided regulatory '\n",
      "                        'text. Also verify and audit the example yourself to ensure factual accuracy and strict '\n",
      "                        'fidelity to the source.\\n'\n",
      "                        '\\n'\n",
      "                        'Output ONLY the final valid JSON in this exact format:\\n'\n",
      "                        '{\"in... [truncated]',\n",
      "  'system_gen': 'You are a senior Life Insurance Operations expert with deep knowledge of Australian life insurance '\n",
      "                'processes, including policy administration, underwriting, claims handling, reinsurance, customer '\n",
      "                'service, and regulatory compliance (APRA standards).\\n'\n",
      "                '\\n'\n",
      "                'Your task is to create ONE high-quality supervised fine-tuning example based solely on the provided '\n",
      "                'regulatory text. The example should reflect real-w... [truncated]'}\n"
     ]
    }
   ],
   "source": [
    "# Helper: print loaded PROMPTS for quick review\n",
    "import pprint\n",
    "log(\"PROMPTS loaded from: \" + str(PROMPTS_PATH))\n",
    "keys = list(PROMPTS.keys())\n",
    "log(\"PROMPTS keys: \" + \", \".join(keys))\n",
    "# Print truncated preview of each prompt to avoid flooding the notebook\n",
    "preview = {}\n",
    "for k, v in PROMPTS.items():\n",
    "    if isinstance(v, str):\n",
    "        preview[k] = v if len(v) < 400 else v[:400] + \"... [truncated]\"\n",
    "    else:\n",
    "        preview[k] = str(type(v))\n",
    "print(\"\\nPROMPTS preview (truncated):\")\n",
    "pp = pprint.PrettyPrinter(indent=2, width=120)\n",
    "pp.pprint(preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8012657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_connection():\n",
    "    test_url = f\"{OLLAMA_URL}/api/tags\"  # Lists loaded models\n",
    "    log(\"Testing Ollama connection...\", end=\" \")\n",
    "    try:\n",
    "        response = requests.get(test_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        models_data = response.json()\n",
    "        model_names = [m[\"name\"] for m in models_data.get(\"models\", [])]\n",
    "        log(\"[OK]\")\n",
    "        if model_names:\n",
    "            log(f\"Available models: {', '.join(model_names)}\")\n",
    "        else:\n",
    "            log(\"No models currently loaded on the server.\")\n",
    "\n",
    "        missing = []\n",
    "        if TEACHER_MODEL not in model_names:\n",
    "            missing.append(TEACHER_MODEL)\n",
    "        if AUDITOR_MODEL not in model_names:\n",
    "            missing.append(AUDITOR_MODEL)\n",
    "        if missing:\n",
    "            log(f\"Warning: Required models not loaded: {', '.join(missing)}\")\n",
    "            log(\"Please pull them with: ollama pull <model>\")\n",
    "        return True\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        log(\"[FAILED] Connection timeout (10s)\")\n",
    "        return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        log(\"[FAILED] Cannot connect to Ollama server\")\n",
    "        log(f\"Check if Ollama is running on {OLLAMA_URL} and network is reachable\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        log(f\"[FAILED] Unexpected error: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH SETUP (small cell)\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).parent.resolve()\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd().resolve()\n",
    "\n",
    "RAW_DATA_DIR = SCRIPT_DIR / \"data\" / \"raw\" / \"in-progress\"\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "PROCESSED_DIR = SCRIPT_DIR / \"data\" / \"processed\" / TIMESTAMP\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log(f\"Input PDFs: {RAW_DATA_DIR}\")\n",
    "log(f\"Outputs will be saved to: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4dc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache backends imports & initialization (focused)\n",
    "# Prefer Redis when REDIS_URL is set; otherwise fall back to DiskCache for local use.\n",
    "try:\n",
    "    import redis\n",
    "except Exception:\n",
    "    redis = None\n",
    "\n",
    "try:\n",
    "    from diskcache import Cache as DiskCache\n",
    "except Exception:\n",
    "    DiskCache = None\n",
    "\n",
    "_cache_client = None\n",
    "\n",
    "def init_cache():\n",
    "    \"\"\"Initialize cache client: prefer Redis if REDIS_URL provided, otherwise DiskCache.\"\"\"\n",
    "    global _cache_client\n",
    "    if _cache_client is not None:\n",
    "        return\n",
    "\n",
    "    if REDIS_URL and redis is not None:\n",
    "        try:\n",
    "            _cache_client = redis.from_url(REDIS_URL)\n",
    "            _cache_client.ping()\n",
    "            log(f\"Using Redis cache at {REDIS_URL}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            log(f\"Could not connect to Redis ({e}), falling back to DiskCache\")\n",
    "\n",
    "    if DiskCache is None:\n",
    "        log(\"DiskCache not available. Install with `pip install diskcache` or set REDIS_URL to a running Redis server.\")\n",
    "        raise RuntimeError(\"No cache backend available\")\n",
    "\n",
    "    _cache_client = DiskCache(CACHE_DIR)\n",
    "    log(f\"Using DiskCache at {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashing utilities (small cell)\n",
    "import hashlib\n",
    "\n",
    "def pdf_sha256(path: Path) -> str:\n",
    "    \"\"\"Return SHA256 hex of a file's bytes.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def chunk_sha256(chunk: str) -> str:\n",
    "    return hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache get/set helpers (small cell)\n",
    "import json\n",
    "\n",
    "def cache_chunks(pdf_hash: str, chunks, ttl=CHUNK_TTL):\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        _cache_client.set(key, json.dumps(chunks, ensure_ascii=False), ex=ttl)\n",
    "    else:\n",
    "        _cache_client.set(key, chunks, expire=ttl)\n",
    "\n",
    "\n",
    "def get_cached_chunks(pdf_hash: str):\n",
    "    init_cache()\n",
    "    key = f\"chunks:{pdf_hash}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        v = _cache_client.get(key)\n",
    "        return json.loads(v) if v else None\n",
    "    else:\n",
    "        return _cache_client.get(key)\n",
    "\n",
    "\n",
    "def cache_sft_pair(chunk_hash: str, pair, ttl=SFT_TTL):\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        _cache_client.set(key, json.dumps(pair, ensure_ascii=False), ex=ttl)\n",
    "    else:\n",
    "        _cache_client.set(key, pair, expire=ttl)\n",
    "\n",
    "\n",
    "def get_cached_sft_pair(chunk_hash: str):\n",
    "    init_cache()\n",
    "    key = f\"sft:{chunk_hash}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        v = _cache_client.get(key)\n",
    "        return json.loads(v) if v else None\n",
    "    else:\n",
    "        return _cache_client.get(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP session setup (connection pooling & retries)\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import requests\n",
    "\n",
    "SESSION = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(pool_connections=MAX_LLM_CONCURRENCY * 2, pool_maxsize=MAX_LLM_CONCURRENCY * 2, max_retries=retries)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "SESSION.mount(\"https://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS: simple in-memory timing recorder for LLM calls\n",
    "METRICS = {\"calls\": []}\n",
    "\n",
    "def record_call(model: str, duration: float, success: bool, error: str | None = None):\n",
    "    METRICS[\"calls\"].append({\"model\": model, \"duration\": duration, \"success\": bool(success), \"error\": str(error) if error else None})\n",
    "\n",
    "\n",
    "def summarise_metrics():\n",
    "    import statistics\n",
    "    by_model = {}\n",
    "    for c in METRICS[\"calls\"]:\n",
    "        m = c[\"model\"]\n",
    "        by_model.setdefault(m, []).append(c)\n",
    "\n",
    "    lines = []\n",
    "    for m, calls in by_model.items():\n",
    "        durations = [c[\"duration\"] for c in calls if c[\"duration\"] is not None]\n",
    "        successes = sum(1 for c in calls if c[\"success\"])\n",
    "        total = len(calls)\n",
    "        mean = statistics.mean(durations) if durations else 0\n",
    "        p95 = sorted(durations)[int(len(durations) * 0.95)] if durations else 0\n",
    "        lines.append(f\"{m}: calls={total} success={successes} mean={mean:.2f}s p95={p95:.2f}s\")\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama(model, prompt, system_prompt=\"\", session=None):\n",
    "    \"\"\"Send a single chat request to Ollama using a shared session by default.\n",
    "    Records per-call duration and success into METRICS.\n",
    "    \"\"\"\n",
    "    session = session or SESSION\n",
    "    url = f\"{OLLAMA_URL}/api/chat\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt} if system_prompt else None,\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"num_predict\": 1024\n",
    "        }\n",
    "    }\n",
    "    # Remove None from messages list\n",
    "    payload[\"messages\"] = [m for m in payload[\"messages\"] if m is not None]\n",
    "\n",
    "    log(f\"      > Sending to {model}...\", end=\"\")\n",
    "\n",
    "    for attempt in range(3):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = session.post(url, json=payload, timeout=300)\n",
    "            response.raise_for_status()\n",
    "            duration = time.time() - start\n",
    "\n",
    "            data = response.json()\n",
    "            raw_content = data[\"message\"][\"content\"]\n",
    "            log(\" [Done]\")\n",
    "\n",
    "            log(f\"      Raw output from {model}:\\n{raw_content[:500]}...\")\n",
    "\n",
    "            content = raw_content.strip()\n",
    "            if content.startswith(\"```json\"):\n",
    "                content = content[7:]\n",
    "            if content.endswith(\"```\"):\n",
    "                content = content[:-3]\n",
    "            content = content.strip()\n",
    "\n",
    "            try:\n",
    "                parsed = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                log(\"      ! Direct parse failed. Trying regex extraction...\")\n",
    "                match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if match:\n",
    "                    parsed = json.loads(match.group(0))\n",
    "                else:\n",
    "                    log(\"      ! Could not extract valid JSON\")\n",
    "                    record_call(model, duration, False, error=\"invalid-json\")\n",
    "                    return None\n",
    "\n",
    "            record_call(model, duration, True)\n",
    "            return parsed\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            duration = time.time() - start\n",
    "            record_call(model, duration, False, error=str(e))\n",
    "            log(f\"\\n      ! Attempt {attempt+1} failed: HTTP {response.status_code} {str(e)}\")\n",
    "            if response.text:\n",
    "                log(f\"      Response: {response.text[:300]}\")\n",
    "        except Exception as e:\n",
    "            duration = time.time() - start\n",
    "            record_call(model, duration, False, error=str(e))\n",
    "            log(f\"\\n      ! Attempt {attempt+1} failed: {str(e)[:200]}\")\n",
    "            time.sleep(3)\n",
    "\n",
    "    record_call(model, None, False, error=\"all attempts failed\")\n",
    "    log(f\"      ! All attempts failed for {model}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking helper (focused)\n",
    "\n",
    "def chunk_text_to_chunks(md_content: str, min_len: int = 300, max_len: int = 3000):\n",
    "    \"\"\"Split markdown content into chunks (paragraphs) with length constraints.\"\"\"\n",
    "    chunks = [c.strip() for c in md_content.split(\"\\n\\n\") if min_len < len(c.strip()) < max_len]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf20a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_audit(chunk):\n",
    "    \"\"\"Pipeline:\n",
    "    1. Teacher generates a realistic, high-quality SFT example for life insurance operations.\n",
    "    2. Auditor verifies factual accuracy against the source text.\n",
    "    Returns a clean {\"instruction\": ..., \"output\": ...} pair.\n",
    "    Records pipeline timing as `pipeline:generate_and_audit` in METRICS.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # --- TEACHER: Generate operational-focused training pair (loaded from config) ---\n",
    "    system_gen = PROMPTS.get(\"system_gen\")\n",
    "    gen_prompt = PROMPTS.get(\n",
    "        \"gen_prompt_template\",\n",
    "        \"Using only the following extract from Australian life insurance regulatory documentation, create one high-quality training example for fine-tuning an LLM to excel in life insurance operations:\\n\\n{chunk}\"\n",
    "    ).format(chunk=chunk)\n",
    "\n",
    "    raw_pair = call_ollama(TEACHER_MODEL, gen_prompt, system_gen)\n",
    "    if not raw_pair:\n",
    "        return None\n",
    "\n",
    "    # --- AUDITOR: Strict fact-checking against source (loaded from config) ---\n",
    "    audit_system = PROMPTS.get(\"audit_system\")\n",
    "    audit_prompt = PROMPTS.get(\n",
    "        \"audit_prompt_template\",\n",
    "        \"Source Text:\\n{chunk}\\n\\nGenerated Pair:\\n{generated}\\n\\nVerify factual accuracy against the source. Correct errors. Return only the final valid JSON.\"\n",
    "    ).format(chunk=chunk, generated=json.dumps(raw_pair, indent=2))\n",
    "\n",
    "    final_pair = call_ollama(AUDITOR_MODEL, audit_prompt, audit_system)\n",
    "\n",
    "    pipeline_duration = time.time() - start\n",
    "    record_call(\"pipeline:generate_and_audit\", pipeline_duration, True if final_pair else False)\n",
    "\n",
    "    return final_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e43ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_audit_single(chunk):\n",
    "    \"\"\"Single-call pipeline: ask one model to both generate an SFT pair and audit it in the same prompt.\n",
    "    May reduce total time by eliminating a second round-trip (auditor call).\n",
    "    \"\"\"\n",
    "    system_prompt = PROMPTS.get(\"single_call_system\")\n",
    "    prompt = PROMPTS.get(\"single_call_prompt_template\", \"Source Text:\\n{chunk}\\n\\nCreate the training example and self-audit it; return only the final JSON.\").format(chunk=chunk)\n",
    "\n",
    "    start = time.time()\n",
    "    out = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    record_call(\"pipeline:single_generate_and_audit\", time.time() - start, True if out else False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_json_array_or_objects(text: str):\n",
    "    \"\"\"Attempt to parse a response that is either a JSON array or multiple JSON objects.\n",
    "    Returns list of parsed objects or None.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, dict):\n",
    "            return [parsed]\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "    except Exception:\n",
    "        # Fallback: extract all {...} objects with regex\n",
    "        objs = re.findall(r\"\\{(?:[^{}]|(?R))*\\}\", text, flags=re.DOTALL)\n",
    "        results = []\n",
    "        for o in objs:\n",
    "            try:\n",
    "                results.append(json.loads(o))\n",
    "            except Exception:\n",
    "                continue\n",
    "        if results:\n",
    "            return results\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_and_audit_batch(chunks: list[str]):\n",
    "    \"\"\"Batch pipeline: send multiple chunks in a single prompt and return a list of SFT pairs.\n",
    "    Returns a list in same order as `chunks` or None on failure.\n",
    "    \"\"\"\n",
    "    # Validate prompt size\n",
    "    total_chars = sum(len(c) for c in chunks)\n",
    "    if total_chars > MAX_BATCH_CHARS:\n",
    "        log(f\"      ! Batch too large ({total_chars} chars) — reduce BATCH_SIZE\")\n",
    "        return None\n",
    "\n",
    "    system_prompt = PROMPTS.get(\"batch_system\")\n",
    "    # Compose prompt with numbered blocks using template from config\n",
    "    parts = []\n",
    "    block_template = PROMPTS.get(\"batch_block_template\", \"--- SOURCE {i} ---\\\\n{chunk}\\\\n\")\n",
    "    for i, c in enumerate(chunks, start=1):\n",
    "        parts.append(block_template.format(i=i, chunk=c))\n",
    "    prompt = \"\\n\".join(parts)\n",
    "\n",
    "    start = time.time()\n",
    "    raw = call_ollama(TEACHER_MODEL, prompt, system_prompt)\n",
    "    duration = time.time() - start\n",
    "    # call_ollama already recorded low-level call metrics; record batch pipeline time explicitly\n",
    "    record_call(\"pipeline:batch_generate_and_audit\", duration, True if raw else False)\n",
    "\n",
    "    if not raw:\n",
    "        return None\n",
    "\n",
    "    # raw should be parsed JSON (list or object). If call_ollama returned dict/list already, handle it.\n",
    "    if isinstance(raw, list):\n",
    "        return raw\n",
    "    if isinstance(raw, dict):\n",
    "        # Single object — wrap\n",
    "        return [raw]\n",
    "\n",
    "    # If raw is string (call_ollama normally returns parsed), attempt parsing from raw string\n",
    "    # But in our implementation call_ollama returns parsed JSON or None. We'll still be robust:\n",
    "    parsed = _parse_json_array_or_objects(str(raw))\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_chunk(chunk: str, idx: int, semaphore: threading.BoundedSemaphore):\n",
    "    \"\"\"Process a single chunk: check cache, run pipeline (single-call or two-step), cache result.\n",
    "    When USE_SINGLE_CALL is True we optionally run the strict two-step auditor on a random sample defined by AUDIT_SAMPLE_RATE.\n",
    "    \"\"\"\n",
    "    chunk_hash = chunk_sha256(chunk)\n",
    "\n",
    "    cached_pair = get_cached_sft_pair(chunk_hash)\n",
    "    if cached_pair:\n",
    "        log(f\"      > Cache hit for chunk {idx+1}\")\n",
    "        return idx, json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "\n",
    "    with semaphore:\n",
    "        if USE_BATCHING:\n",
    "            # batching handled at higher level; this function should not be used for batching mode\n",
    "            raise RuntimeError(\"process_chunk shouldn't be used in BATCHING mode\")\n",
    "\n",
    "        if USE_SINGLE_CALL:\n",
    "            sft_pair = generate_and_audit_single(chunk)\n",
    "            # occasional strict audit checks (detect regressions without doubling runtime)\n",
    "            if sft_pair and random.random() < AUDIT_SAMPLE_RATE:\n",
    "                strict_pair = generate_and_audit(chunk)\n",
    "                if strict_pair:\n",
    "                    # Prefer strict audited pair\n",
    "                    sft_pair = strict_pair\n",
    "        else:\n",
    "            sft_pair = generate_and_audit(chunk)\n",
    "\n",
    "    if sft_pair and isinstance(sft_pair, dict) and \"instruction\" in sft_pair:\n",
    "        cache_sft_pair(chunk_hash, sft_pair)\n",
    "        return idx, json.dumps(sft_pair, ensure_ascii=False) + \"\\n\"\n",
    "    else:\n",
    "        log(f\"      ! Failed chunk {idx+1}\")\n",
    "        return idx, None\n",
    "\n",
    "\n",
    "def process_pdfs_with_batching(max_chunks_per_pdf: int = None):\n",
    "    \"\"\"Process PDFs in batching mode. Groups uncached chunks in batches and sends each batch in one LLM request.\"\"\"\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        log(f\"Error: No PDF files found in {RAW_DATA_DIR}\")\n",
    "        log(f\"Note: Place your PDFs in: {RAW_DATA_DIR}\")\n",
    "        return\n",
    "\n",
    "    log(f\"Found {len(pdf_files)} PDF(s) in 'in-progress' folder.\")\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "\n",
    "        log(f\"\\n--- Processing (BATCH MODE): {pdf_path.name} → {output_file.name} ---\")\n",
    "\n",
    "        try:\n",
    "            result = converter.convert(pdf_path)\n",
    "            md_content = result.document.export_to_markdown()\n",
    "        except Exception as e:\n",
    "            log(f\"      ! Failed to convert PDF: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Chunk and cache per-PDF\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        cached = get_cached_chunks(pdf_hash)\n",
    "        if cached:\n",
    "            chunks = cached\n",
    "            log(f\"Using {len(chunks)} cached chunks for {pdf_path.name}\")\n",
    "        else:\n",
    "            chunks = chunk_text_to_chunks(md_content)\n",
    "            cache_chunks(pdf_hash, chunks)\n",
    "            log(f\"Extracted and cached {len(chunks)} chunks.\")\n",
    "\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        log(f\"Processing {process_count} chunks in batch mode (BATCH_SIZE={BATCH_SIZE})...\")\n",
    "\n",
    "        # Determine uncached indices\n",
    "        uncached_indices = []\n",
    "        for i in range(process_count):\n",
    "            chh = chunk_sha256(chunks[i])\n",
    "            if get_cached_sft_pair(chh) is None:\n",
    "                uncached_indices.append(i)\n",
    "\n",
    "        # Build batches of indices but respect MAX_BATCH_CHARS\n",
    "        batches = []\n",
    "        current = []\n",
    "        current_chars = 0\n",
    "        for idx in uncached_indices:\n",
    "            c = chunks[idx]\n",
    "            if len(current) >= BATCH_SIZE or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "                batches.append(current)\n",
    "                current = []\n",
    "                current_chars = 0\n",
    "            current.append(idx)\n",
    "            current_chars += len(c)\n",
    "        if current:\n",
    "            batches.append(current)\n",
    "\n",
    "        successful_entries = 0\n",
    "        file_lock = threading.Lock()\n",
    "        results_buffer = [None] * process_count\n",
    "\n",
    "        # Process batches with a thread pool bounded by BATCH_CONCURRENCY\n",
    "        with ThreadPoolExecutor(max_workers=BATCH_CONCURRENCY) as executor:\n",
    "            future_to_batch = {}\n",
    "            for batch_idxs in batches:\n",
    "                batch_chunks = [chunks[i] for i in batch_idxs]\n",
    "                future = executor.submit(generate_and_audit_batch, batch_chunks)\n",
    "                future_to_batch[future] = batch_idxs\n",
    "\n",
    "            for future in tqdm(as_completed(future_to_batch), total=len(future_to_batch), desc=f\"BATCH SFT [{pdf_stem}]\"):\n",
    "                batch_idxs = future_to_batch[future]\n",
    "                try:\n",
    "                    out_list = future.result()\n",
    "                except Exception as e:\n",
    "                    log(f\"      ! Batch failed: {e}\")\n",
    "                    out_list = None\n",
    "\n",
    "                if out_list and isinstance(out_list, list):\n",
    "                    # Expect out_list to be same length (or at least same number) as batch_chunks\n",
    "                    for idx_in_batch, obj in enumerate(out_list):\n",
    "                        target_idx = batch_idxs[idx_in_batch] if idx_in_batch < len(batch_idxs) else None\n",
    "                        if target_idx is not None and isinstance(obj, dict) and \"instruction\" in obj:\n",
    "                            cache_sft_pair(chunk_sha256(chunks[target_idx]), obj)\n",
    "                            results_buffer[target_idx] = json.dumps(obj, ensure_ascii=False) + \"\\n\"\n",
    "                else:\n",
    "                    log(\"      ! Batch returned invalid output\")\n",
    "\n",
    "        # For any still missing (either cache hit earlier or not produced), fill from cache\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "\n",
    "        # Write results in order\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    with file_lock:\n",
    "                        f.write(line)\n",
    "                        f.flush()\n",
    "                        successful_entries += 1\n",
    "\n",
    "        log(f\"Saved {successful_entries}/{process_count} high-quality entries → {output_file}\")\n",
    "\n",
    "    log(f\"\\nAll processing complete! Outputs in: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bbf424",
   "metadata": {},
   "source": [
    "## Optional: Redis Docker Compose (use if you want a Redis server)\n",
    "\n",
    "If you prefer Redis, add this to a `docker-compose.redis.yml` and run `docker compose -f docker-compose.redis.yml up -d`.\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  redis:\n",
    "    image: redis:7\n",
    "    restart: unless-stopped\n",
    "    ports:\n",
    "      - '6379:6379'\n",
    "    volumes:\n",
    "      - redis-data:/data\n",
    "\n",
    "volumes:\n",
    "  redis-data:\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- Point `REDIS_URL` env var to `redis://localhost:6379/0` in your `.env` file to enable Redis caching.\n",
    "- If `REDIS_URL` is not set or Redis is unreachable, the notebook falls back to `diskcache` (no extra service needed).\n",
    "\n",
    "---\n",
    "\n",
    "## Local (no-docker) alternative: DiskCache ✅\n",
    "\n",
    "- `diskcache` is a fast, pure-Python on-disk cache (`pip install diskcache`) and is used by default if Redis is not configured.\n",
    "- It provides TTLs, eviction, and works well for local experimentation without running external services.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01304fcc",
   "metadata": {},
   "source": [
    "### Quick setup & env vars\n",
    "\n",
    "- Install dependencies:\n",
    "\n",
    "```bash\n",
    "pip install diskcache redis\n",
    "```\n",
    "\n",
    "- .env recommendations:\n",
    "\n",
    "```\n",
    "OLLAMA_URL=http://localhost:11434\n",
    "TEACHER_MODEL=qwen2.5:72b-instruct\n",
    "AUDITOR_MODEL=deepseek-r1:70b\n",
    "# Optional: set to use redis\n",
    "REDIS_URL=redis://localhost:6379/0\n",
    "# Concurrency\n",
    "MAX_LLM_CONCURRENCY=8\n",
    "# Batching (optional)\n",
    "USE_BATCHING=0\n",
    "BATCH_SIZE=4\n",
    "BATCH_CONCURRENCY=2\n",
    "# Single-call self-audit (recommended)\n",
    "USE_SINGLE_CALL=1\n",
    "AUDIT_SAMPLE_RATE=0.05\n",
    "```\n",
    "\n",
    "- Run notes:\n",
    "  - By default the notebook uses DiskCache (no Redis required).\n",
    "  - To enable Redis, start the Redis service (Docker compose above) and set `REDIS_URL` in `.env`.\n",
    "  - `USE_SINGLE_CALL=1` uses a single model call that both generates and self-audits (big speedup).\n",
    "  - `USE_BATCHING=1` groups N chunks per prompt (see `BATCH_SIZE`). This reduces number of LLM requests but increases per-call latency — test and tune `BATCH_SIZE` and `BATCH_CONCURRENCY` for your machine.\n",
    "  - Use `AUDIT_SAMPLE_RATE` to run strict two-step auditing on a small random sample (recommended default 5%).\n",
    "\n",
    "---\n",
    "\n",
    "This completes the caching + bounded concurrency + batching implementation. Next you may run the notebook and observe batch vs single-call trade-offs using the benchmark cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch throughput benchmark: USE_BATCHING True, BATCH_SIZE 4\n",
    "USE_BATCHING = True\n",
    "AUDIT_SAMPLE_RATE = 0.0\n",
    "BATCH_SIZE = 4\n",
    "BATCH_CONCURRENCY = 2\n",
    "MAX_LLM_CONCURRENCY = 2\n",
    "\n",
    "# Reconfigure session\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "SESSION = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(pool_connections=MAX_LLM_CONCURRENCY * 2, pool_maxsize=MAX_LLM_CONCURRENCY * 2, max_retries=retries)\n",
    "SESSION.mount(\"http://\", adapter)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "\n",
    "# Prepare 8 uncached chunks\n",
    "pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "pdf_path = pdf_files[0]\n",
    "pdf_hash = pdf_sha256(pdf_path)\n",
    "chunks = get_cached_chunks(pdf_hash) or []\n",
    "if len(chunks) < 8:\n",
    "    raise RuntimeError(\"Not enough chunks to run batch throughput test\")\n",
    "for i in range(8):\n",
    "    chh = chunk_sha256(chunks[i])\n",
    "    key = f\"sft:{chh}\"\n",
    "    if redis is not None and isinstance(_cache_client, redis.Redis):\n",
    "        _cache_client.delete(key)\n",
    "    else:\n",
    "        try:\n",
    "            del _cache_client[key]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Clear metrics and run batch processing\n",
    "METRICS[\"calls\"].clear()\n",
    "process_pdfs_with_batching(max_chunks_per_pdf=8)\n",
    "print(\"\\n--- METRICS SUMMARY (batch throughput test) ---\")\n",
    "print(summarise_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autotuner helpers — split into focused cells below\n",
    "# The heavy lifting functions are defined in subsequent small cells for readability and stepwise execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d063f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reconfigure_session_for_concurrency(concurrency: int):\n",
    "    \"\"\"Recreate SESSION to update pool sizes for concurrency.\"\"\"\n",
    "    global SESSION\n",
    "    import requests\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    SESSION = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(pool_connections=concurrency * 2, pool_maxsize=concurrency * 2, max_retries=retries)\n",
    "    SESSION.mount(\"http://\", adapter)\n",
    "    SESSION.mount(\"https://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f69408bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_single_call(chunks, concurrency=1, repeat=1):\n",
    "    \"\"\"Benchmark generate_and_audit_single on given chunks with specified concurrency.\n",
    "    Returns throughput (chunks/sec) and summary dict.\n",
    "    \"\"\"\n",
    "    _reconfigure_session_for_concurrency(concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_single, c) for c in chunks]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        success = sum(1 for r in results if r and isinstance(r, dict) and \"instruction\" in r)\n",
    "        return duration, success\n",
    "\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(s for _, s in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\": \"single_call\", \"concurrency\": concurrency, \"throughput\": throughput, \"total_processed\": total_processed, \"total_time\": total_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c75c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_batch(chunks, batch_size=4, batch_concurrency=1, repeat=1):\n",
    "    \"\"\"Benchmark batching pipeline using generate_and_audit_batch and return throughput.\n",
    "    \"\"\"\n",
    "    # Build batches\n",
    "    batches = []\n",
    "    current = []\n",
    "    current_chars = 0\n",
    "    for c in chunks:\n",
    "        if len(current) >= batch_size or (current_chars + len(c)) > MAX_BATCH_CHARS:\n",
    "            batches.append(current)\n",
    "            current = []\n",
    "            current_chars = 0\n",
    "        current.append(c)\n",
    "        current_chars += len(c)\n",
    "    if current:\n",
    "        batches.append(current)\n",
    "\n",
    "    _reconfigure_session_for_concurrency(batch_concurrency)\n",
    "    METRICS[\"calls\"].clear()\n",
    "\n",
    "    def _run_once():\n",
    "        t0 = time.perf_counter()\n",
    "        with ThreadPoolExecutor(max_workers=batch_concurrency) as ex:\n",
    "            futures = [ex.submit(generate_and_audit_batch, b) for b in batches]\n",
    "            results = [f.result() for f in futures]\n",
    "        t1 = time.perf_counter()\n",
    "        duration = t1 - t0\n",
    "        processed = 0\n",
    "        for res in results:\n",
    "            if isinstance(res, list):\n",
    "                processed += len(res)\n",
    "        return duration, processed\n",
    "\n",
    "    runs = [_run_once() for _ in range(repeat)]\n",
    "    total_processed = sum(p for _, p in runs)\n",
    "    total_time = sum(d for d, _ in runs)\n",
    "    throughput = total_processed / total_time if total_time > 0 else 0\n",
    "    return {\"mode\": \"batch\", \"batch_size\": batch_size, \"batch_concurrency\": batch_concurrency, \"throughput\": throughput, \"total_processed\": total_processed, \"total_time\": total_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6650f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autotune(chunks, concurrency_options=[1,2,4], batch_sizes=[None,2,4], batch_concurrency_options=[1,2], repeat=1):\n",
    "    \"\"\"Run benchmarks across options and return the best config by throughput.\"\"\"\n",
    "    results = []\n",
    "    print(f\"Autotune: testing {len(chunks)} chunks | single-call conc: {concurrency_options} | batch_sizes: {batch_sizes} | batch_conc: {batch_concurrency_options}\")\n",
    "\n",
    "    for conc in concurrency_options:\n",
    "        r = benchmark_single_call(chunks, concurrency=conc, repeat=repeat)\n",
    "        print(f\"single_call conc={conc} -> throughput={r['throughput']:.3f} chunks/sec\")\n",
    "        results.append(r)\n",
    "\n",
    "    for bsize in [b for b in batch_sizes if b is not None]:\n",
    "        for bconc in batch_concurrency_options:\n",
    "            r = benchmark_batch(chunks, batch_size=bsize, batch_concurrency=bconc, repeat=repeat)\n",
    "            print(f\"batch bsize={bsize} bconc={bconc} -> throughput={r['throughput']:.3f} chunks/sec\")\n",
    "            results.append(r)\n",
    "\n",
    "    best = max(results, key=lambda x: x[\"throughput\"]) if results else None\n",
    "    sorted_results = sorted(results, key=lambda x: x[\"throughput\"], reverse=True)\n",
    "    print(\"\\nTop configs:\")\n",
    "    for s in sorted_results[:5]:\n",
    "        print(s)\n",
    "\n",
    "    print(\"\\nBest config:\", best)\n",
    "    return {\"best\": best, \"all\": sorted_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0221edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6 probe chunks from Priority_Protection_Product_Disclosure_Statement.pdf\n"
     ]
    }
   ],
   "source": [
    "# Autotuner probe setup (small cell)\n",
    "PROBE_CHUNKS = 6  # small sample by default\n",
    "pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    raise RuntimeError(\"No PDFs found in RAW_DATA_DIR for autotune probe\")\n",
    "\n",
    "pdf_path = pdf_files[0]\n",
    "pdf_hash = pdf_sha256(pdf_path)\n",
    "chunks = get_cached_chunks(pdf_hash)\n",
    "if not chunks:\n",
    "    raise RuntimeError(\"No cached chunks found. Run the pipeline or set CACHE first\")\n",
    "\n",
    "probe_chunks = chunks[:PROBE_CHUNKS]\n",
    "print(f\"Using {len(probe_chunks)} probe chunks from {pdf_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a4a25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running autotuner (quick probe). This will use the live models and may incur runtime.\n",
      "Autotune: testing 6 chunks | single-call conc: [1, 2] | batch_sizes: [None, 2, 4] | batch_conc: [1, 2]\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"Where can detailed information about the replacement of existing Priority Protection plans be found?\", \"output\": \"Detailed information about the replacement of existing Priority Protection Income Protection and Income Protection Accident Only plans, as well as Priority Protection plans with Term Level premiums, is incorporated by reference in this PDS. The material (AIA07702-11/25) can be accessed on the aia.com.au website or requested free of charge by contacting AIA on 1800 33...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"What is the AIA Insurance Superannuation Scheme No2, and who is the trustee?\", \"output\": \"The AIA Insurance Superannuation Scheme No2 (the Scheme) is a risk-only superannuation product that is part of the Smart Future Trust ABN 68 964 712 340. The trustee of the Scheme is Equity Trustees Superannuation Limited (ABN 50 055 641 757, AFSL 229757, RSE License L0001458).\"}...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"How can an individual apply for Priority Protection in Australia?\", \"output\": \"An individual can apply for Priority Protection through an Australian Financial Services Licensee (AFSL) that is authorised to sell life insurance. This includes a financial adviser who is an authorised representative of the AFSL. The AFSL may offer Priority Protection with certain limitations, such as restricting options, benefits, and sums insured, and may do so without providing personal financial ...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"Who is responsible for issuing interests in superannuation funds according to the PDS?\", \"output\": \"The trustee of the Scheme is responsible for issuing interests in superannuation funds, as AIA Australia is not an RSE licensee and legally not able to issue such interests.\"}...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"What should a potential investor do before deciding to acquire Priority Protection or Priority Protection for Platform Investors insurance products?\", \"output\": \"A potential investor should read the PDS, including any incorporated by reference material, before making a decision to acquire Priority Protection or Priority Protection for Platform Investors insurance products. The PDS is designed to help investors determine if these products will meet their needs and to compare them...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"What is the purpose of the general nature statement in a PDS?\", \"output\": \"The general nature statement in a PDS serves to inform readers that the information provided is not tailored to their specific objectives, financial situation, or needs. It emphasizes that before making any decision based on this information, individuals should consider its appropriateness for their personal circumstances.\"}...\n",
      "single_call conc=1 -> throughput=0.105 chunks/sec\n",
      "      > Sending to qwen2.5:72b-instruct...      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"What is the AIA Insurance Superannuation Scheme No2, and who is the trustee?\", \"output\": \"The AIA Insurance Superannuation Scheme No2 (the Scheme) is a risk-only superannuation product that is part of the Smart Future Trust ABN 68 964 712 340. The trustee of the Scheme is Equity Trustees Superannuation Limited (ABN 50 055 641 757, AFSL 229757, RSE License L0001458).\"}...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"Where can detailed information about the replacement of existing Priority Protection plans be found?\", \"output\": \"Detailed information about the replacement of existing Priority Protection Income Protection and Income Protection Accident Only plans, as well as Priority Protection plans with Term Level premiums, is incorporated by reference in this PDS. The material (AIA07702-11/25) can be accessed on the aia.com.au website or requested free of charge by contacting AIA on 1800 33...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"How can an individual apply for Priority Protection in Australia?\", \"output\": \"An individual can apply for Priority Protection through an Australian Financial Services Licensee (AFSL) that is authorised to sell life insurance. This includes a financial adviser who is an authorised representative of the AFSL. The AFSL may offer Priority Protection with certain limitations, such as restricting options, benefits, and sums insured, and may do so without providing personal financial ...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"Who is responsible for issuing interests in superannuation funds according to the PDS?\", \"output\": \"The trustee of the Scheme is responsible for issuing interests in superannuation funds, as AIA Australia is not an RSE licensee and legally not able to issue such interests.\"}...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"What should a potential investor do before deciding to acquire Priority Protection or Priority Protection for Platform Investors insurance products?\", \"output\": \"A potential investor should read the PDS, including any incorporated by reference material, before making a decision to acquire Priority Protection or Priority Protection for Platform Investors insurance products. The PDS is designed to help investors determine if these products will meet their needs and to compare them...\n",
      " [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "{\"instruction\": \"What is the purpose of the general nature statement in a PDS?\", \"output\": \"The general nature statement in a PDS serves to inform readers that the information provided is not tailored to their specific objectives, financial situation, or needs. It emphasizes that before making any decision based on this information, individuals should consider its appropriateness with respect to their personal circumstances.\"}...\n",
      "single_call conc=2 -> throughput=0.113 chunks/sec\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "    {\n",
      "        \"instruction\": \"Where can detailed information on the replacement of existing Priority Protection plans be found?\",\n",
      "        \"output\": \"Detailed information on the replacement of existing Priority Protection Income Protection and Income Protection Accident Only plans with Term Level premiums is incorporated by reference in this PDS. The material (AIA07702-11/25) can be accessed on the aia.com.au website or requested free of charge by contacting AIA on 1800 333 613.\"\n",
      "    },\n",
      "    {\n",
      " ...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "    {\n",
      "        \"instruction\": \"Explain how an individual can apply for Priority Protection through an Australian Financial Services Licensee (AFSL) and what this process entails.\",\n",
      "        \"output\": \"An individual can apply for Priority Protection through an Australian Financial Services Licensee (AFSL) that is authorized to sell life insurance. This includes financial advisers who are authorized representatives of the AFSL. The AFSL may offer Priority Protection with certain limitations, such ...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "    {\n",
      "        \"instruction\": \"What is the purpose of reading the PDS for Priority Protection and Priority Protection for Platform Investors insurance products?\",\n",
      "        \"output\": \"The PDS (including any incorporated by reference material) should be read before making a decision to acquire the Priority Protection or Priority Protection for Platform Investors insurance products or applying for membership of the Scheme. It is intended to help you decide whether these products will meet your need...\n",
      "batch bsize=2 bconc=1 -> throughput=0.093 chunks/sec\n",
      "      > Sending to qwen2.5:72b-instruct...      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "    {\n",
      "        \"instruction\": \"Explain how an individual can apply for Priority Protection through an Australian Financial Services Licensee (AFSL) and what this entails.\",\n",
      "        \"output\": \"An individual can apply for Priority Protection through an Australian Financial Services Licensee (AFSL) that is authorized to sell life insurance. This includes financial advisers who are authorized representatives of the AFSL. The AFSL may offer Priority Protection with certain limitations, such as restr...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "    {\n",
      "        \"instruction\": \"Where can detailed information on the replacement of existing Priority Protection plans be found?\",\n",
      "        \"output\": \"Detailed information on the replacement of existing Priority Protection Income Protection and Income Protection Accident Only plans with Term Level premiums is incorporated by reference in this PDS. The material (AIA07702-11/25) can be accessed on the aia.com.au website or requested free of charge by contacting AIA on 1800 333 613.\"\n",
      "    },\n",
      "    {\n",
      " ...\n",
      " [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "    {\n",
      "        \"instruction\": \"What is the purpose of reading the PDS for Priority Protection and Priority Protection for Platform Investors insurance products?\",\n",
      "        \"output\": \"The PDS (including any incorporated by reference material) should be read before making a decision to acquire the Priority Protection or Priority Protection for Platform Investors insurance products or applying for membership of the Scheme. It is intended to help you decide whether these products will meet your need...\n",
      "batch bsize=2 bconc=2 -> throughput=0.092 chunks/sec\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "  {\n",
      "    \"instruction\": \"Where can detailed information on the replacement of existing Priority Protection plans be found?\",\n",
      "    \"output\": \"Detailed information on the replacement of existing Priority Protection Income Protection and Income Protection Accident Only plans with Term Level premiums is incorporated by reference in this PDS. This material (AIA07702-11/25) can be accessed on the aia.com.au website or requested free of charge by contacting AIA on 1800 333 613.\"\n",
      "  },\n",
      "  {\n",
      "    \"instructi...\n",
      "      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "    {\n",
      "        \"instruction\": \"What is the purpose of reading the PDS for Priority Protection and Priority Protection for Platform Investors insurance products?\",\n",
      "        \"output\": \"The PDS (including any incorporated by reference material) should be read before making a decision to acquire the Priority Protection or Priority Protection for Platform Investors insurance products or applying for membership of the Scheme. It is intended to help you decide whether these products will meet your need...\n",
      "batch bsize=4 bconc=1 -> throughput=0.095 chunks/sec\n",
      "      > Sending to qwen2.5:72b-instruct...      > Sending to qwen2.5:72b-instruct... [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "  {\n",
      "    \"instruction\": \"Where can detailed information on the replacement of existing Priority Protection plans be found?\",\n",
      "    \"output\": \"Detailed information on the replacement of existing Priority Protection Income Protection and Income Protection Accident Only plans with Term Level premiums is incorporated by reference in this PDS. This material (AIA07702-11/25) can be accessed on the aia.com.au website or requested free of charge by contacting AIA on 1800 333 613.\"\n",
      "  },\n",
      "  {\n",
      "    \"instructi...\n",
      " [Done]\n",
      "      Raw output from qwen2.5:72b-instruct:\n",
      "[\n",
      "    {\n",
      "        \"instruction\": \"What is the purpose of reading the PDS for Priority Protection and Priority Protection for Platform Investors insurance products?\",\n",
      "        \"output\": \"The PDS (including any incorporated by reference material) should be read before making a decision to acquire the Priority Protection or Priority Protection for Platform Investors insurance products or applying for membership of the Scheme. It is intended to help you decide whether these products will meet your need...\n",
      "batch bsize=4 bconc=2 -> throughput=0.095 chunks/sec\n",
      "\n",
      "Top configs:\n",
      "{'mode': 'single_call', 'concurrency': 2, 'throughput': 0.11300484792212409, 'total_processed': 6, 'total_time': 53.09506724998937}\n",
      "{'mode': 'single_call', 'concurrency': 1, 'throughput': 0.10483676233695516, 'total_processed': 6, 'total_time': 57.23183229099959}\n",
      "{'mode': 'batch', 'batch_size': 4, 'batch_concurrency': 2, 'throughput': 0.0953141482381946, 'total_processed': 6, 'total_time': 62.949731083004735}\n",
      "{'mode': 'batch', 'batch_size': 4, 'batch_concurrency': 1, 'throughput': 0.09520258681452311, 'total_processed': 6, 'total_time': 63.02349758299533}\n",
      "{'mode': 'batch', 'batch_size': 2, 'batch_concurrency': 1, 'throughput': 0.09310181341505104, 'total_processed': 6, 'total_time': 64.44557608402101}\n",
      "\n",
      "Best config: {'mode': 'single_call', 'concurrency': 2, 'throughput': 0.11300484792212409, 'total_processed': 6, 'total_time': 53.09506724998937}\n",
      "\n",
      "Recommended: USE_SINGLE_CALL=1, MAX_LLM_CONCURRENCY= 2\n"
     ]
    }
   ],
   "source": [
    "# Autotuner probe run (separate small cell) — runs the quick probe and reports recommendation\n",
    "concurrency_options = [1, 2]\n",
    "batch_sizes = [None, 2, 4]\n",
    "batch_concurrency_options = [1, 2]\n",
    "\n",
    "print(\"Running autotuner (quick probe). This will use the live models and may incur runtime.\")\n",
    "res = autotune(probe_chunks, concurrency_options=concurrency_options, batch_sizes=batch_sizes, batch_concurrency_options=batch_concurrency_options, repeat=1)\n",
    "\n",
    "best = res.get(\"best\")\n",
    "if best and best.get(\"mode\") == \"single_call\":\n",
    "    print(\"\\nRecommended: USE_SINGLE_CALL=1, MAX_LLM_CONCURRENCY=\", best[\"concurrency\"])\n",
    "elif best and best.get(\"mode\") == \"batch\":\n",
    "    print(\"\\nRecommended: USE_SINGLE_CALL=0, USE_BATCHING=1, BATCH_SIZE=\", best[\"batch_size\"], \", BATCH_CONCURRENCY=\", best[\"batch_concurrency\"])\n",
    "else:\n",
    "    print(\"No clear best configuration found; inspect results in `res` variable\")\n",
    "\n",
    "AUTOTUNE_RESULTS = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42b7beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration recommendation:\n",
      " {'mode': 'single_call', 'concurrency': 2, 'throughput': 0.11300484792212409, 'total_processed': 6, 'total_time': 53.09506724998937}\n",
      "\n",
      "Top 5 configs:\n",
      "{'concurrency': 2,\n",
      " 'mode': 'single_call',\n",
      " 'throughput': 0.11300484792212409,\n",
      " 'total_processed': 6,\n",
      " 'total_time': 53.09506724998937}\n",
      "{'concurrency': 1,\n",
      " 'mode': 'single_call',\n",
      " 'throughput': 0.10483676233695516,\n",
      " 'total_processed': 6,\n",
      " 'total_time': 57.23183229099959}\n",
      "{'batch_concurrency': 2,\n",
      " 'batch_size': 4,\n",
      " 'mode': 'batch',\n",
      " 'throughput': 0.0953141482381946,\n",
      " 'total_processed': 6,\n",
      " 'total_time': 62.949731083004735}\n",
      "{'batch_concurrency': 1,\n",
      " 'batch_size': 4,\n",
      " 'mode': 'batch',\n",
      " 'throughput': 0.09520258681452311,\n",
      " 'total_processed': 6,\n",
      " 'total_time': 63.02349758299533}\n",
      "{'batch_concurrency': 1,\n",
      " 'batch_size': 2,\n",
      " 'mode': 'batch',\n",
      " 'throughput': 0.09310181341505104,\n",
      " 'total_processed': 6,\n",
      " 'total_time': 64.44557608402101}\n"
     ]
    }
   ],
   "source": [
    "# Summarize AUTOTUNE_RESULTS (top 5) for quick review\n",
    "from pprint import pprint\n",
    "try:\n",
    "    best = AUTOTUNE_RESULTS.get(\"best\")\n",
    "    print(\"Best configuration recommendation:\\n\", best)\n",
    "    print('\\nTop 5 configs:')\n",
    "    for s in AUTOTUNE_RESULTS.get(\"all\", [])[:5]:\n",
    "        pprint(s)\n",
    "except Exception as e:\n",
    "    print(\"No AUTOTUNE_RESULTS found:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: print recent METRICS calls sequence\n",
    "for i, c in enumerate(METRICS[\"calls\"][-16:]):\n",
    "    print(i+1, c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ddcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preview: load latest processed .train.jsonl, show samples and basic stats\n",
    "import json, random, statistics\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def load_jsonl(path):\n",
    "    valid = []\n",
    "    invalid = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, raw in enumerate(f):\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                valid.append(obj)\n",
    "            except Exception as e:\n",
    "                invalid.append((i, str(e), line[:300]))\n",
    "    return valid, invalid\n",
    "\n",
    "\n",
    "def summarize_entries(entries):\n",
    "    inst_lens = [len(e.get(\"instruction\", \"\")) for e in entries]\n",
    "    out_lens = [len(e.get(\"output\", \"\")) for e in entries]\n",
    "    return {\n",
    "        \"count\": len(entries),\n",
    "        \"inst_mean\": round(statistics.mean(inst_lens), 1) if inst_lens else 0,\n",
    "        \"inst_median\": int(statistics.median(inst_lens)) if inst_lens else 0,\n",
    "        \"out_mean\": round(statistics.mean(out_lens), 1) if out_lens else 0,\n",
    "        \"out_median\": int(statistics.median(out_lens)) if out_lens else 0,\n",
    "    }\n",
    "\n",
    "# Find latest processed file\n",
    "files = sorted(PROCESSED_DIR.glob(\"*.train.jsonl\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not files:\n",
    "    print(\"No processed .train.jsonl files found in:\", PROCESSED_DIR)\n",
    "else:\n",
    "    path = files[0]\n",
    "    print(\"Previewing:\", path)\n",
    "\n",
    "    entries, invalid = load_jsonl(path)\n",
    "    total_lines = sum(1 for _ in open(path, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "    print(f\"Total lines in file: {total_lines}\")\n",
    "    print(f\"Valid entries: {len(entries)} | Invalid lines: {len(invalid)}\")\n",
    "    print(\"\\nBasic stats:\", summarize_entries(entries))\n",
    "\n",
    "    # duplicate check (by instruction)\n",
    "    insts = [e.get(\"instruction\", \"\") for e in entries]\n",
    "    dup_counts = [(t, c) for t, c in Counter(insts).most_common() if c > 1]\n",
    "    print(\"\\nTop duplicate instructions (first 5):\")\n",
    "    for t, c in dup_counts[:5]:\n",
    "        print(f\"- {c}x: {t[:120]!r}\")\n",
    "\n",
    "    # Show first 5 samples\n",
    "    print(\"\\nFirst 5 samples:\")\n",
    "    for i, e in enumerate(entries[:5], start=1):\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        print(\"Instruction:\", e.get(\"instruction\", \"\")[:400])\n",
    "        print(\"Output:\", e.get(\"output\", \"\")[:800])\n",
    "\n",
    "    # Random samples\n",
    "    print(\"\\nRandom samples:\")\n",
    "    for i, e in enumerate(random.sample(entries, min(3, len(entries))), start=1):\n",
    "        print(f\"\\n--- Random {i} ---\")\n",
    "        print(\"Instruction:\", e.get(\"instruction\", \"\")[:400])\n",
    "        print(\"Output:\", e.get(\"output\", \"\")[:800])\n",
    "\n",
    "    # Show any invalid lines (first 5)\n",
    "    if invalid:\n",
    "        print(\"\\nInvalid lines (first 5):\")\n",
    "        for idx, err, snippet in invalid[:5]:\n",
    "            print(f\"- line {idx}: {err} -> {snippet!r}\")\n",
    "\n",
    "    # LLM metrics summary (if any)\n",
    "    print(\"\\nLLM metrics summary:\\n\", summarise_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5: Run the Processing\n",
    "if __name__ == \"__main__\":\n",
    "    if USE_BATCHING:\n",
    "        process_pdfs_with_batching()\n",
    "    else:\n",
    "        process_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf54c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry-run: Convert the first PDF, extract the first chunk, and generate one SFT example for inspection\n",
    "from docling.document_converter import DocumentConverter\n",
    "converter = DocumentConverter()\n",
    "pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "if not pdf_files:\n",
    "    raise RuntimeError(f\"No PDFs found in {RAW_DATA_DIR}; please add a PDF to test the dry-run\")\n",
    "pdf_path = pdf_files[0]\n",
    "log(f\"Using PDF: {pdf_path.name}\")\n",
    "result = converter.convert(pdf_path)\n",
    "md = result.document.export_to_markdown()\n",
    "chunks = chunk_text_to_chunks(md)\n",
    "if not chunks:\n",
    "    raise RuntimeError(\"No chunks extracted from the PDF content\")\n",
    "log(f\"Extracted {len(chunks)} chunks; using chunk 0 for dry-run\")\n",
    "chunk0 = chunks[0]\n",
    "print(\"--- CHUNK SNIPPET ---\")\n",
    "print(chunk0[:800])\n",
    "print(\"--- END SNIPPET ---\\n\")\n",
    "\n",
    "# Run a single-call self-audit generation (fast) for inspection\n",
    "out = generate_and_audit_single(chunk0)\n",
    "print(\"Generated SFT pair (single-call):\\n\", out)\n",
    "\n",
    "# Show quick LLM metrics\n",
    "print(\"\\nMETRICS SUMMARY:\\n\", summarise_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a76c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs(max_chunks_per_pdf: int = None):\n",
    "    \"\"\"Process PDFs in non-batching mode. Processes uncached chunks concurrently using `process_chunk`.\n",
    "\n",
    "    Writes ordered `*.train.jsonl` files in `PROCESSED_DIR`.\n",
    "    \"\"\"\n",
    "    converter = DocumentConverter()\n",
    "    pdf_files = list(RAW_DATA_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        log(f\"Error: No PDF files found in {RAW_DATA_DIR}\")\n",
    "        log(f\"Note: Place your PDFs in: {RAW_DATA_DIR}\")\n",
    "        return\n",
    "\n",
    "    log(f\"Found {len(pdf_files)} PDF(s) in 'in-progress' folder.\")\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        pdf_stem = pdf_path.stem\n",
    "        output_file = PROCESSED_DIR / f\"{pdf_stem}.train.jsonl\"\n",
    "\n",
    "        log(f\"\\n--- Processing: {pdf_path.name} → {output_file.name} ---\")\n",
    "\n",
    "        try:\n",
    "            result = converter.convert(pdf_path)\n",
    "            md_content = result.document.export_to_markdown()\n",
    "        except Exception as e:\n",
    "            log(f\"      ! Failed to convert PDF: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Chunk and cache per-PDF\n",
    "        pdf_hash = pdf_sha256(pdf_path)\n",
    "        cached = get_cached_chunks(pdf_hash)\n",
    "        if cached:\n",
    "            chunks = cached\n",
    "            log(f\"Using {len(chunks)} cached chunks for {pdf_path.name}\")\n",
    "        else:\n",
    "            chunks = chunk_text_to_chunks(md_content)\n",
    "            cache_chunks(pdf_hash, chunks)\n",
    "            log(f\"Extracted and cached {len(chunks)} chunks.\")\n",
    "\n",
    "        process_count = len(chunks) if max_chunks_per_pdf is None else min(len(chunks), max_chunks_per_pdf)\n",
    "        log(f\"Processing {process_count} chunks (single-call={USE_SINGLE_CALL}, batching={USE_BATCHING})...\")\n",
    "\n",
    "        # Determine uncached indices\n",
    "        uncached_indices = []\n",
    "        for i in range(process_count):\n",
    "            chh = chunk_sha256(chunks[i])\n",
    "            if get_cached_sft_pair(chh) is None:\n",
    "                uncached_indices.append(i)\n",
    "\n",
    "        results_buffer = [None] * process_count\n",
    "        successful_entries = 0\n",
    "        file_lock = threading.Lock()\n",
    "\n",
    "        if uncached_indices:\n",
    "            semaphore = threading.BoundedSemaphore(MAX_LLM_CONCURRENCY)\n",
    "            with ThreadPoolExecutor(max_workers=MAX_LLM_CONCURRENCY) as executor:\n",
    "                future_to_idx = {executor.submit(process_chunk, chunks[i], i, semaphore): i for i in uncached_indices}\n",
    "\n",
    "                for future in tqdm(as_completed(future_to_idx), total=len(future_to_idx), desc=f\"SFT [{pdf_stem}]\"):\n",
    "                    try:\n",
    "                        idx, line = future.result()\n",
    "                    except Exception as e:\n",
    "                        log(f\"      ! Chunk job failed: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    if line:\n",
    "                        results_buffer[idx] = line\n",
    "\n",
    "        # Fill results from cache for any missing\n",
    "        for i in range(process_count):\n",
    "            if results_buffer[i] is None:\n",
    "                cached_pair = get_cached_sft_pair(chunk_sha256(chunks[i]))\n",
    "                if cached_pair:\n",
    "                    results_buffer[i] = json.dumps(cached_pair, ensure_ascii=False) + \"\\n\"\n",
    "\n",
    "        # Write results in order\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in results_buffer:\n",
    "                if line:\n",
    "                    with file_lock:\n",
    "                        f.write(line)\n",
    "                        f.flush()\n",
    "                        successful_entries += 1\n",
    "\n",
    "        log(f\"Saved {successful_entries}/{process_count} high-quality entries → {output_file}\")\n",
    "\n",
    "    log(f\"\\nAll processing complete! Outputs in: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ca2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check outputs: list processed .train.jsonl files and preview first file\n",
    "files = sorted(PROCESSED_DIR.glob(\"*.train.jsonl\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not files:\n",
    "    print(\"No processed files found in:\", PROCESSED_DIR)\n",
    "else:\n",
    "    print(f\"Found {len(files)} processed file(s). Latest: {files[0].name}\")\n",
    "    p = files[0]\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        lines = [next(f).strip() for _ in range(min(5, sum(1 for _ in open(p, 'r', encoding='utf-8'))))]\n",
    "    print('\\nFirst up to 5 lines (sample):')\n",
    "    for i, ln in enumerate(lines, start=1):\n",
    "        print(f\"{i}: {ln[:400]}\")\n",
    "    # Print metrics summary\n",
    "    print('\\nLLM metrics summary:\\n', summarise_metrics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".trainingEnv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
